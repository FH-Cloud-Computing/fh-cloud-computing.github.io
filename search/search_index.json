{"config":{"lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Cloud Computing Fall 2020 Welcome to the Cloud Computing Fall 2020! On this page you will find all learning materials you need to learn in order to pass the course. Please note that this lecture is worth 4 ECTS. This is calculated to be roughly 112 hours of study (5-7 hours of study per week recommended). Lectures # Lectures are provided on the lectures page . Each lecture contains a text document, a PPTX, an audio book in MP3 and M4B (Apple) format, as well as a video . You can also listen to individual sections from the text document. All content is available in all formats so you only need to use the formats you prefer. You are expected to understand and memorize the \u201eIn a hurry?\u201d sections from the text version for the theory test. Practice # Practice sections are provided on the practice page . Each practice session is provided as a written tutorial and a video as well. All content is present in the video and the text so you only need to use the format you prefer. Source code is provided as a Git repository. You are expected to do each practice session at least once. Project work # Project work is based on the practice sessions . Your project work assignment is described on the project work page . Deadlines are described on the deadlines page . You are expected to hand in your project work continuously, please don't leave it for the last second. You have the option to hand in early. Online consultations & getting help. # Online consultations are provided on a regular basis. Please find detailed description on how to find help on the getting help page . Grading & testing # Grading is described on the grading page . Testing is described on the testing page . About us # Peter Wenzl # E-mail: peter.wenzl@edu.fh-campuswien.ac.at Computer Science Master (Dipl. Ing.), TU Wien, 2000 Telecoms Focused Career Ericsson Austria mobilkom Austria Oracle Austria GmBH (CGBU) Frequentis AG Janos Pasztor # E-mail: janos.pasztor@edu.fh-campuswien.ac.at Linkedin: www.linkedin.com/in/janoszen/ Website: pasztor.at Software developer and DevOps engineer background (10+ years experience) Web Focused Career Currently works as a Developer Advocate at A1 Telekom / A1 Digital / Exoscale Certified Kubernetes Application Developer","title":"Overview"},{"location":"#lectures","text":"Lectures are provided on the lectures page . Each lecture contains a text document, a PPTX, an audio book in MP3 and M4B (Apple) format, as well as a video . You can also listen to individual sections from the text document. All content is available in all formats so you only need to use the formats you prefer. You are expected to understand and memorize the \u201eIn a hurry?\u201d sections from the text version for the theory test.","title":"Lectures"},{"location":"#practice","text":"Practice sections are provided on the practice page . Each practice session is provided as a written tutorial and a video as well. All content is present in the video and the text so you only need to use the format you prefer. Source code is provided as a Git repository. You are expected to do each practice session at least once.","title":"Practice"},{"location":"#project_work","text":"Project work is based on the practice sessions . Your project work assignment is described on the project work page . Deadlines are described on the deadlines page . You are expected to hand in your project work continuously, please don't leave it for the last second. You have the option to hand in early.","title":"Project work"},{"location":"#online_consultations_getting_help","text":"Online consultations are provided on a regular basis. Please find detailed description on how to find help on the getting help page .","title":"Online consultations &amp; getting help."},{"location":"#grading_testing","text":"Grading is described on the grading page . Testing is described on the testing page .","title":"Grading &amp; testing"},{"location":"#about_us","text":"","title":"About us"},{"location":"#peter_wenzl","text":"E-mail: peter.wenzl@edu.fh-campuswien.ac.at Computer Science Master (Dipl. Ing.), TU Wien, 2000 Telecoms Focused Career Ericsson Austria mobilkom Austria Oracle Austria GmBH (CGBU) Frequentis AG","title":"Peter Wenzl"},{"location":"#janos_pasztor","text":"E-mail: janos.pasztor@edu.fh-campuswien.ac.at Linkedin: www.linkedin.com/in/janoszen/ Website: pasztor.at Software developer and DevOps engineer background (10+ years experience) Web Focused Career Currently works as a Developer Advocate at A1 Telekom / A1 Digital / Exoscale Certified Kubernetes Application Developer","title":"Janos Pasztor"},{"location":"deadlines/","text":"Each sprint has a hard deadline. Sprints can be handed in earlier. If a deadline is missed the points are not awarded and the solution is uploaded to GitHub so the next sprint can be accomplished. Sprint 1: Instance Pools # Deadline: 1 st of October 2020 In this sprint you must demonstrate your ability to set up an instance pool and a network load balancer. Pass criteria: your webservice must answer on an IP address and balance traffic across all instances running in an instance pool. The number of instances will be changed in the demonstration and your web service must adapt accordingly. Sprint 2: Monitoring # Deadline: 1 st of November 2020 In this sprint you must demonstrate your ability to monitor a varying number of instances set up on an instance pool in the previous sprint. Pass criteria: Your monitoring instance must be set up with Prometheus and Grafana running. Prometheus must track the instances in the instance pool using custom service discovery and collect their CPU usage in a graph in Grafana. Sprint 3: Autoscaling # Deadline: 1 st of December 2020 In this sprint you must demonstrate your ability to receive a webhook from Grafana and adjust the number of instances in the instance pool from the previous sprint under load. Pass criteria: Your service will be hit with a large number of requests and it must scale up as outlined in the project work document .","title":"Deadlines"},{"location":"deadlines/#sprint_1_instance_pools","text":"Deadline: 1 st of October 2020 In this sprint you must demonstrate your ability to set up an instance pool and a network load balancer. Pass criteria: your webservice must answer on an IP address and balance traffic across all instances running in an instance pool. The number of instances will be changed in the demonstration and your web service must adapt accordingly.","title":"Sprint 1: Instance Pools"},{"location":"deadlines/#sprint_2_monitoring","text":"Deadline: 1 st of November 2020 In this sprint you must demonstrate your ability to monitor a varying number of instances set up on an instance pool in the previous sprint. Pass criteria: Your monitoring instance must be set up with Prometheus and Grafana running. Prometheus must track the instances in the instance pool using custom service discovery and collect their CPU usage in a graph in Grafana.","title":"Sprint 2: Monitoring"},{"location":"deadlines/#sprint_3_autoscaling","text":"Deadline: 1 st of December 2020 In this sprint you must demonstrate your ability to receive a webhook from Grafana and adjust the number of instances in the instance pool from the previous sprint under load. Pass criteria: Your service will be hit with a large number of requests and it must scale up as outlined in the project work document .","title":"Sprint 3: Autoscaling"},{"location":"exercises/","text":"","title":"Overview"},{"location":"exercises/1-iaas/","text":"In this exercise we will take a look at a simple IaaS provider and set up our first virtual machine.","title":"1. IaaS"},{"location":"glossary/","text":"API # Application Program Interfaces are methods for programs to exchange data without the involvement of humans. APIs often involve a schematic description of how the data looks like and where it needs to be sent. Today many APIs are based on HTTP . HTTP # The Hypertext Transfer Protocol is the protocol that powers the world wide web allowing for the easy up- and download of data. With HTTP each file (resource) has a unique URL on a server and can be linked. Rack # A closet with standardized mounts for servers. Router # A network device that forwards layer 3 (IP) packets between separate networks. Switch # A network device that forwards Ethernet frames (packets) between devices. It does not perform layer 3 (IP) routing.","title":"A-Z"},{"location":"glossary/#api","text":"Application Program Interfaces are methods for programs to exchange data without the involvement of humans. APIs often involve a schematic description of how the data looks like and where it needs to be sent. Today many APIs are based on HTTP .","title":"API"},{"location":"glossary/#http","text":"The Hypertext Transfer Protocol is the protocol that powers the world wide web allowing for the easy up- and download of data. With HTTP each file (resource) has a unique URL on a server and can be linked.","title":"HTTP"},{"location":"glossary/#rack","text":"A closet with standardized mounts for servers.","title":"Rack"},{"location":"glossary/#router","text":"A network device that forwards layer 3 (IP) packets between separate networks.","title":"Router"},{"location":"glossary/#switch","text":"A network device that forwards Ethernet frames (packets) between devices. It does not perform layer 3 (IP) routing.","title":"Switch"},{"location":"grading/","text":"Grade construction # 55% written examination (individual). 45% project work (15% per sprint). Project work grading # 5% in each sprint for implementing it using an automation framework such as Terraform or Ansible. It must have the ability to install the exercise in an empty Exoscale account without human interaction. 10% in each sprint for demonstrating a working functionality. Marks # Positive grade: Minimum 60% score in written exam plus 60% of overall score required. 1 2 3 4 5 90% \u2014 100% 80% \u2014 89% 70% \u2014 79% 60% \u2014 69% <60%","title":"Grading"},{"location":"grading/#grade_construction","text":"55% written examination (individual). 45% project work (15% per sprint).","title":"Grade construction"},{"location":"grading/#project_work_grading","text":"5% in each sprint for implementing it using an automation framework such as Terraform or Ansible. It must have the ability to install the exercise in an empty Exoscale account without human interaction. 10% in each sprint for demonstrating a working functionality.","title":"Project work grading"},{"location":"grading/#marks","text":"Positive grade: Minimum 60% score in written exam plus 60% of overall score required. 1 2 3 4 5 90% \u2014 100% 80% \u2014 89% 70% \u2014 79% 60% \u2014 69% <60%","title":"Marks"},{"location":"help/","text":"Everybody needs help, and since this is a new situation we have prepared a few channels where you can get help. Before you ask for help # Make sure you strip the code you have a problem with of all unnecessary code, comments, etc. Make sure you upload your code to GitHub so others can take a look at it. Make sure you think through and describe your problem in more detail as you would in a conversation, as digitally it may be harder to follow. Where to ask for help # The primary avenue of getting help is on the Slack channel for this course . Join the weekly online consultation sessions. If you need to send us something, please do so on the email addresses on the introduction page .","title":"Getting help"},{"location":"help/#before_you_ask_for_help","text":"Make sure you strip the code you have a problem with of all unnecessary code, comments, etc. Make sure you upload your code to GitHub so others can take a look at it. Make sure you think through and describe your problem in more detail as you would in a conversation, as digitally it may be harder to follow.","title":"Before you ask for help"},{"location":"help/#where_to_ask_for_help","text":"The primary avenue of getting help is on the Slack channel for this course . Join the weekly online consultation sessions. If you need to send us something, please do so on the email addresses on the introduction page .","title":"Where to ask for help"},{"location":"lectures/","text":"Lecture 1: Introduction # NIST Definition of Cloud IaaS, PaaS, SaaS, FaaS Public vs. Private Cloud Basic \u201chosting\u201d service vs. managed service Benefits/Risks/Regulations: Scalability, Privacy, GDPR, Safe Harbor/Privacy Shield Business Models: Overbooking, Pay-per-Use, Standardization & Automation Go to lecture \u00bb Lecture 2: Cloud capabilities # Cloud roles: Platform/Service provider vs. customers (developers, enterprises) Cloud architectures & definitions Operational requiremens Cloud performance Cloud service provisioning Extra Lectures: Various cloud stacks, Cloud networking, Security concepts Go to lecture \u00bb Lecture 3: IaaS introduction # Intro to Cloud Platforms (Exoscale, Amazon AWS/EC2 and Google Cloud Platform) Virtual Server+Storage+Networking capabilities Automation via Terraform Hands-on demos Go to lecture \u00bb Lecture 4: Beyond IaaS # Container Services (Docker & Kubernetes) Differences Container vs. classical server virtualization PaaS, SaaS, FaaS (Concepts, Examples) Hands-on demos Go to lecture \u00bb Lecture 5: Cloud-native software development # Cloud-native software basics & definitions Microservices 12-factor Apps Frameworks and tools Hands-on demos Go to lecture \u00bb","title":"Overview"},{"location":"lectures/#lecture_1_introduction","text":"NIST Definition of Cloud IaaS, PaaS, SaaS, FaaS Public vs. Private Cloud Basic \u201chosting\u201d service vs. managed service Benefits/Risks/Regulations: Scalability, Privacy, GDPR, Safe Harbor/Privacy Shield Business Models: Overbooking, Pay-per-Use, Standardization & Automation Go to lecture \u00bb","title":"Lecture 1: Introduction"},{"location":"lectures/#lecture_2_cloud_capabilities","text":"Cloud roles: Platform/Service provider vs. customers (developers, enterprises) Cloud architectures & definitions Operational requiremens Cloud performance Cloud service provisioning Extra Lectures: Various cloud stacks, Cloud networking, Security concepts Go to lecture \u00bb","title":"Lecture 2: Cloud capabilities"},{"location":"lectures/#lecture_3_iaas_introduction","text":"Intro to Cloud Platforms (Exoscale, Amazon AWS/EC2 and Google Cloud Platform) Virtual Server+Storage+Networking capabilities Automation via Terraform Hands-on demos Go to lecture \u00bb","title":"Lecture 3: IaaS introduction"},{"location":"lectures/#lecture_4_beyond_iaas","text":"Container Services (Docker & Kubernetes) Differences Container vs. classical server virtualization PaaS, SaaS, FaaS (Concepts, Examples) Hands-on demos Go to lecture \u00bb","title":"Lecture 4: Beyond IaaS"},{"location":"lectures/#lecture_5_cloud-native_software_development","text":"Cloud-native software basics & definitions Microservices 12-factor Apps Frameworks and tools Hands-on demos Go to lecture \u00bb","title":"Lecture 5: Cloud-native software development"},{"location":"lectures/1-cloud-intro/","text":"Download PPTX \ud83d\udcbb Download MP3 \ud83c\udfa7 Download M4B \ud83c\udfa7 Watch Video \ud83c\udfac Introduction to the Cloud As the popular saying goes: The cloud is just somebody else's computer. There is no magic, just using the cloud does not magically solve problems we are having with a traditional infrastructure. This lecture will teach you how a cloud is built and what the typical services are that they offer as well as the pitfalls which may come with such setups. What is a Server? # In a hurry? Servers: Nowadays either x86 or ARM architecture. Redundant, hot-swap hardware (power supply, fans, etc). Flat build profile for rack mounts. Out-Of-Bounds management interface for monitoring and remote management. Power consumption is a concern. Buying servers can take a long time and present an up-front investment. In older times, servers were completely different from the machines we used for regular work. Not just in weight and form factor, but in architecture. The landscape would stretch from SPARC and Power architectures to the x86 architecture we use in our PCs. Over time, however, the x86 architecture took over to the point where a server today, from an architectural standpoint, is exactly the same as the machine you are using right now. This means that you can copy the machine you are using onto a server and chances are it will run without any modification. The only notable exception is the rise of the ARM architecture which is popular in phones and tables and is known for its low power consumption. In recent years there has been a small but noticeable trend to run ARM in a datacenter. At home it may not matter if your computer uses 200 or 300 watts of power. In a datacenter, at the scale of hundreds, thousands or tens of thousands of servers, saving even a few percent of power will translate to huge cost savings. The difference in build is, however, quite apparent. While some servers, mainly built for office use, have the standard \u201ctower\u201d build, most servers have a flat profile designed to be mounted in racks as displayed on the picture. Since most servers are not high enough to take full size expansion cards (graphics cards, network cards, etc), servers may contain a separate removable component for these usually called a riser. An HP Proliant G5 server pulled out of its rack. Source: Wikipedia Server racks are standardized closets that have mounting screws for rails that allow pulling servers out even mid operation and replace components while the server is running. Servers also come with a high level of redundancy. While you may have a single power supply in your home computer, servers typically have two that are able to take power from different power inputs. This makes sure that the server keeps running even if one of the two power supplies fail, or if one of the two power inputs goes down. Also in contrast to your home setup these servers contain an Out-Of-Bounds Management interface that allows remote management of servers even when they are turned off. The hardware components built into the server report their health status to this OOB management interface which allows for the simultaneous monitoring of thousands of machines. Note This is a fairly generic description of servers. Different vendors may chose to leave out certain features of their more \u201cbudget\u201d line of servers, or call certain components differently. HP, for example, calls their OOBM \u201cIntegrated Lights Out\u201d, Dell \u201cDRAC - Dell Remote Access Control\u201d, etc. When it comes to purchasing servers larger companies tend to go with the same components for a longer period of time and they also buy support from the vendor. This sometimes includes hardware replacement done entirely by the vendor in the datacenter without the need for the customer to have staff on site. However, purchasing a specific set of components or ordering larger quantities of servers presents a logistics challenge and can sometimes take up to 3-4 months. Buying hardware is also an up-front investment which is hard to justify when demands change rapidly. What components are redundant in a server? Power supply CPU RAM Fan OOBM What is the purpose of an OOBM? Remotely manage a server Receive hardware malfunction alerts The Anatomy of a Datacenter # In a hurry? Datacenter components: Racks to house servers. Larger customers have cages for their racks. Cabling under the floor. Redundant cooling, fire suppression systems and power supply. Some datacenters provide internet connectivity. Eco friendliness is becoming a factor. Since the cloud is just somebody else's computer, that computer needs to be hosted somewhere. Servers are almost exclusively hosted in datacenters. Let's take a look at what is involved in running a datacenter. First of all, as mentioned above, most servers are going to be rack-mounted so you need a bunch of racks. These racks are installed in rows, often with a fake floor to allow for cabling to go under the floor. A datacenter with racks. Source: Wikipedia Since servers produce a lot of heat, a datacenter also requires cooling. There are a variety of ways to solve cooling, some are more \u201cgreen\u201d than others. Some datacenters, for example, opt to install a \u201ccold aisles\u201d where the cold air is pumped between two rack rows and is pushed through the racks to cool the servers. Apart from cooling, datacenters also require automated fire suppression systems simply because of the amount of electricity going through. Datacenters usually go with a non-destructive fire suppression system such as lowering the oxygen content of the air enough to stop the fire. All critical systems in a datacenter (power, cooling, fire suppression systems) are usually built in a redundant fashion because the loss of either of those systems will potentially mean a complete shutdown for the datacenter. Datacenter operators usually have further contingency plans in place, too, such as a UPS (battery) system, diesel generator, fuel truck on standby, hotline to the fire department, etc. to make sure the datacenter can keep its required uptime. On the networking side of things, matters get slightly more complicated. Some datacenter providers also offer you the ability to use their network uplink (also redundant), but larger customers will prefer to host their own networking equipment and negotiate their own internet uplink contracts. Since there is no generic rule for how datacenters handle this, we will dispense with a description. It is also worth noting that larger customers (banks, cloud providers, etc) usually prefer to have their own racks in a separated gated area called a \u201ccage\u201d to which they control access. The Anatomy of the Internet # In a hurry? Internet: IP ranges are advertised using BGP. Providers connect direcly or using internet exchanges. 16 global providers form the backbone of the internet (tier 1). Once the physical infrastructure is set up there is also the question of how to connect to the Internet. As mentioned before, networks can be very complicated and there is no one size fits all solution. Smaller customers will typically use the network infrastructure provided by the datacenter while larger customers will host their own network equipment. Again, generally speaking racks will be equipped with a Top-of-Rack switch to provide layer 2 (Ethernet) connectivity between servers. Several ToR may have interconnects between each other and are usually connected to one or more routers. Routers provide layer 3 (IP) routing to other customers in the same datacenter, internet exchange , or may be connected via dedicated fiber to another provider. Note If you are not familiar with computer networks we recommend giving the Geek University CCNA course a quick read . While you will not need everything, you will have to understand how IP addresses, netmasks, etc work in order to pass this course. Providers on the internet exchange data about which network they are hosting using the Border Gateway Protocol . Each provider's router announces the IP address ranges they are hosting to their peer providers, who in turn forward these announcements in an aggregated form to other providers. Providers have agreements with each other, or with an Internet Exchange, about exchanging a certain amount of traffic. These agreements may be paid if the traffic is very asymmetric or one provider is larger than the other. Alternatively providers can come to an arrangement to exchange traffic for free. Internet exchanges facilitate the exchange between many providers for a modest fee allowing cost-effective exchange of data. Depending on the exchange the rules are different. Local exchanges, for example, may only allow advertising local (in-country) addresses, while others are built for a specific purpose. Generally speaking providers can be classified into 3 categories. Tier 1 providers are the global players that are present on every continent. They form the backbone of the Internet. At the time of writing there are 16 such networks . Tier 2 are the providers who are directly connected to the tier 1 providers, while tier 3 is everyone else. Software Stack # In a hurry? Software stack: Virtualization. Operating system. Application runtime. Application. The purpose of all this is, of course, to run an application. Each server hosts an operating system which is responsible for managing the hardware. Operating systems provide a simplified API for applications to do hardware-related operations such as dealing with files or talking to the network. This part of the operating system is called the kernel. Other parts form the userland. The userland includes user applications such as a logging software. Specifically on Linux and Unix systems the userland also contains a package manager used to install other software. Modern x86 server CPUs (and some desktop CPUs) also have a number of features that help with virtualization. Virtualization lets the server administrator run multiple guest operating systems efficiently and share the server resources between them. Did you know? You can find out if an Intel CPU supports hardware virtualization by looking for the VT-x feature on the Intel ARK . Unfortunately AMD does not have an easy to use list but you can look for the AMD-V feature on AMD CPUs. Note Virtualization is different from containerization (which we will talk about later) in that with virtualization each guest operating system has its own kernel whereas containers share a kernel between them. There is one more important aspect of finally getting an application to run: the runtime environment. Except for a few rare occasions applications need a runtime environment. If the application is compiled to machine code they still need so-called shared libraries. Shared libraries are common across multiple applications and can be installed and updated independently from the application itself. This makes for a more efficient update process, but also means that the right set of libraries need to be installed for applications. If the applications are written higher level languages like Java, Javascript, PHP, etc. they need the appropriate runtime environment for that language. One notable exception to the runtime environment requirement is the programming language Go . Go compiles everything normally located in libraries into a single binary along with the application. This makes it exceptionally simple to deploy Go applications into containers. The Cloud # In a hurry? Typical cloud features: API Dynamic scaling Can be classified into IaaS, PaaS and SaaS IaaS service offerings typically include: Virtualization. Network infrastructure. Everything required to run the above. PaaS service offerings typically include a managed service ready to be consumed by a developer. SaaS service offerings typically include a managed service ready to be consumed by a non-technical end user. All of the previously discussed things were available before the \u201ccloud\u201d. You could pay a provider to give you access to a virtual machine where you could run your applications. What changed with the cloud, however, is the fact that you no longer had to write a support ticket for changed and everything became self service. The cloud age started with an infamous e-mail from Jeff Bezos to his engineers in 2002 forcing them to use APIs to exchange data between teams. The exact e-mail is no longer available but it went along these lines: 1) All teams will henceforth expose their data and functionality through service interfaces. 2) Teams must communicate with each other through these interfaces. 3) There will be no other form of interprocess communication allowed: no direct linking, no direct reads of another team\u2019s data store, no shared-memory model, no back-doors whatsoever. The only communication allowed is via service interface calls over the network. 4) It doesn\u2019t matter what technology is used. HTTP, Corba, Pubsub, custom protocols \u2014 doesn\u2019t matter. 5) All service interfaces, without exception, must be designed from the ground up to be externalizable. That is to say, the team must plan and design to be able to expose the interface to developers in the outside world. No exceptions. 6) Anyone who doesn\u2019t do this will be fired. This marked the beginning of Amazon Web Services the first and also the most successful public cloud offering. The first public release of AWS was in 2004 with SQS their message queue service, and got completely overhauled in 2006 where the Elastic Compute (EC2) and the Simple Storage Service (S3) service made its first public appearance. The APIs provided by cloud providers allow for a large amount of flexibility. If new servers are needed they can be launched within a few minutes. If there are too many servers they can be deleted. The same goes for other services: with the API (and the appropriate billing model) comes flexibility to adapt to change. The other factor that makes it easy to adapt to change is of course the fact that these services are managed. The cloud customer doesn't need to hire a team of engineers to build a database service, for example, it can be consumed without knowing how exactly the database is set up. Generally speaking, cloud services can be classified into three categories: Infrastructure as a Service (IaaS) providing virtual machines and network infrastructure, Platform as a Service (PaaS) offering services for developers to use, and Software as a Service (SaaS) offering end-user services. Note SaaS will not be discussed in this course. Infrastructure as a Service (IaaS) # The most basic of cloud service offerings is IaaS. IaaS means that the cloud provider will manage the infrastructure used by the customer. Infrastructure in this sense means the ability to manage (provision, start,stop) virtual machines. In very rare cases some providers also offer \u201cbare metal\u201d machines in this fashion. However, most bare metal providers do not offer a true IaaS as machines cannot be ordered using an API, have an up-front fee and are billed on a monthly basis. IaaS also includes the network connectivity to the Internet. Almost all IaaS providers also offer a built-in firewall, virtual private networks (VPC) that can be used to connect virtual machines together without sending the traffic over the Internet, and other network services. Note that network services can differ greatly. For example, some providers implement private networks on a regional basis while other providers offer private networks that can be used to connect virtual machines that are located in different regions. Managed Services (PaaS) # In a hurry? Managed services: The provider is typically responsible for provisioning, backups, monitoring and restoring the service if needed. Low entry cost. Little in-house know-how required. Vendor lock-in for non-standard services. If problems arise they can be hard to debug. Apart from offering virtual machines and network infrastructure as a service many cloud providers also offer additional services that are typically used by developers such as managed databases. These services are managed in the sense that the developer does not need to run the operating system and the database software itself. However, the developer has to configure the database such that it works according to the needs of the application. The provider takes over duties like installing the service, making backups, monitoring, and restoring the service in case of an outage. Did you know? There are purely PaaS providers that do not offer IaaS, only a developer-friendly platform to run software on. Your run of the mill average PHP web hosting provider does qualify if you can order the service using an API. One notable example that rose to prominence is Heroku . The pricing of these managed services varies greatly but they usually follow the cloud model. Most cloud providers offer at least a low cost or even free entry version that has a small markup on top of the IaaS costs. The massive benefit of using these managed services is, of course, that you do not need to have in-house knowledge about how to operate them. You don't need an in-house database operator, you \u201cjust\u201d need to know how to set up the database in the cloud. This lets your company focus on the core business they are trying to build and outsource the know-how required to build these services on top of the IaaS layer. As you might imagine managed servies also have downsides. Most importantly they present a clear vendor lock-in. This means that if you want to move to a different cloud provider you will have a hard time doing so if you are using one of the more specialized services. In other words the level of standardization across providers matters. It is also worth mentioning that managed services tend to work well for a large number of customers but a few number of customers can run into hard to debug problems. This debugging difficulty arises out of the inherent opacity of the services: you, the customer, don't see what's happening on the IaaS layer. If a database, for example, fills the available bandwidth may not be notified and are left guessing why your database misbehaves. Business Models # In a hurry? Billing models: IaaS is typically priced per-second based on the instance size. PaaS can be priced similar to IaaS but also per-request or data volume. Data volume is typically priced based on usage. Some providers charge for data volume even for internal traffic. The flexibility of cloud providers comes from their usage-based pricing. This can vary depending on the type of service used. For example, virtual machines (IaaS) is typically priced on a per-second basis. When you start a virtual machine for only 5 minutes you will only pay for 5 minutes of runtime. This lets you optimize the usage costs of the cloud if you use automation to start and stop machines or services on demand. Typically the cost savings are realized in these scenarios: On-off usage # This usage type is typical for batch processing. Machines are only started when there is a workload (e.g. video conversion, machine learning training job, etc.) Growth usage # Projects that see a growth curve often opt to use a cloud provider as well since buying hardware in larger quantities typically takes 2-4 months. Sudden spike usage # Cloud providers can also be useful if a service encounters a sudden load spike. This is typically the case for webshops around Black Friday and other sales events. Periodic spike usage # Almost every service has usage spikes depending on the time of day. This can be used to scale the service up and down. Per-request billing # IaaS services are typically priced based on an allocated amount of resources determined at the start of the virtual machines. Some PaaS services also use this billing model. (Typically Databases as a Service.) Other PaaS services often opt for a per-request or a data volume based billing approach. Cost planning with the cloud # Cost planning is an important part of the job of a cloud architect. Depending on the billing model the cloud provider adopts this can be fairly simple to almost impossible. As a rule of thumb the billing model of larger cloud providers (AWS, Azure, Google Cloud) is more complex than smaller cloud providers (DigitalOcean, Exoscale, Upcloud, etc). It is worth noting that data transfer is typically charged based on volume. Some cloud providers even charge for internal traffic between availability zones. Cost-comparison with on premises systems # One of the most important cases for cost analysis will be the comparison with on premises architecture. It is quite common to create a cost comparison where the investment required for a hardware purchase is compared with the cloud cost over 3 or 5 years. These comparisons can be quite misleading because they often don't contain any cost attribution for the required work and they also do not use the advantages of the cloud in terms of scaling. Without these factors a hardware purchase will almost always be cheaper when calculated over 5 years and be roughly equal when compared over 3 years. Private vs. Public Cloud # In a hurry? Private cloud: Hosted on-premises or in the public cloud using only private connections. Large cloud providers offer their services \u201cin a box\u201d to host yourself. Most cloud applications will reside on a public cloud and be accessible from the Internet. However, for some usecases access over the public Internet is not desirable or even strictly forbidden by laws or regulations. In order cases companies may decide that it is their policy that certain systems must never be connected to the public Internet, or only connect via a self-hosted gateway. In these cases a private cloud is desirable to minimize the risk of cross-contamination or data exposure due to other tenants being on the same infrastructure. Such cross-contamination is not uncommon, for example in recent years there have been a litany of CPU bugs such as Meltdown and Spectre . Public cloud providers have extended their offers to address these concerns. Their service offerings now include dedicated hypervisors to mitigate CPU bugs, the ability to connect the cloud bypassing the Internet , or accessing the cloud API's from a private network . Some cloud providers even went as far offering a self-hosted setup where the public cloud offering can be brought on-premises. These features, especially the connectivity-related features, also allow for the creation of a hybrid cloud where a traditional infrastructure can be connected to the cloud. This gives a company the ability to leverage the flexibility of the cloud but also keep static / legacy systems for financial or engineering reasons. Automation # In a hurry? Automation: Documents how a cloud is set up. Gives a reproducible environment. Allows for spinning up multiple copies of the same environment. Not all tools are equal. Some tools allow for manual changes after running them (e.g. Ansible), others don't (e.g. Terraform). As you can see from the previous sections cloud computing doesn't necessarily make it simpler to deploy an application. In this regard setting up a server and not documenting it is the same as setting up a cloud and not documenting it. In the end in both cases modifications will be hard to carry out later since details of the implementation are lost. It will also be hard to create a near-identical copy of the environment for development, testing, etc. purposes. This is partially due to the lack of documentation, and partially because of the manual work involved. In the past this problem was addressed by cloning virtual machines which was an approach with limited success as customizations were hard to make. The rise of API's brought a much welcome change: automation tools such as Puppet, Ansible and Terraform not only automate the installation of a certain software but also document how the setup works. When engineers first go into automation they tend to reserve a part of the work to be done manually. For example they might use Ansible to create virtual machines and install some basic tools and install the rest of the software stack by hand. This approach is workable where the software stack cannot be automated but should be avoided for software where this is not the case as the lack of automation usually also means a lack of documentation as described above. Automation tools are also not equally suited for each task. Terraform, for example, expects full control of the infrastructure that is created by it. Manual installation steps afterwards are not supported. This approach gives Terraform the advantage that it can also remove the infrastructure it creates. Removing the infrastructure is paramount when there are multiple temporary environments deployed such as a dev or testing environment. Ansible, on the other hand, allows for manual changes but does not automatically implement a tear-down procedure for the environment that has been created. This makes Ansible environments, and Ansible code harder to test and maintain but more suited for environments where traditional IT is still a concern. Regulation # In a hurry? GDPR: Applies to all companies, world-wide, that handle personal data of EU citizens. Companies must keep track of why and how data is handled. Data subjects have wide ranging, but not unlimited rights to request information, correction and deletion of data. Data breaches have to be reported and may carry a fine. The right of deletion means that backups have to be set up in such a way that data can be deleted. CLOUD Act: US authorities can access data stored by US companies in Europe. DMCA: Copyright infringements can be solved by sending a DMCA takedown notice to the provider. Providers have to restore the content if the uploader sends a counter-notification. CDA Section 230: Shields providers from liability if their systems host illegal content which they don't know about. Privacy Shield: Establishes a legal basis for transfering data from Europe to the US. General Data Protection Regulation (G.D.P.R., EU) # The GDPR (or DSGVO in German-speaking countries) is the general overhaul of privacy protections in the EU. The GDPR replaces much of the previously country-specific privacy protections present in the EU. Jurisdiction # The GDPR applies to all companies that deal with the data of EU citizens around the globe. This is made possible by trade agreements previously already in place. All companies that handle the data of EU citizens even if the companies are not located in the EU. Structure # Data subject: the person whose data is being handled. Personal identifiable data: (PI) data that makes it possible to uniquely identify a data subject. PI can also be created when previously non-PI data is combined to build a profile. Data controller: The company that has a relationship with the data subject and is given the data for a certain task. Data processor: A company that processes data on behalf of the data controller. The data processor must have a data processing agreement (DPA) with the data controller. IaaS and PaaS cloud providers are data processors. Purposes of data processing # One new limitation the GDPR brings to privacy regulation is the fact that there are fundamental limits to data processing. You, the data controller, cannot simply collect data for one purpose and then use that same data for another purpose. The purposes to which data is collected have to be clearly defined and in several cases the data subject has to provide their explicit consent. (In other words it is not enough to add a text to the signup form that says that they agree to everything all at once.) Legal grounds to data processing are detailed in Article 6 of the GDPR . These are: If the data subject has given their explicit consent to process their data for a specific purpose. If the processing is necessary to perform a contract with the data subject. In other words you do not need additional consent if you are processing data in order to fulfill a service to the data subject. To perform a legal obligation . For example, you have to keep an archive of your invoices to show in case of a tax audit. To protect the vial interests of the data subject or another natural person . To perform a task which is in the public interest , or if an official authority has been vested in the data controller. For the purposes of a legitimate interest of the data controller (with exceptions). This might seem like a catch-all but courts have defined legitimate interests very narrowly. The data controller must show that the legitimate interest exists and cannot be achieved in any other way than with the data processing in question. One good example for the legitimate interests clause would be a webshop that is collecting data about their visitors in order to spot fraudulent orders. Data subjects' rights # Chapter 3 of the GDPR deals with the rights of the data subject. These are: The right to be informed. This right lets the data subject request several pieces of information: The purposes of processing. The categories of personal data concerned. Who received their data, in particular recipients in third countries or international organisations. How long the data will be stored. The right to request modification, deletion or restricting the processing of the personal data in question. The right to log a complaint. The source of the personal data if not provided by the data subject themselves. If and how automated decision-making, profiling is taking place. The right to fix incorrect data. The right of deletion. \u201cThe right to be forgotten.\u201d This right puts several architectural limites on cloud systems as, for example, backups must be built in such a way that deletion requests are repeated after a restore. Note that this right is not without limits, legal obligations, for example, override it. The right to restrict processing. If a deletion cannot be requested the data subject can request that their data should only be processed to the purposes that are strictly required. The right to data portability. The data subject has to be provided a copy of their data in a machine-readable form. The right to object automated individual decision-making and profiling. Data breaches # Chapter IV Section 2 is probably the first legislation around the world that explicitly requires security of data processing from companies that handle personal data. Article 33 of this chapter specifically requires that data breaches must be disclosed to the supervisory authorities. Supervisory authorities in turn have the right to impose fines up to 4% or 20.000.000 \u20ac of the global revenue of a company. This means that companies can no longer sweep data breaches under the rug and must spend resources to secure their data processing. In the scope of the cloud this means that our cloud environment has to be set up in a secure way to avoid data breaches. Clarifying Lawful Overseas Use of Data Act (C.L.O.U.D., 2018, USA) # The CLOUD Act, or House Rule 4943 is the latest addition to the US legislation pertaining to cloud providers. This act says that a cloud provider must hand over data to US authorities if requested even if that data is stored in a different country. This act has been widely criticized and is, according to several legal scholars, in contradiction of the GDPR. The large US cloud providers have opened up subsidiaries in the EU in order to try and shield their european customers from this act and a few purely EU cloud providers have also capitalized on this. It remains to be seen how effective this move is. Digital Millennium Copyright Act (D.M.C.A., 1998, USA) # The Digital Millenium Copyright Act clarifies how copyright works in the USA. Since the USA is part of the WIPO copyright treaty and a significant amount of providers are based in the USA nowadays all countries align themselves with the DMCA when it comes to dealing with online copyright infringement. Title II of the DMCA creates a safe harbor for online service providers against copyright infringement committed by their users. However, they have to remove infringing content as soon as they are properly notified of it via a DMCA takedown notice. The author of a DMCA takedown notice must swear by the penalty of perjury that they are, or are acting on behalf of the copyright owner. (If they were to make this pledge in bad faith they could end up in prison.) If the DMCA takedown notice has been sent erroneously the original uploader is free to send a counter-notification, also swearing under the penalty of perjury. In this case the provider notifies the original sender and restores the content. The original sender then has to take the uploader to court to pursue the matter further. Communications Decency Act, Section 230 (C.D.A., 1996, USA) # This section of of the CDA is very short: No provider or user of an interactive computer service shall be treated as the publisher or speaker of any information provided by another information content provider. To someone not well versed in US legal lingo this may be sound like gibberish but it is, in fact, one of the most important pieces of legislation pertaining to operating platforms in the cloud. In US legal history the speaker or publisher of certain information is responsible for the information being provided. Before the CDA any provider hosting illegal content would be responsible for said content. The CDA changed that by shielding providers from liability. This included web hosting providers as well as newspapers that had a content section. The CDA is not limitless, providers still have to remove infringing content if notified. They may also employ moderation proactively. Privacy Shield (2016, EU-US) # The Privacy Shield agreement is the successor to the failed International Safe Harbor Privacy Principles framework and is intend to protect the privacy of EU citizens when using US services or their data is transferred to the USA. The Privacy Shield has been enhanced in 2017 by the EU\u2013US Umbrella Agreement which fixes many of the issues with the Privacy Shield. While there are still valid criticisms it remains to be seen if this agreement will also be declared invalid as the predecessor.","title":"1. Introduction to the Cloud"},{"location":"lectures/1-cloud-intro/#what_is_a_server","text":"In a hurry? Servers: Nowadays either x86 or ARM architecture. Redundant, hot-swap hardware (power supply, fans, etc). Flat build profile for rack mounts. Out-Of-Bounds management interface for monitoring and remote management. Power consumption is a concern. Buying servers can take a long time and present an up-front investment. In older times, servers were completely different from the machines we used for regular work. Not just in weight and form factor, but in architecture. The landscape would stretch from SPARC and Power architectures to the x86 architecture we use in our PCs. Over time, however, the x86 architecture took over to the point where a server today, from an architectural standpoint, is exactly the same as the machine you are using right now. This means that you can copy the machine you are using onto a server and chances are it will run without any modification. The only notable exception is the rise of the ARM architecture which is popular in phones and tables and is known for its low power consumption. In recent years there has been a small but noticeable trend to run ARM in a datacenter. At home it may not matter if your computer uses 200 or 300 watts of power. In a datacenter, at the scale of hundreds, thousands or tens of thousands of servers, saving even a few percent of power will translate to huge cost savings. The difference in build is, however, quite apparent. While some servers, mainly built for office use, have the standard \u201ctower\u201d build, most servers have a flat profile designed to be mounted in racks as displayed on the picture. Since most servers are not high enough to take full size expansion cards (graphics cards, network cards, etc), servers may contain a separate removable component for these usually called a riser. An HP Proliant G5 server pulled out of its rack. Source: Wikipedia Server racks are standardized closets that have mounting screws for rails that allow pulling servers out even mid operation and replace components while the server is running. Servers also come with a high level of redundancy. While you may have a single power supply in your home computer, servers typically have two that are able to take power from different power inputs. This makes sure that the server keeps running even if one of the two power supplies fail, or if one of the two power inputs goes down. Also in contrast to your home setup these servers contain an Out-Of-Bounds Management interface that allows remote management of servers even when they are turned off. The hardware components built into the server report their health status to this OOB management interface which allows for the simultaneous monitoring of thousands of machines. Note This is a fairly generic description of servers. Different vendors may chose to leave out certain features of their more \u201cbudget\u201d line of servers, or call certain components differently. HP, for example, calls their OOBM \u201cIntegrated Lights Out\u201d, Dell \u201cDRAC - Dell Remote Access Control\u201d, etc. When it comes to purchasing servers larger companies tend to go with the same components for a longer period of time and they also buy support from the vendor. This sometimes includes hardware replacement done entirely by the vendor in the datacenter without the need for the customer to have staff on site. However, purchasing a specific set of components or ordering larger quantities of servers presents a logistics challenge and can sometimes take up to 3-4 months. Buying hardware is also an up-front investment which is hard to justify when demands change rapidly. What components are redundant in a server? Power supply CPU RAM Fan OOBM What is the purpose of an OOBM? Remotely manage a server Receive hardware malfunction alerts","title":"What is a Server?"},{"location":"lectures/1-cloud-intro/#the_anatomy_of_a_datacenter","text":"In a hurry? Datacenter components: Racks to house servers. Larger customers have cages for their racks. Cabling under the floor. Redundant cooling, fire suppression systems and power supply. Some datacenters provide internet connectivity. Eco friendliness is becoming a factor. Since the cloud is just somebody else's computer, that computer needs to be hosted somewhere. Servers are almost exclusively hosted in datacenters. Let's take a look at what is involved in running a datacenter. First of all, as mentioned above, most servers are going to be rack-mounted so you need a bunch of racks. These racks are installed in rows, often with a fake floor to allow for cabling to go under the floor. A datacenter with racks. Source: Wikipedia Since servers produce a lot of heat, a datacenter also requires cooling. There are a variety of ways to solve cooling, some are more \u201cgreen\u201d than others. Some datacenters, for example, opt to install a \u201ccold aisles\u201d where the cold air is pumped between two rack rows and is pushed through the racks to cool the servers. Apart from cooling, datacenters also require automated fire suppression systems simply because of the amount of electricity going through. Datacenters usually go with a non-destructive fire suppression system such as lowering the oxygen content of the air enough to stop the fire. All critical systems in a datacenter (power, cooling, fire suppression systems) are usually built in a redundant fashion because the loss of either of those systems will potentially mean a complete shutdown for the datacenter. Datacenter operators usually have further contingency plans in place, too, such as a UPS (battery) system, diesel generator, fuel truck on standby, hotline to the fire department, etc. to make sure the datacenter can keep its required uptime. On the networking side of things, matters get slightly more complicated. Some datacenter providers also offer you the ability to use their network uplink (also redundant), but larger customers will prefer to host their own networking equipment and negotiate their own internet uplink contracts. Since there is no generic rule for how datacenters handle this, we will dispense with a description. It is also worth noting that larger customers (banks, cloud providers, etc) usually prefer to have their own racks in a separated gated area called a \u201ccage\u201d to which they control access.","title":"The Anatomy of a Datacenter"},{"location":"lectures/1-cloud-intro/#the_anatomy_of_the_internet","text":"In a hurry? Internet: IP ranges are advertised using BGP. Providers connect direcly or using internet exchanges. 16 global providers form the backbone of the internet (tier 1). Once the physical infrastructure is set up there is also the question of how to connect to the Internet. As mentioned before, networks can be very complicated and there is no one size fits all solution. Smaller customers will typically use the network infrastructure provided by the datacenter while larger customers will host their own network equipment. Again, generally speaking racks will be equipped with a Top-of-Rack switch to provide layer 2 (Ethernet) connectivity between servers. Several ToR may have interconnects between each other and are usually connected to one or more routers. Routers provide layer 3 (IP) routing to other customers in the same datacenter, internet exchange , or may be connected via dedicated fiber to another provider. Note If you are not familiar with computer networks we recommend giving the Geek University CCNA course a quick read . While you will not need everything, you will have to understand how IP addresses, netmasks, etc work in order to pass this course. Providers on the internet exchange data about which network they are hosting using the Border Gateway Protocol . Each provider's router announces the IP address ranges they are hosting to their peer providers, who in turn forward these announcements in an aggregated form to other providers. Providers have agreements with each other, or with an Internet Exchange, about exchanging a certain amount of traffic. These agreements may be paid if the traffic is very asymmetric or one provider is larger than the other. Alternatively providers can come to an arrangement to exchange traffic for free. Internet exchanges facilitate the exchange between many providers for a modest fee allowing cost-effective exchange of data. Depending on the exchange the rules are different. Local exchanges, for example, may only allow advertising local (in-country) addresses, while others are built for a specific purpose. Generally speaking providers can be classified into 3 categories. Tier 1 providers are the global players that are present on every continent. They form the backbone of the Internet. At the time of writing there are 16 such networks . Tier 2 are the providers who are directly connected to the tier 1 providers, while tier 3 is everyone else.","title":"The Anatomy of the Internet"},{"location":"lectures/1-cloud-intro/#software_stack","text":"In a hurry? Software stack: Virtualization. Operating system. Application runtime. Application. The purpose of all this is, of course, to run an application. Each server hosts an operating system which is responsible for managing the hardware. Operating systems provide a simplified API for applications to do hardware-related operations such as dealing with files or talking to the network. This part of the operating system is called the kernel. Other parts form the userland. The userland includes user applications such as a logging software. Specifically on Linux and Unix systems the userland also contains a package manager used to install other software. Modern x86 server CPUs (and some desktop CPUs) also have a number of features that help with virtualization. Virtualization lets the server administrator run multiple guest operating systems efficiently and share the server resources between them. Did you know? You can find out if an Intel CPU supports hardware virtualization by looking for the VT-x feature on the Intel ARK . Unfortunately AMD does not have an easy to use list but you can look for the AMD-V feature on AMD CPUs. Note Virtualization is different from containerization (which we will talk about later) in that with virtualization each guest operating system has its own kernel whereas containers share a kernel between them. There is one more important aspect of finally getting an application to run: the runtime environment. Except for a few rare occasions applications need a runtime environment. If the application is compiled to machine code they still need so-called shared libraries. Shared libraries are common across multiple applications and can be installed and updated independently from the application itself. This makes for a more efficient update process, but also means that the right set of libraries need to be installed for applications. If the applications are written higher level languages like Java, Javascript, PHP, etc. they need the appropriate runtime environment for that language. One notable exception to the runtime environment requirement is the programming language Go . Go compiles everything normally located in libraries into a single binary along with the application. This makes it exceptionally simple to deploy Go applications into containers.","title":"Software Stack"},{"location":"lectures/1-cloud-intro/#the_cloud","text":"In a hurry? Typical cloud features: API Dynamic scaling Can be classified into IaaS, PaaS and SaaS IaaS service offerings typically include: Virtualization. Network infrastructure. Everything required to run the above. PaaS service offerings typically include a managed service ready to be consumed by a developer. SaaS service offerings typically include a managed service ready to be consumed by a non-technical end user. All of the previously discussed things were available before the \u201ccloud\u201d. You could pay a provider to give you access to a virtual machine where you could run your applications. What changed with the cloud, however, is the fact that you no longer had to write a support ticket for changed and everything became self service. The cloud age started with an infamous e-mail from Jeff Bezos to his engineers in 2002 forcing them to use APIs to exchange data between teams. The exact e-mail is no longer available but it went along these lines: 1) All teams will henceforth expose their data and functionality through service interfaces. 2) Teams must communicate with each other through these interfaces. 3) There will be no other form of interprocess communication allowed: no direct linking, no direct reads of another team\u2019s data store, no shared-memory model, no back-doors whatsoever. The only communication allowed is via service interface calls over the network. 4) It doesn\u2019t matter what technology is used. HTTP, Corba, Pubsub, custom protocols \u2014 doesn\u2019t matter. 5) All service interfaces, without exception, must be designed from the ground up to be externalizable. That is to say, the team must plan and design to be able to expose the interface to developers in the outside world. No exceptions. 6) Anyone who doesn\u2019t do this will be fired. This marked the beginning of Amazon Web Services the first and also the most successful public cloud offering. The first public release of AWS was in 2004 with SQS their message queue service, and got completely overhauled in 2006 where the Elastic Compute (EC2) and the Simple Storage Service (S3) service made its first public appearance. The APIs provided by cloud providers allow for a large amount of flexibility. If new servers are needed they can be launched within a few minutes. If there are too many servers they can be deleted. The same goes for other services: with the API (and the appropriate billing model) comes flexibility to adapt to change. The other factor that makes it easy to adapt to change is of course the fact that these services are managed. The cloud customer doesn't need to hire a team of engineers to build a database service, for example, it can be consumed without knowing how exactly the database is set up. Generally speaking, cloud services can be classified into three categories: Infrastructure as a Service (IaaS) providing virtual machines and network infrastructure, Platform as a Service (PaaS) offering services for developers to use, and Software as a Service (SaaS) offering end-user services. Note SaaS will not be discussed in this course.","title":"The Cloud"},{"location":"lectures/1-cloud-intro/#infrastructure_as_a_service_iaas","text":"The most basic of cloud service offerings is IaaS. IaaS means that the cloud provider will manage the infrastructure used by the customer. Infrastructure in this sense means the ability to manage (provision, start,stop) virtual machines. In very rare cases some providers also offer \u201cbare metal\u201d machines in this fashion. However, most bare metal providers do not offer a true IaaS as machines cannot be ordered using an API, have an up-front fee and are billed on a monthly basis. IaaS also includes the network connectivity to the Internet. Almost all IaaS providers also offer a built-in firewall, virtual private networks (VPC) that can be used to connect virtual machines together without sending the traffic over the Internet, and other network services. Note that network services can differ greatly. For example, some providers implement private networks on a regional basis while other providers offer private networks that can be used to connect virtual machines that are located in different regions.","title":"Infrastructure as a Service (IaaS)"},{"location":"lectures/1-cloud-intro/#managed_services_paas","text":"In a hurry? Managed services: The provider is typically responsible for provisioning, backups, monitoring and restoring the service if needed. Low entry cost. Little in-house know-how required. Vendor lock-in for non-standard services. If problems arise they can be hard to debug. Apart from offering virtual machines and network infrastructure as a service many cloud providers also offer additional services that are typically used by developers such as managed databases. These services are managed in the sense that the developer does not need to run the operating system and the database software itself. However, the developer has to configure the database such that it works according to the needs of the application. The provider takes over duties like installing the service, making backups, monitoring, and restoring the service in case of an outage. Did you know? There are purely PaaS providers that do not offer IaaS, only a developer-friendly platform to run software on. Your run of the mill average PHP web hosting provider does qualify if you can order the service using an API. One notable example that rose to prominence is Heroku . The pricing of these managed services varies greatly but they usually follow the cloud model. Most cloud providers offer at least a low cost or even free entry version that has a small markup on top of the IaaS costs. The massive benefit of using these managed services is, of course, that you do not need to have in-house knowledge about how to operate them. You don't need an in-house database operator, you \u201cjust\u201d need to know how to set up the database in the cloud. This lets your company focus on the core business they are trying to build and outsource the know-how required to build these services on top of the IaaS layer. As you might imagine managed servies also have downsides. Most importantly they present a clear vendor lock-in. This means that if you want to move to a different cloud provider you will have a hard time doing so if you are using one of the more specialized services. In other words the level of standardization across providers matters. It is also worth mentioning that managed services tend to work well for a large number of customers but a few number of customers can run into hard to debug problems. This debugging difficulty arises out of the inherent opacity of the services: you, the customer, don't see what's happening on the IaaS layer. If a database, for example, fills the available bandwidth may not be notified and are left guessing why your database misbehaves.","title":"Managed Services (PaaS)"},{"location":"lectures/1-cloud-intro/#business_models","text":"In a hurry? Billing models: IaaS is typically priced per-second based on the instance size. PaaS can be priced similar to IaaS but also per-request or data volume. Data volume is typically priced based on usage. Some providers charge for data volume even for internal traffic. The flexibility of cloud providers comes from their usage-based pricing. This can vary depending on the type of service used. For example, virtual machines (IaaS) is typically priced on a per-second basis. When you start a virtual machine for only 5 minutes you will only pay for 5 minutes of runtime. This lets you optimize the usage costs of the cloud if you use automation to start and stop machines or services on demand. Typically the cost savings are realized in these scenarios:","title":"Business Models"},{"location":"lectures/1-cloud-intro/#on-off_usage","text":"This usage type is typical for batch processing. Machines are only started when there is a workload (e.g. video conversion, machine learning training job, etc.)","title":"On-off usage"},{"location":"lectures/1-cloud-intro/#growth_usage","text":"Projects that see a growth curve often opt to use a cloud provider as well since buying hardware in larger quantities typically takes 2-4 months.","title":"Growth usage"},{"location":"lectures/1-cloud-intro/#sudden_spike_usage","text":"Cloud providers can also be useful if a service encounters a sudden load spike. This is typically the case for webshops around Black Friday and other sales events.","title":"Sudden spike usage"},{"location":"lectures/1-cloud-intro/#periodic_spike_usage","text":"Almost every service has usage spikes depending on the time of day. This can be used to scale the service up and down.","title":"Periodic spike usage"},{"location":"lectures/1-cloud-intro/#per-request_billing","text":"IaaS services are typically priced based on an allocated amount of resources determined at the start of the virtual machines. Some PaaS services also use this billing model. (Typically Databases as a Service.) Other PaaS services often opt for a per-request or a data volume based billing approach.","title":"Per-request billing"},{"location":"lectures/1-cloud-intro/#cost_planning_with_the_cloud","text":"Cost planning is an important part of the job of a cloud architect. Depending on the billing model the cloud provider adopts this can be fairly simple to almost impossible. As a rule of thumb the billing model of larger cloud providers (AWS, Azure, Google Cloud) is more complex than smaller cloud providers (DigitalOcean, Exoscale, Upcloud, etc). It is worth noting that data transfer is typically charged based on volume. Some cloud providers even charge for internal traffic between availability zones.","title":"Cost planning with the cloud"},{"location":"lectures/1-cloud-intro/#cost-comparison_with_on_premises_systems","text":"One of the most important cases for cost analysis will be the comparison with on premises architecture. It is quite common to create a cost comparison where the investment required for a hardware purchase is compared with the cloud cost over 3 or 5 years. These comparisons can be quite misleading because they often don't contain any cost attribution for the required work and they also do not use the advantages of the cloud in terms of scaling. Without these factors a hardware purchase will almost always be cheaper when calculated over 5 years and be roughly equal when compared over 3 years.","title":"Cost-comparison with on premises systems"},{"location":"lectures/1-cloud-intro/#private_vs_public_cloud","text":"In a hurry? Private cloud: Hosted on-premises or in the public cloud using only private connections. Large cloud providers offer their services \u201cin a box\u201d to host yourself. Most cloud applications will reside on a public cloud and be accessible from the Internet. However, for some usecases access over the public Internet is not desirable or even strictly forbidden by laws or regulations. In order cases companies may decide that it is their policy that certain systems must never be connected to the public Internet, or only connect via a self-hosted gateway. In these cases a private cloud is desirable to minimize the risk of cross-contamination or data exposure due to other tenants being on the same infrastructure. Such cross-contamination is not uncommon, for example in recent years there have been a litany of CPU bugs such as Meltdown and Spectre . Public cloud providers have extended their offers to address these concerns. Their service offerings now include dedicated hypervisors to mitigate CPU bugs, the ability to connect the cloud bypassing the Internet , or accessing the cloud API's from a private network . Some cloud providers even went as far offering a self-hosted setup where the public cloud offering can be brought on-premises. These features, especially the connectivity-related features, also allow for the creation of a hybrid cloud where a traditional infrastructure can be connected to the cloud. This gives a company the ability to leverage the flexibility of the cloud but also keep static / legacy systems for financial or engineering reasons.","title":"Private vs. Public Cloud"},{"location":"lectures/1-cloud-intro/#automation","text":"In a hurry? Automation: Documents how a cloud is set up. Gives a reproducible environment. Allows for spinning up multiple copies of the same environment. Not all tools are equal. Some tools allow for manual changes after running them (e.g. Ansible), others don't (e.g. Terraform). As you can see from the previous sections cloud computing doesn't necessarily make it simpler to deploy an application. In this regard setting up a server and not documenting it is the same as setting up a cloud and not documenting it. In the end in both cases modifications will be hard to carry out later since details of the implementation are lost. It will also be hard to create a near-identical copy of the environment for development, testing, etc. purposes. This is partially due to the lack of documentation, and partially because of the manual work involved. In the past this problem was addressed by cloning virtual machines which was an approach with limited success as customizations were hard to make. The rise of API's brought a much welcome change: automation tools such as Puppet, Ansible and Terraform not only automate the installation of a certain software but also document how the setup works. When engineers first go into automation they tend to reserve a part of the work to be done manually. For example they might use Ansible to create virtual machines and install some basic tools and install the rest of the software stack by hand. This approach is workable where the software stack cannot be automated but should be avoided for software where this is not the case as the lack of automation usually also means a lack of documentation as described above. Automation tools are also not equally suited for each task. Terraform, for example, expects full control of the infrastructure that is created by it. Manual installation steps afterwards are not supported. This approach gives Terraform the advantage that it can also remove the infrastructure it creates. Removing the infrastructure is paramount when there are multiple temporary environments deployed such as a dev or testing environment. Ansible, on the other hand, allows for manual changes but does not automatically implement a tear-down procedure for the environment that has been created. This makes Ansible environments, and Ansible code harder to test and maintain but more suited for environments where traditional IT is still a concern.","title":"Automation"},{"location":"lectures/1-cloud-intro/#regulation","text":"In a hurry? GDPR: Applies to all companies, world-wide, that handle personal data of EU citizens. Companies must keep track of why and how data is handled. Data subjects have wide ranging, but not unlimited rights to request information, correction and deletion of data. Data breaches have to be reported and may carry a fine. The right of deletion means that backups have to be set up in such a way that data can be deleted. CLOUD Act: US authorities can access data stored by US companies in Europe. DMCA: Copyright infringements can be solved by sending a DMCA takedown notice to the provider. Providers have to restore the content if the uploader sends a counter-notification. CDA Section 230: Shields providers from liability if their systems host illegal content which they don't know about. Privacy Shield: Establishes a legal basis for transfering data from Europe to the US.","title":"Regulation"},{"location":"lectures/1-cloud-intro/#general_data_protection_regulation_gdpr_eu","text":"The GDPR (or DSGVO in German-speaking countries) is the general overhaul of privacy protections in the EU. The GDPR replaces much of the previously country-specific privacy protections present in the EU.","title":"General Data Protection Regulation (G.D.P.R., EU)"},{"location":"lectures/1-cloud-intro/#jurisdiction","text":"The GDPR applies to all companies that deal with the data of EU citizens around the globe. This is made possible by trade agreements previously already in place. All companies that handle the data of EU citizens even if the companies are not located in the EU.","title":"Jurisdiction"},{"location":"lectures/1-cloud-intro/#structure","text":"Data subject: the person whose data is being handled. Personal identifiable data: (PI) data that makes it possible to uniquely identify a data subject. PI can also be created when previously non-PI data is combined to build a profile. Data controller: The company that has a relationship with the data subject and is given the data for a certain task. Data processor: A company that processes data on behalf of the data controller. The data processor must have a data processing agreement (DPA) with the data controller. IaaS and PaaS cloud providers are data processors.","title":"Structure"},{"location":"lectures/1-cloud-intro/#purposes_of_data_processing","text":"One new limitation the GDPR brings to privacy regulation is the fact that there are fundamental limits to data processing. You, the data controller, cannot simply collect data for one purpose and then use that same data for another purpose. The purposes to which data is collected have to be clearly defined and in several cases the data subject has to provide their explicit consent. (In other words it is not enough to add a text to the signup form that says that they agree to everything all at once.) Legal grounds to data processing are detailed in Article 6 of the GDPR . These are: If the data subject has given their explicit consent to process their data for a specific purpose. If the processing is necessary to perform a contract with the data subject. In other words you do not need additional consent if you are processing data in order to fulfill a service to the data subject. To perform a legal obligation . For example, you have to keep an archive of your invoices to show in case of a tax audit. To protect the vial interests of the data subject or another natural person . To perform a task which is in the public interest , or if an official authority has been vested in the data controller. For the purposes of a legitimate interest of the data controller (with exceptions). This might seem like a catch-all but courts have defined legitimate interests very narrowly. The data controller must show that the legitimate interest exists and cannot be achieved in any other way than with the data processing in question. One good example for the legitimate interests clause would be a webshop that is collecting data about their visitors in order to spot fraudulent orders.","title":"Purposes of data processing"},{"location":"lectures/1-cloud-intro/#data_subjects_rights","text":"Chapter 3 of the GDPR deals with the rights of the data subject. These are: The right to be informed. This right lets the data subject request several pieces of information: The purposes of processing. The categories of personal data concerned. Who received their data, in particular recipients in third countries or international organisations. How long the data will be stored. The right to request modification, deletion or restricting the processing of the personal data in question. The right to log a complaint. The source of the personal data if not provided by the data subject themselves. If and how automated decision-making, profiling is taking place. The right to fix incorrect data. The right of deletion. \u201cThe right to be forgotten.\u201d This right puts several architectural limites on cloud systems as, for example, backups must be built in such a way that deletion requests are repeated after a restore. Note that this right is not without limits, legal obligations, for example, override it. The right to restrict processing. If a deletion cannot be requested the data subject can request that their data should only be processed to the purposes that are strictly required. The right to data portability. The data subject has to be provided a copy of their data in a machine-readable form. The right to object automated individual decision-making and profiling.","title":"Data subjects' rights"},{"location":"lectures/1-cloud-intro/#data_breaches","text":"Chapter IV Section 2 is probably the first legislation around the world that explicitly requires security of data processing from companies that handle personal data. Article 33 of this chapter specifically requires that data breaches must be disclosed to the supervisory authorities. Supervisory authorities in turn have the right to impose fines up to 4% or 20.000.000 \u20ac of the global revenue of a company. This means that companies can no longer sweep data breaches under the rug and must spend resources to secure their data processing. In the scope of the cloud this means that our cloud environment has to be set up in a secure way to avoid data breaches.","title":"Data breaches"},{"location":"lectures/1-cloud-intro/#clarifying_lawful_overseas_use_of_data_act_cloud_2018_usa","text":"The CLOUD Act, or House Rule 4943 is the latest addition to the US legislation pertaining to cloud providers. This act says that a cloud provider must hand over data to US authorities if requested even if that data is stored in a different country. This act has been widely criticized and is, according to several legal scholars, in contradiction of the GDPR. The large US cloud providers have opened up subsidiaries in the EU in order to try and shield their european customers from this act and a few purely EU cloud providers have also capitalized on this. It remains to be seen how effective this move is.","title":"Clarifying Lawful Overseas Use of Data Act (C.L.O.U.D., 2018, USA)"},{"location":"lectures/1-cloud-intro/#digital_millennium_copyright_act_dmca_1998_usa","text":"The Digital Millenium Copyright Act clarifies how copyright works in the USA. Since the USA is part of the WIPO copyright treaty and a significant amount of providers are based in the USA nowadays all countries align themselves with the DMCA when it comes to dealing with online copyright infringement. Title II of the DMCA creates a safe harbor for online service providers against copyright infringement committed by their users. However, they have to remove infringing content as soon as they are properly notified of it via a DMCA takedown notice. The author of a DMCA takedown notice must swear by the penalty of perjury that they are, or are acting on behalf of the copyright owner. (If they were to make this pledge in bad faith they could end up in prison.) If the DMCA takedown notice has been sent erroneously the original uploader is free to send a counter-notification, also swearing under the penalty of perjury. In this case the provider notifies the original sender and restores the content. The original sender then has to take the uploader to court to pursue the matter further.","title":"Digital Millennium Copyright Act (D.M.C.A., 1998, USA)"},{"location":"lectures/1-cloud-intro/#communications_decency_act_section_230_cda_1996_usa","text":"This section of of the CDA is very short: No provider or user of an interactive computer service shall be treated as the publisher or speaker of any information provided by another information content provider. To someone not well versed in US legal lingo this may be sound like gibberish but it is, in fact, one of the most important pieces of legislation pertaining to operating platforms in the cloud. In US legal history the speaker or publisher of certain information is responsible for the information being provided. Before the CDA any provider hosting illegal content would be responsible for said content. The CDA changed that by shielding providers from liability. This included web hosting providers as well as newspapers that had a content section. The CDA is not limitless, providers still have to remove infringing content if notified. They may also employ moderation proactively.","title":"Communications Decency Act, Section 230 (C.D.A., 1996, USA)"},{"location":"lectures/1-cloud-intro/#privacy_shield_2016_eu-us","text":"The Privacy Shield agreement is the successor to the failed International Safe Harbor Privacy Principles framework and is intend to protect the privacy of EU citizens when using US services or their data is transferred to the USA. The Privacy Shield has been enhanced in 2017 by the EU\u2013US Umbrella Agreement which fixes many of the issues with the Privacy Shield. While there are still valid criticisms it remains to be seen if this agreement will also be declared invalid as the predecessor.","title":"Privacy Shield (2016, EU-US)"},{"location":"lectures/2-capabilities/","text":"","title":"2. Cloud capabilities"},{"location":"lectures/3-iaas/","text":"Download PPTX \ud83d\udcbb Download MP3 \ud83c\udfa7 Download M4B \ud83c\udfa7 Watch Video \ud83c\udfac Infrastructure as a Service Infrastructure as a Service, or IaaS is a service offering by most cloud providers that provides virtual machines and the accompanying infrastructure as a service. This lecture will discuss the details of how an IaaS service is built. Virtual machines # Virtualization is a surprisingly old technology. The first virtualized system was the IBM System/370 mainframe with the VM/370 operating system in 1972. The system was different from how we understand virtualization today, but the goal was the same: separate workloads from each other. When you think about mainframes you have to consider that these machines were very expensive and machine time was a scarce resource. Most programs back in those days were batch jobs . They processed a large set of data at once and then terminated. Initially CPUs in personal computers did not have application separation. The x86 line of Intel CPUs only received the protected mode feature with the 80286 in 1982. The operating system (MS/DOS) would run in in real mode and applications could then switch into the new mode to isolate applications from each other. One such application making use of the new mode was Windows that ran on top of MS/DOS. Protected mode introduced the concept of rings in the CPU. The operating system kernel would run in ring 0, device drivers would run in ring 1 and 2 while applications would run in ring 3. The lower ring number meant the higher privilege level. Note Device drivers today typically run on ring 0 instead of 1 or 2. This ring system allowed the operating system to restrict the higher ring numbers from accessing certain functions or memory locations. However, most applications in the day would violate the restrictions of protected mode and could not run in the new mode. Note If you try and set up a really old computer game like Commander Keen in DOSBox you will realize that you have to provide the game itself with very hardware-specific settings. You will, for example, have to provide details for your sound card. This is because the game itself incorporated sound card drivers for Sound Blaster 16 or Gravis Ultrasound cards. A game that would do this could not run in protected mode. To work around the problems with protected mode the 80386 successor introduced virtual mode . The new virtual 8086 mode (VM86) introduced a number of compatibility fixes to enable running old real mode programs in a multitasking environment such as Windows without problems. For instance the CPU would create a simulated virtual memory space the program could write to and translate the virtual addresses to physical addresses internally. It would also capture sensitive instructions and turn them over for control to the kernel. Note VM86 does not capture every instruction the application runs in virtual mode, only the sensitive CPU instructions. This enables legacy applications to run at a reasonable speed. In the mid 2000's CPUs became so powerful that it made sense to not only virtualize applications but whole operating systems including their kernel. This allowed multiple operating systems to run in parallel. However, without CPU support only software virtualization could be achieved. In other words early virtualization software had to simulate a CPU in ring 0 to the guest operating system. Some virtualization techniques, such as Xen required the guest operating system to run a modified kernel to facilitate them running in ring 3. Others employed a number of techniques we won't go into here. While initially expensive hardware support followed suit. In 2005 Intel added the VT-x (Vanderpool) feature to its new Pentium 4 CPUs followed by AMDs SVM/AMD-V technology in 2006 in the Athlon 64, Athlon 64 X2, and Athlon 64 FX processors. VT-x and AMD-V added new ring -1 to accommodate hypervisors . This new ring allowed for separation between several operating systems running at ring 0. Later CPU releases added features such as Direct Input/Output virtualization , network virtualization or even graphics card virtualization. These features allowed for more efficient virtualization and sharing hardware devices between several virtual machines. Note Intel also introduced a ring -2 for the Intel Management Engine, a chip that functions as an OOBM in modern Intel chips. The ME runs its own operating system, a MINIX variant and has been the target of severe criticism for its secrecy and power over the machine. Several bugs have been found in the ME that let an attacker hide a malware inside the ME. Virtualization also gave rise to Infrastructure as a Service. AWS was the first service that offered virtual machines as a service starting in 2006 with a Xen-based offer. They not only offered virtual machines but they did so that a customer could order or cancel the service using an Application Program Interface. This allowed customers to create virtual machines as they needed it and they were billed for it on an hourly basis. (Later on AWS and other cloud providers moved to a per-second billing.) The presence of an API makes the difference between IaaS and plain old virtual machines as a service. IaaS allows a customer to scale their application dynamically according to their current demand. Typical instance types # When the cloud became popular in the late 2000s several providers attempted to offer a service that was fully dynamic in their sizes. The customer could set how many GB of RAM they needed and how many CPU cores. However, this model has been phased out by most providers since it is difficult to manage such a dynamic environment. Instead cloud providers nowadays opt to offer fixed machine sizes (think of t-shirt sizes). To accommodate high-CPU and high RAM workloads there are several different instance types, typically: Shared CPU: These are small instances where a single CPU core is shared between multiple virtual machines, sometimes leading to high steal time . Sometimes this offering includes a burst capability (such as the Amazon T instances) where a VM can temporarily use more CPU. Standard, dedicated core CPU: These instance types receive one or more physical cores leading to a more stable performance without the ability to burst beyond their limits. High CPU: These instance types are usually hosted on physical servers that have a very high CPU to RAM ratio. Accordingly, the virtual machine offering includes more CPU than the standard offering. High RAM: This offering is the exact opposite of the high CPU offering. The machines on offer here include more RAM with very little CPU. Storage: These instance types contain large amounts of local storage (see below in the storage section). Hardware-specific: These instance types offer access to dedicated hardware such as graphics cards (GPUs) or FPGAs. Automation # As discussed before, that makes an IaaS cloud provider a cloud provider is the fact that they offer an API to automate the provisioning and deprovisioning of virtual machines as needed. However, that's not all. Simply starting a virtual machine is not enough, the software needs to be installed in it. Initially this problem would be solved by creating templates for the operating system that launches. In larger cloud setups these templates included a pre-installed agent for configuration management that would report to a central service and fetch its manifest of software to install. Thankfully in the last decade a lot has happened and Cloud Init has established itself as a defacto standard in the IaaS world. Every cloud provider nowadays offers the ability to submit a user data field when creating a virtual machine. This user data file is read by Cloud Init (or its Windows alternative Cloudbase Init ) and is executed at the first start of the virtual machine. A DevOps engineer can simply inject a script that runs at the first start that takes care of all the installation steps required. Tools like Terraform or Ansible assist with managing the whole process of provisioning the virtual machines and supplying it with the correct user data script. Virtual machine pools # One other use of user data are virtual machine pools. Each cloud provider adopts a different name for them, ranging from instance pools to autoscaling groups. The concept is the same everywhere: you supply the cloud with a configuration how you would like your virtual machines to look like and the cloud will take care that the given number of machines are always running. If a machine crashes or fails a health check the cloud deletes the machine and creates a new one. The number of machines in a pool can, of course, be changed either manually or in some cases automatically using rules for automatic scaling. Combined with the aforementioned user data this can be a very powerful tool to create a dynamically sized pool of machines and is the prime choice for creating a scalable architecture. These pools are often integrated with the various load-balancer offerings cloud providers have in their portfolio to direct traffic to the dynamic number of instances. Some cloud providers integrate them with their Functions as a Service offering as well allowing you to run a custom function whenever a machine starts or stops. This can be used to, for example, update your own service discovery database. Storage # Hopefully we have managed to convey the dynamic nature of the cloud in our description so far. This brings up the question of where data should be stored. This section will explain the different storage options that you can use to store files directly from your virtual machine. We will touch on databases and other storage options in a later section. How do filesystems work? # Before we dive into the different options let's take a look at how filesystems work. In a non-cloud environment you would insert a hard drive into your computer. This hard drive can either be a spinning disk (often referred to as HDD) or a solid-state drive (SSD). Spinning disks have a read head that goes over the platter that spins under it. This means that information access on HDD requires the head to get into the correct position to read the data which takes time. This positioning is done by the controller chip that's on the disk itself. SSD's have different problems. Information access is instant since any block can be accessed immediately but the chip on the disk still needs to keep track of which cell is unusable, caching, etc. In other words the controller chip deals with the low-level functions on the disk for both HDD and SSD. The drive itself is accessible via either a SATA (Serial ATA) or a SAS (Serial Attached SCSI) connection. In case of servers these disks are often assembled into a RAID array for redundancy and higher performance. Most RAID controllers will also implement caching which often also requires a built-in battery unit to function properly. In both cases the data is stored in blocks of fixed sizes (such as 4 kB). The filesystem itself contains the mapping of which file consists of which blocks on the disk. This is an important distinction: block storage solutions, no matter if they are network-bound or use a more classic connection such as fibre channel only know about the blocks on the disk. They do not know anything about files. Since filesystems are complex beasts block storage devices can only be used by a single machine at any given time. Network filesystems on the other hand are more resource intensive but can track which file is open from which machine and can be used in a distributed fashion. Local Storage # The most obvious storage option, of course, is a local storage. By local storage we mean a hard drive (SSD or HDD) that is integrated directly into the machine that's running the hypervisor. This storage option has the benefit of offering a high performance disk at a relatively affordable price. The drawback is that the disk is local. If the physical machine running your VM has a problem your data may be lost. For all storage options the implementation of backups is critical but with local storage building redundancy on top of the virtual machine may be required to build a reliable system. Network Block Storage # Network block storage means a block storage that is delivered over the network. When talking about network we are not only talking about your standard IP / Ethernet network but also a direct SCSI over Fibre Channel infrastructure. The latter is often used in expensive enterprise storage systems that provide attachable disks to several physical machines. TODO: add illustration for fibre channel storage systems The simplest and easiest to set up network block storage is probably ISCSI, a protocol that runs the SCSI storage protocol over a commodity IP network. In other words one computer can take a block device and let another computer use it. However, as mentioned previously in this case the computer using the ISCSI device is in full control over the files that are stored on the storage device. It is also worth mentioning that ISCSI does not guarantee any redundancy beyond the RAID or storage system the offering computer already has. More advanced storage systems such as Ceph RBD or replicated enterprise storage systems offer redundancy such that when one storage instance fails others can take its place without outage. Cloud provider offerings such as EBS by Amazon also offer this kind of redundancy with severe limitations as to how many EBS volumes can be attached to a virtual machine. Network File Systems # In contrast to network block storage network file systems offer access to data not on a block level, but on a file level. Over the various network file system protocols machines using these file systems can open, read and write files, and even place locks on them. The filesystem has to keep track of which machine has which file open, or has locks on which file. When machine edit the same file in parallel the filesystem has to ensure that these writes are consistent. This means that network file systems are either much slower than block-level access (e.g. NFS ) or require a great deal more CPU and RAM to keep track of the changes across the network (e.g. CephFS ). Object storage # Object storage systems are similar to network file systems in that they deal with files rather than blocks. However, they do not have the same synchronization capabilities as network file systems. Files can generally only be read or written as a whole and they also don't have the ability to lock a file. While object storages technically can be used as a filesystem on an operating system level for example by using s3fs this is almost always a bad idea due to the exceptionally bad performance and stability issues. Operating system level integration should only be used as a last resort and object storages should be integrated on the application level. We will discuss object storage services in detail in our next lesson. Network # The next big topic concerning IaaS services is networks. Before we go into the cloud-aspect let's look at how the underlying infrastructure is built. As indicated in the first lecture it is strongly recommended that you familiarize yourself with the basics of computer networking, such as the Ethernet, IP and TCP protocols as you will need them to understand this section. How cloud networks are built # So, let's get started. Imagine a data center from the first lecture. Your task is to build an IaaS cloud provider. You put your servers that will serve as your hosts for virtual machines in the racks. These servers will be connected to the Top-of-Rack switches (yes, two for redundancy) using 10 GBit/s network cables. The switches are themselves connected among each other and across racks with several 100 GBit/s. Now comes the tricky part: how do you create private networks for each customer? One option would be to use 802.11Q VLAN tags separating each network from each other. However, that limits you to 2 12 = 4096 private networks. That may seem like much but for a public cloud provider that is not sufficient. Therefore, public cloud providers use overlay protocols like VXLAN to work around this problem. We describe this not for you to learn but to highlight that public cloud networks are complex . A cloud provider cannot guarantee a fixed bandwidth between two virtual machines (unless they are put on the same physical server using affinity groups). This is one of the reasons that often the ideal setup for cloud architectures is one that uses multiple, medium-sized instances rather than few large sized ones. Underlying network architectures offered by cloud providers # When we look at the network offerings by cloud providers there are three types: Virtual machines receive private IP addresses and a gateway or load balancer handles the public-to-private IP translations. This is the case with the larger cloud providers such as AWS , Azure and GCP at the time of writing. Virtual machines have one public IP address on the first network interface and additional private networks can be attached as new, separate network interfaces. This is the case with most smaller IaaS providers such as DigitalOcean , Hetzner , Upcloud , or Exoscale . Fully dynamic network configuration that allows the customer to define their network setup and IP assignment dynamically. This is typically offered by IaaS providers that target enterprise customers who wish to migrate their classic on-premises infrastructure and require the flexibility they had when using their own hardware. This is the case with 1&1 IONOS . TODO: Vultr? Linode? Deutsche Telekom Cloud? Alibaba Cloud? TODO: add illustration Out of group 2 it is worth mentioning that the services that are available on the public network (firewalls, load balancers) are often not available on private networks. Firewalling # IaaS providers often also offer network firewalls as a service, included in the platform. Firewalls generally have two rule types: INGRESS (from the Internet or other machines to the current VM) and EGRESS (From the current VM to) everywhere else. Firewall providers often employ the concept of security groups . The implementation varies greatly, but in general security groups are a reusable set of rules that can be applied to a VM. For most cloud providers you will need to create an explicit rule allowing traffic to flow between two machines in the same security group. The advantage of security groups is that the rules can be made in such a way that they reference other security groups rather than specific IP addresses. For example, the database security group could be set to allow connections only from the appserver security group but not from anywhere else. This can help with the dynamic nature of the cloud since you do not need to hard-code the IP addresses of the application servers. TODO: add screenshot of security group configuration Network load balancers # Network load balancers are an option some cloud providers offer. In contrast to Application Load Balancers they do not offer protocol decoding (such as routing requests to backends based on the requested web address), they only balance incoming connections to a pool of backends. TODO add illustration Depending on the cloud provider in question network load balancers may or may not offer terminating encrypted connections (SSL/TLS), and may be bound to virtual machine pools. It is also cloud provider specific if load balancers are offered in private networks or not. When designing an architecture it is worth considering if the real IP address of the connecting client will be needed. If the backend needs to know the real IP address of the client and the network load balancer handles SSL/TLS termination that combination may not be suitable for the task unless a specific trick such as the proxy protocol from Digital Ocean . Network load balancers without SSL/TLS termination should, in general, make the client IP available to the backends. When talking about load balancers an interesting question is the load balancing strategy. Most load balancers support either round robin (selecting the next backend in the list) or source hashing (routing the same connecting IP to the same backend). Most load balancers also support health checks to take backends that are not able to serve traffic out of the rotation. VPNs, private interconnects, and routing services # DNS # Monitoring # Automation #","title":"3. Infrastructure as a Service"},{"location":"lectures/3-iaas/#virtual_machines","text":"Virtualization is a surprisingly old technology. The first virtualized system was the IBM System/370 mainframe with the VM/370 operating system in 1972. The system was different from how we understand virtualization today, but the goal was the same: separate workloads from each other. When you think about mainframes you have to consider that these machines were very expensive and machine time was a scarce resource. Most programs back in those days were batch jobs . They processed a large set of data at once and then terminated. Initially CPUs in personal computers did not have application separation. The x86 line of Intel CPUs only received the protected mode feature with the 80286 in 1982. The operating system (MS/DOS) would run in in real mode and applications could then switch into the new mode to isolate applications from each other. One such application making use of the new mode was Windows that ran on top of MS/DOS. Protected mode introduced the concept of rings in the CPU. The operating system kernel would run in ring 0, device drivers would run in ring 1 and 2 while applications would run in ring 3. The lower ring number meant the higher privilege level. Note Device drivers today typically run on ring 0 instead of 1 or 2. This ring system allowed the operating system to restrict the higher ring numbers from accessing certain functions or memory locations. However, most applications in the day would violate the restrictions of protected mode and could not run in the new mode. Note If you try and set up a really old computer game like Commander Keen in DOSBox you will realize that you have to provide the game itself with very hardware-specific settings. You will, for example, have to provide details for your sound card. This is because the game itself incorporated sound card drivers for Sound Blaster 16 or Gravis Ultrasound cards. A game that would do this could not run in protected mode. To work around the problems with protected mode the 80386 successor introduced virtual mode . The new virtual 8086 mode (VM86) introduced a number of compatibility fixes to enable running old real mode programs in a multitasking environment such as Windows without problems. For instance the CPU would create a simulated virtual memory space the program could write to and translate the virtual addresses to physical addresses internally. It would also capture sensitive instructions and turn them over for control to the kernel. Note VM86 does not capture every instruction the application runs in virtual mode, only the sensitive CPU instructions. This enables legacy applications to run at a reasonable speed. In the mid 2000's CPUs became so powerful that it made sense to not only virtualize applications but whole operating systems including their kernel. This allowed multiple operating systems to run in parallel. However, without CPU support only software virtualization could be achieved. In other words early virtualization software had to simulate a CPU in ring 0 to the guest operating system. Some virtualization techniques, such as Xen required the guest operating system to run a modified kernel to facilitate them running in ring 3. Others employed a number of techniques we won't go into here. While initially expensive hardware support followed suit. In 2005 Intel added the VT-x (Vanderpool) feature to its new Pentium 4 CPUs followed by AMDs SVM/AMD-V technology in 2006 in the Athlon 64, Athlon 64 X2, and Athlon 64 FX processors. VT-x and AMD-V added new ring -1 to accommodate hypervisors . This new ring allowed for separation between several operating systems running at ring 0. Later CPU releases added features such as Direct Input/Output virtualization , network virtualization or even graphics card virtualization. These features allowed for more efficient virtualization and sharing hardware devices between several virtual machines. Note Intel also introduced a ring -2 for the Intel Management Engine, a chip that functions as an OOBM in modern Intel chips. The ME runs its own operating system, a MINIX variant and has been the target of severe criticism for its secrecy and power over the machine. Several bugs have been found in the ME that let an attacker hide a malware inside the ME. Virtualization also gave rise to Infrastructure as a Service. AWS was the first service that offered virtual machines as a service starting in 2006 with a Xen-based offer. They not only offered virtual machines but they did so that a customer could order or cancel the service using an Application Program Interface. This allowed customers to create virtual machines as they needed it and they were billed for it on an hourly basis. (Later on AWS and other cloud providers moved to a per-second billing.) The presence of an API makes the difference between IaaS and plain old virtual machines as a service. IaaS allows a customer to scale their application dynamically according to their current demand.","title":"Virtual machines"},{"location":"lectures/3-iaas/#typical_instance_types","text":"When the cloud became popular in the late 2000s several providers attempted to offer a service that was fully dynamic in their sizes. The customer could set how many GB of RAM they needed and how many CPU cores. However, this model has been phased out by most providers since it is difficult to manage such a dynamic environment. Instead cloud providers nowadays opt to offer fixed machine sizes (think of t-shirt sizes). To accommodate high-CPU and high RAM workloads there are several different instance types, typically: Shared CPU: These are small instances where a single CPU core is shared between multiple virtual machines, sometimes leading to high steal time . Sometimes this offering includes a burst capability (such as the Amazon T instances) where a VM can temporarily use more CPU. Standard, dedicated core CPU: These instance types receive one or more physical cores leading to a more stable performance without the ability to burst beyond their limits. High CPU: These instance types are usually hosted on physical servers that have a very high CPU to RAM ratio. Accordingly, the virtual machine offering includes more CPU than the standard offering. High RAM: This offering is the exact opposite of the high CPU offering. The machines on offer here include more RAM with very little CPU. Storage: These instance types contain large amounts of local storage (see below in the storage section). Hardware-specific: These instance types offer access to dedicated hardware such as graphics cards (GPUs) or FPGAs.","title":"Typical instance types"},{"location":"lectures/3-iaas/#automation","text":"As discussed before, that makes an IaaS cloud provider a cloud provider is the fact that they offer an API to automate the provisioning and deprovisioning of virtual machines as needed. However, that's not all. Simply starting a virtual machine is not enough, the software needs to be installed in it. Initially this problem would be solved by creating templates for the operating system that launches. In larger cloud setups these templates included a pre-installed agent for configuration management that would report to a central service and fetch its manifest of software to install. Thankfully in the last decade a lot has happened and Cloud Init has established itself as a defacto standard in the IaaS world. Every cloud provider nowadays offers the ability to submit a user data field when creating a virtual machine. This user data file is read by Cloud Init (or its Windows alternative Cloudbase Init ) and is executed at the first start of the virtual machine. A DevOps engineer can simply inject a script that runs at the first start that takes care of all the installation steps required. Tools like Terraform or Ansible assist with managing the whole process of provisioning the virtual machines and supplying it with the correct user data script.","title":"Automation"},{"location":"lectures/3-iaas/#virtual_machine_pools","text":"One other use of user data are virtual machine pools. Each cloud provider adopts a different name for them, ranging from instance pools to autoscaling groups. The concept is the same everywhere: you supply the cloud with a configuration how you would like your virtual machines to look like and the cloud will take care that the given number of machines are always running. If a machine crashes or fails a health check the cloud deletes the machine and creates a new one. The number of machines in a pool can, of course, be changed either manually or in some cases automatically using rules for automatic scaling. Combined with the aforementioned user data this can be a very powerful tool to create a dynamically sized pool of machines and is the prime choice for creating a scalable architecture. These pools are often integrated with the various load-balancer offerings cloud providers have in their portfolio to direct traffic to the dynamic number of instances. Some cloud providers integrate them with their Functions as a Service offering as well allowing you to run a custom function whenever a machine starts or stops. This can be used to, for example, update your own service discovery database.","title":"Virtual machine pools"},{"location":"lectures/3-iaas/#storage","text":"Hopefully we have managed to convey the dynamic nature of the cloud in our description so far. This brings up the question of where data should be stored. This section will explain the different storage options that you can use to store files directly from your virtual machine. We will touch on databases and other storage options in a later section.","title":"Storage"},{"location":"lectures/3-iaas/#how_do_filesystems_work","text":"Before we dive into the different options let's take a look at how filesystems work. In a non-cloud environment you would insert a hard drive into your computer. This hard drive can either be a spinning disk (often referred to as HDD) or a solid-state drive (SSD). Spinning disks have a read head that goes over the platter that spins under it. This means that information access on HDD requires the head to get into the correct position to read the data which takes time. This positioning is done by the controller chip that's on the disk itself. SSD's have different problems. Information access is instant since any block can be accessed immediately but the chip on the disk still needs to keep track of which cell is unusable, caching, etc. In other words the controller chip deals with the low-level functions on the disk for both HDD and SSD. The drive itself is accessible via either a SATA (Serial ATA) or a SAS (Serial Attached SCSI) connection. In case of servers these disks are often assembled into a RAID array for redundancy and higher performance. Most RAID controllers will also implement caching which often also requires a built-in battery unit to function properly. In both cases the data is stored in blocks of fixed sizes (such as 4 kB). The filesystem itself contains the mapping of which file consists of which blocks on the disk. This is an important distinction: block storage solutions, no matter if they are network-bound or use a more classic connection such as fibre channel only know about the blocks on the disk. They do not know anything about files. Since filesystems are complex beasts block storage devices can only be used by a single machine at any given time. Network filesystems on the other hand are more resource intensive but can track which file is open from which machine and can be used in a distributed fashion.","title":"How do filesystems work?"},{"location":"lectures/3-iaas/#local_storage","text":"The most obvious storage option, of course, is a local storage. By local storage we mean a hard drive (SSD or HDD) that is integrated directly into the machine that's running the hypervisor. This storage option has the benefit of offering a high performance disk at a relatively affordable price. The drawback is that the disk is local. If the physical machine running your VM has a problem your data may be lost. For all storage options the implementation of backups is critical but with local storage building redundancy on top of the virtual machine may be required to build a reliable system.","title":"Local Storage"},{"location":"lectures/3-iaas/#network_block_storage","text":"Network block storage means a block storage that is delivered over the network. When talking about network we are not only talking about your standard IP / Ethernet network but also a direct SCSI over Fibre Channel infrastructure. The latter is often used in expensive enterprise storage systems that provide attachable disks to several physical machines. TODO: add illustration for fibre channel storage systems The simplest and easiest to set up network block storage is probably ISCSI, a protocol that runs the SCSI storage protocol over a commodity IP network. In other words one computer can take a block device and let another computer use it. However, as mentioned previously in this case the computer using the ISCSI device is in full control over the files that are stored on the storage device. It is also worth mentioning that ISCSI does not guarantee any redundancy beyond the RAID or storage system the offering computer already has. More advanced storage systems such as Ceph RBD or replicated enterprise storage systems offer redundancy such that when one storage instance fails others can take its place without outage. Cloud provider offerings such as EBS by Amazon also offer this kind of redundancy with severe limitations as to how many EBS volumes can be attached to a virtual machine.","title":"Network Block Storage"},{"location":"lectures/3-iaas/#network_file_systems","text":"In contrast to network block storage network file systems offer access to data not on a block level, but on a file level. Over the various network file system protocols machines using these file systems can open, read and write files, and even place locks on them. The filesystem has to keep track of which machine has which file open, or has locks on which file. When machine edit the same file in parallel the filesystem has to ensure that these writes are consistent. This means that network file systems are either much slower than block-level access (e.g. NFS ) or require a great deal more CPU and RAM to keep track of the changes across the network (e.g. CephFS ).","title":"Network File Systems"},{"location":"lectures/3-iaas/#object_storage","text":"Object storage systems are similar to network file systems in that they deal with files rather than blocks. However, they do not have the same synchronization capabilities as network file systems. Files can generally only be read or written as a whole and they also don't have the ability to lock a file. While object storages technically can be used as a filesystem on an operating system level for example by using s3fs this is almost always a bad idea due to the exceptionally bad performance and stability issues. Operating system level integration should only be used as a last resort and object storages should be integrated on the application level. We will discuss object storage services in detail in our next lesson.","title":"Object storage"},{"location":"lectures/3-iaas/#network","text":"The next big topic concerning IaaS services is networks. Before we go into the cloud-aspect let's look at how the underlying infrastructure is built. As indicated in the first lecture it is strongly recommended that you familiarize yourself with the basics of computer networking, such as the Ethernet, IP and TCP protocols as you will need them to understand this section.","title":"Network"},{"location":"lectures/3-iaas/#how_cloud_networks_are_built","text":"So, let's get started. Imagine a data center from the first lecture. Your task is to build an IaaS cloud provider. You put your servers that will serve as your hosts for virtual machines in the racks. These servers will be connected to the Top-of-Rack switches (yes, two for redundancy) using 10 GBit/s network cables. The switches are themselves connected among each other and across racks with several 100 GBit/s. Now comes the tricky part: how do you create private networks for each customer? One option would be to use 802.11Q VLAN tags separating each network from each other. However, that limits you to 2 12 = 4096 private networks. That may seem like much but for a public cloud provider that is not sufficient. Therefore, public cloud providers use overlay protocols like VXLAN to work around this problem. We describe this not for you to learn but to highlight that public cloud networks are complex . A cloud provider cannot guarantee a fixed bandwidth between two virtual machines (unless they are put on the same physical server using affinity groups). This is one of the reasons that often the ideal setup for cloud architectures is one that uses multiple, medium-sized instances rather than few large sized ones.","title":"How cloud networks are built"},{"location":"lectures/3-iaas/#underlying_network_architectures_offered_by_cloud_providers","text":"When we look at the network offerings by cloud providers there are three types: Virtual machines receive private IP addresses and a gateway or load balancer handles the public-to-private IP translations. This is the case with the larger cloud providers such as AWS , Azure and GCP at the time of writing. Virtual machines have one public IP address on the first network interface and additional private networks can be attached as new, separate network interfaces. This is the case with most smaller IaaS providers such as DigitalOcean , Hetzner , Upcloud , or Exoscale . Fully dynamic network configuration that allows the customer to define their network setup and IP assignment dynamically. This is typically offered by IaaS providers that target enterprise customers who wish to migrate their classic on-premises infrastructure and require the flexibility they had when using their own hardware. This is the case with 1&1 IONOS . TODO: Vultr? Linode? Deutsche Telekom Cloud? Alibaba Cloud? TODO: add illustration Out of group 2 it is worth mentioning that the services that are available on the public network (firewalls, load balancers) are often not available on private networks.","title":"Underlying network architectures offered by cloud providers"},{"location":"lectures/3-iaas/#firewalling","text":"IaaS providers often also offer network firewalls as a service, included in the platform. Firewalls generally have two rule types: INGRESS (from the Internet or other machines to the current VM) and EGRESS (From the current VM to) everywhere else. Firewall providers often employ the concept of security groups . The implementation varies greatly, but in general security groups are a reusable set of rules that can be applied to a VM. For most cloud providers you will need to create an explicit rule allowing traffic to flow between two machines in the same security group. The advantage of security groups is that the rules can be made in such a way that they reference other security groups rather than specific IP addresses. For example, the database security group could be set to allow connections only from the appserver security group but not from anywhere else. This can help with the dynamic nature of the cloud since you do not need to hard-code the IP addresses of the application servers. TODO: add screenshot of security group configuration","title":"Firewalling"},{"location":"lectures/3-iaas/#network_load_balancers","text":"Network load balancers are an option some cloud providers offer. In contrast to Application Load Balancers they do not offer protocol decoding (such as routing requests to backends based on the requested web address), they only balance incoming connections to a pool of backends. TODO add illustration Depending on the cloud provider in question network load balancers may or may not offer terminating encrypted connections (SSL/TLS), and may be bound to virtual machine pools. It is also cloud provider specific if load balancers are offered in private networks or not. When designing an architecture it is worth considering if the real IP address of the connecting client will be needed. If the backend needs to know the real IP address of the client and the network load balancer handles SSL/TLS termination that combination may not be suitable for the task unless a specific trick such as the proxy protocol from Digital Ocean . Network load balancers without SSL/TLS termination should, in general, make the client IP available to the backends. When talking about load balancers an interesting question is the load balancing strategy. Most load balancers support either round robin (selecting the next backend in the list) or source hashing (routing the same connecting IP to the same backend). Most load balancers also support health checks to take backends that are not able to serve traffic out of the rotation.","title":"Network load balancers"},{"location":"lectures/3-iaas/#vpns_private_interconnects_and_routing_services","text":"","title":"VPNs, private interconnects, and routing services"},{"location":"lectures/3-iaas/#dns","text":"","title":"DNS"},{"location":"lectures/3-iaas/#monitoring","text":"","title":"Monitoring"},{"location":"lectures/3-iaas/#automation_1","text":"","title":"Automation"},{"location":"lectures/4-xaas/","text":"Download PPTX \ud83d\udcbb Download MP3 \ud83c\udfa7 Download M4B \ud83c\udfa7 Watch Video \ud83c\udfac Platform as a Service and more Application load balancers # CDNs # Object Storage # Cold storage # Databases as a Service (DBaaS) # Relational databases (SQL) # Document databases # Time Series databases # Functions as a Service (FaaS / Lambda) # Containers as a Service (CaaS) # Stream processing # Deployment pipelines #","title":"4. Beyond IaaS: PaaS, SaaS"},{"location":"lectures/4-xaas/#application_load_balancers","text":"","title":"Application load balancers"},{"location":"lectures/4-xaas/#cdns","text":"","title":"CDNs"},{"location":"lectures/4-xaas/#object_storage","text":"","title":"Object Storage"},{"location":"lectures/4-xaas/#cold_storage","text":"","title":"Cold storage"},{"location":"lectures/4-xaas/#databases_as_a_service_dbaas","text":"","title":"Databases as a Service (DBaaS)"},{"location":"lectures/4-xaas/#relational_databases_sql","text":"","title":"Relational databases (SQL)"},{"location":"lectures/4-xaas/#document_databases","text":"","title":"Document databases"},{"location":"lectures/4-xaas/#time_series_databases","text":"","title":"Time Series databases"},{"location":"lectures/4-xaas/#functions_as_a_service_faas_lambda","text":"","title":"Functions as a Service (FaaS / Lambda)"},{"location":"lectures/4-xaas/#containers_as_a_service_caas","text":"","title":"Containers as a Service (CaaS)"},{"location":"lectures/4-xaas/#stream_processing","text":"","title":"Stream processing"},{"location":"lectures/4-xaas/#deployment_pipelines","text":"","title":"Deployment pipelines"},{"location":"lectures/5-cloud-native/","text":"","title":"5. Cloud-native software development"},{"location":"projectwork/","text":"You are the cloud architect for a small work-for-hire company. A client wants to hire you but they are sceptical about your abilities to build an autoscaling service. They propose a proof of concept: build a service that runs a web application that is deliberately slow and load test it. Your cloud management should automatically launch new cloud servers when the load is high and remove servers when demand is low. TODO: which cloud provider? After taking a look at the capabilities of the cloud provider and discussing the constraints with your colleagues you decide that the following approach would be best: You are going to use Terraform to automate the setup and tear down of the cloud infrastructure. This is necessary because if you continuously use the cloud service you will not fit in the budget. You will use instance pools to manage the variable number of cloud servers and Network Load Balancers [TODO add link] to balance the traffic between them. You will set up a dedicated monitoring and management instance which will run Prometheus to automatically monitor a varying number of servers. You will write a custom service discovery agent that creates a file with the IP addresses of the machines in the instance pool for Prometheus to consume. On the instance pool you will deploy the Prometheus node exporter to monitor CPU usage. You will install Grafana to provide a monitoring dashboard and the ability to send webhooks. You will configure an alert webhook in Grafana that sends a webhook to an application written by you. If the average CPU usage is above 80%, or below 20% to scale up or down respectively a webhook is sent. You will write an application that receives this webhook and every 60 seconds scales the instance pool up or down if a webhook has been received. As you also have to demonstrate to the client that you can work in an agile methodology you agree in 4 week sprints with a demo at the end of each sprint as outlined in the deadlines document.","title":"Project work"},{"location":"testing/","text":"Tests will be conducted starting in December 2020. The exam will last 120 minute and consist of a multiple choice test. During the test you will be required to obey the following rules: Have an empty room with no disturbances. Close all other applications except for the single browser window the test runs in. Remove all electronic devices and notes from your desk and person. No phones, no smartwatches of any kind are allowed. Unless medically required no food is allowed during the test and only clear liquids in clear, unlabeled containers are allowed. During the test you will not be allowed to talk. You will be required to turn on your camera the entire time during the test. You will also be required to show that your environment suits the criteria described above. You will be required to turn on your screen sharing during the entire test.","title":"Testing"}]}