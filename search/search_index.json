{"config":{"lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Download PPTX \ud83d\udcbb Cloud Computing Fall 2020 Welcome to the Cloud Computing Fall 2020! On this page you will find all learning materials you need to learn in order to pass the course. Please note that this lecture is worth 4 ECTS. This is calculated to be roughly 112 hours of study/exercise/project work (5-7 hours per week recommended). Pre-requisites # In order to pass this course you are expected to have an understanding of computer systems and networks (operating a Linux system, IP addresses, netmasks, etc) as well as basic skills in writing shell scripts. You will also require a computer you can install a small number of tools on (Terraform, etc). Lectures # Lectures are provided on the lectures page . Each lecture contains a text document, a PPTX, an audio book in MP3 format, as well as a video . You can also listen to individual sections from the text document. All content is available in all formats so you only need to use the formats you prefer. The theory test will require an understanding of these materials . Take a look at the sample questions included in the text version. Exercises # Exercise sections are provided on the exercises page . Each exercise session is provided as a written tutorial. Source code is provided as a Git repository . You are expected to do each exercise session at least once. Project work # Project work is based on the practice sessions . Your project work assignment is described on the project work page . Deadlines are described on the deadlines page . You are expected to hand in your project work continuously, please don't leave it for the last second. You have the option to hand in early. Online consultations & getting help. # Online consultations are provided on a regular basis. Please find detailed description on how to find help on the getting help page . Grading & testing # Grading is described on the grading page . Testing is described on the testing page . About us # Peter Wenzl # Linkedin: linkedin.com/in/peterwenzl/ E-mail: peter.wenzl@edu.fh-campuswien.ac.at Computer Science Master (Dipl. Ing.), TU Wien, 2000 Telecoms Focused Career Ericsson Austria mobilkom Austria Oracle Austria GmBH (CGBU) Frequentis AG Janos Pasztor # Website: pasztor.at Linkedin: linkedin.com/in/janoszen/ Twitter: twitter.com/janoszen/ E-mail: janos.pasztor@edu.fh-campuswien.ac.at Software developer and DevOps engineer background (10+ years experience) Cloud Focused Career Deloitte, A1 Group, Entrecloud (startup), Ixolit group, Docler group. Certified Kubernetes Application Developer","title":"Overview"},{"location":"#pre-requisites","text":"In order to pass this course you are expected to have an understanding of computer systems and networks (operating a Linux system, IP addresses, netmasks, etc) as well as basic skills in writing shell scripts. You will also require a computer you can install a small number of tools on (Terraform, etc).","title":"Pre-requisites"},{"location":"#lectures","text":"Lectures are provided on the lectures page . Each lecture contains a text document, a PPTX, an audio book in MP3 format, as well as a video . You can also listen to individual sections from the text document. All content is available in all formats so you only need to use the formats you prefer. The theory test will require an understanding of these materials . Take a look at the sample questions included in the text version.","title":"Lectures"},{"location":"#exercises","text":"Exercise sections are provided on the exercises page . Each exercise session is provided as a written tutorial. Source code is provided as a Git repository . You are expected to do each exercise session at least once.","title":"Exercises"},{"location":"#project_work","text":"Project work is based on the practice sessions . Your project work assignment is described on the project work page . Deadlines are described on the deadlines page . You are expected to hand in your project work continuously, please don't leave it for the last second. You have the option to hand in early.","title":"Project work"},{"location":"#online_consultations_getting_help","text":"Online consultations are provided on a regular basis. Please find detailed description on how to find help on the getting help page .","title":"Online consultations &amp; getting help."},{"location":"#grading_testing","text":"Grading is described on the grading page . Testing is described on the testing page .","title":"Grading &amp; testing"},{"location":"#about_us","text":"","title":"About us"},{"location":"#peter_wenzl","text":"Linkedin: linkedin.com/in/peterwenzl/ E-mail: peter.wenzl@edu.fh-campuswien.ac.at Computer Science Master (Dipl. Ing.), TU Wien, 2000 Telecoms Focused Career Ericsson Austria mobilkom Austria Oracle Austria GmBH (CGBU) Frequentis AG","title":"Peter Wenzl"},{"location":"#janos_pasztor","text":"Website: pasztor.at Linkedin: linkedin.com/in/janoszen/ Twitter: twitter.com/janoszen/ E-mail: janos.pasztor@edu.fh-campuswien.ac.at Software developer and DevOps engineer background (10+ years experience) Cloud Focused Career Deloitte, A1 Group, Entrecloud (startup), Ixolit group, Docler group. Certified Kubernetes Application Developer","title":"Janos Pasztor"},{"location":"deadlines/","text":"Each sprint has a hard deadline. Sprints can be handed in earlier. If a deadline is missed the points are not awarded and the solution is uploaded to GitHub so the next sprint can be accomplished. Furthermore, if you run out of budget on your Exoscale account a -15% penalty will be incurred. Make sure you monitor your Exoscale account. (Please contact us if you require more budget.) Sprint 1: Instance Pools # Deadline: 15 th of October 2020 In this sprint you must demonstrate your ability to set up an instance pool and a network load balancer. Pass criteria: # Manual solution (5%) # You must demonstrate on a call how you set up your Exoscale environment in detail. Otherwise, the requirements are identical as with the automated solution. Automated solution (15%) # You must upload your Terraform to GitHub or an alternative Git service and submit your link in Moodle. Your code must accept two input variables, exoscale_key and exoscale_secret . The automated checking system will run your code against a completely empty Exoscale account from a standard Ubuntu 20.04 environment. The automated system will check the following: The Terraform code must execute successfully, The presence of a single NLB with a single service listening on port 80, The presence of a single instance pool, That all backends on the NLB are green within 5 minutes of running Terraform, When deleting all instances from the instance pool, the instance pool must provision the new instances and the health check must become green within 5 minutes, When accessing the IP address of the NLB with the /health URL, that URL should respond with \"OK\" and a HTTP status code 200. (The easiest way to achieve this is to run the http load generator .) Please make sure your your code is in the main/master branch of your Git repository. Sprint 2: Monitoring # Deadline: 15 th of November 2020 In this sprint you must demonstrate your ability to monitor a varying number of instances set up on an instance pool in the previous sprint. Pass criteria: Your monitoring instance must be set up with Prometheus and Grafana running. Prometheus must track the instances in the instance pool using custom service discovery and collect their CPU usage in a graph in Grafana. Sprint 3: Autoscaling # Deadline: 15 th of December 2020 In this sprint you must demonstrate your ability to receive a webhook from Grafana and adjust the number of instances in the instance pool from the previous sprint under load. Pass criteria: Your service will be hit with a large number of requests and it must scale up as outlined in the project work document .","title":"Deadlines"},{"location":"deadlines/#sprint_1_instance_pools","text":"Deadline: 15 th of October 2020 In this sprint you must demonstrate your ability to set up an instance pool and a network load balancer.","title":"Sprint 1: Instance Pools"},{"location":"deadlines/#pass_criteria","text":"","title":"Pass criteria:"},{"location":"deadlines/#manual_solution_5","text":"You must demonstrate on a call how you set up your Exoscale environment in detail. Otherwise, the requirements are identical as with the automated solution.","title":"Manual solution (5%)"},{"location":"deadlines/#automated_solution_15","text":"You must upload your Terraform to GitHub or an alternative Git service and submit your link in Moodle. Your code must accept two input variables, exoscale_key and exoscale_secret . The automated checking system will run your code against a completely empty Exoscale account from a standard Ubuntu 20.04 environment. The automated system will check the following: The Terraform code must execute successfully, The presence of a single NLB with a single service listening on port 80, The presence of a single instance pool, That all backends on the NLB are green within 5 minutes of running Terraform, When deleting all instances from the instance pool, the instance pool must provision the new instances and the health check must become green within 5 minutes, When accessing the IP address of the NLB with the /health URL, that URL should respond with \"OK\" and a HTTP status code 200. (The easiest way to achieve this is to run the http load generator .) Please make sure your your code is in the main/master branch of your Git repository.","title":"Automated solution (15%)"},{"location":"deadlines/#sprint_2_monitoring","text":"Deadline: 15 th of November 2020 In this sprint you must demonstrate your ability to monitor a varying number of instances set up on an instance pool in the previous sprint. Pass criteria: Your monitoring instance must be set up with Prometheus and Grafana running. Prometheus must track the instances in the instance pool using custom service discovery and collect their CPU usage in a graph in Grafana.","title":"Sprint 2: Monitoring"},{"location":"deadlines/#sprint_3_autoscaling","text":"Deadline: 15 th of December 2020 In this sprint you must demonstrate your ability to receive a webhook from Grafana and adjust the number of instances in the instance pool from the previous sprint under load. Pass criteria: Your service will be hit with a large number of requests and it must scale up as outlined in the project work document .","title":"Sprint 3: Autoscaling"},{"location":"exercises/","text":"This section walks you through the technologies you will require to finish the project work. Keep in mind that this is but an introduction to the technologies you will use should you become a cloud engineer. It is designed to highlight how modern, cloud-native tools can be used to enhance the cloud beyond its capabilities. Exercise 1: IaaS # Create a virtual machine Automate software installation using user data Create a virtual machine pool Go to the exercise \u00bb Exercise 2: Terraform # Terraform basics Setting up a cloud provider connection Creating a virtual machine Using user data Creating a virtual machine pool Go to the exercise \u00bb Exercise 3: Containers # Writing a Dockerfile Building a container image Pushing to a registry Deploying using docker-compose Go to the exercise \u00bb Exercise 4: Prometheus # Prometheus basics Setting up Prometheus Setting up the node exporter Setting up service discovery Automating Prometheus deployment Go to the exercise \u00bb Exercise 5: Grafana # Grafana basics Setting up Grafana Integrating Grafana with Prometheus Setting up alerts Automating Grafana deployment Go to the exercise \u00bb Exercise 6: API integration # Creating a Grafana webhook microservice in Go Integrating the cloud API Go to the exercise \u00bb","title":"Overview"},{"location":"exercises/#exercise_1_iaas","text":"Create a virtual machine Automate software installation using user data Create a virtual machine pool Go to the exercise \u00bb","title":"Exercise 1: IaaS"},{"location":"exercises/#exercise_2_terraform","text":"Terraform basics Setting up a cloud provider connection Creating a virtual machine Using user data Creating a virtual machine pool Go to the exercise \u00bb","title":"Exercise 2: Terraform"},{"location":"exercises/#exercise_3_containers","text":"Writing a Dockerfile Building a container image Pushing to a registry Deploying using docker-compose Go to the exercise \u00bb","title":"Exercise 3: Containers"},{"location":"exercises/#exercise_4_prometheus","text":"Prometheus basics Setting up Prometheus Setting up the node exporter Setting up service discovery Automating Prometheus deployment Go to the exercise \u00bb","title":"Exercise 4: Prometheus"},{"location":"exercises/#exercise_5_grafana","text":"Grafana basics Setting up Grafana Integrating Grafana with Prometheus Setting up alerts Automating Grafana deployment Go to the exercise \u00bb","title":"Exercise 5: Grafana"},{"location":"exercises/#exercise_6_api_integration","text":"Creating a Grafana webhook microservice in Go Integrating the cloud API Go to the exercise \u00bb","title":"Exercise 6: API integration"},{"location":"exercises/1-iaas/","text":"Infrastructure as a Service This exercise will guide you through the basics of setting up a cloud account, configuring your first firewall and configuring your first instance. Since the project work is based on Exoscale this account will showcase using Exoscale. The principles shown here can be applied to any cloud provider. For the purposes of following this exercise please use the voucher provided. Remember, the voucher is limited to 20 euros, so destroy any resources you create for testing. Starting your first instance # Once you log in to the Exoscale interface you will find several services on the left side: Compute, Storage, DNS, etc. Compute is the name for the IaaS platform discussed in lecture 2 . When you look at the Compute interface you will find left hand menu with several options: Instances, Instance pools, Templates, Firewalling, IP addresses, and so on. As a first exercise we will launch a single instance, so please open the Instances option and click the \"Add\" button on the interface. The \"New instance\" interface presents you with several options. Most importantly, you will select the operating system template, instance size and storage size. These options will be present on any cloud provider and affect the pricing. Instances on Exoscale, and on many other providers, are priced per-second for the instance size (Micro, Tiny, etc), and the storage is priced separately. When an instance is stopped, but not destroyed, only the storage price is charged when the account is not in trial . Since your account is in trial your instance is also charged-for when it is stopped. For the purposes of this exercise please select \"Linux Ubuntu 20.04 LTS 64-bit\", Micro, and a 10 GB disk. You can start the instance in any zone, but we recommend starting it in Vienna for latency reasons. The next option is the key pair. Key pairs are SSH keys used for passwordless authentication (more on that later). For now, please ignore the Keypair, Private Networks and IPv6 options. The next option is important: Security Groups. For now we will stick with the default security group, but remember this option for later as it will become important to building your architecture. As discussed in lecture 2 , security groups provide network firewalling. You can also ignore Anti-Affinity groups for now. Remember for later, anti-affinity groups are useful when you are building a redundant service and you don't want two instances to run on the same physical hardware. That's what anti-affinity groups guarantee. The last item in the list is User Data . Again, as discussed in in lecture 2 , User Data is a way to pass an initialization script to the virtual machine. For our exercise let's put a very simple Bash script here: #!/bin/bash set -e apt update apt install -y nginx Once you are done with the configuration, please click the \"Create\" button. The instance will now be installed and the initialization script will run. Please note the \"Password\" field. Since you have not provided an SSH key the password is the only way to log in to your instance now. Wait until the instance has booted and make a note of the password! While you wait for the installation script to run please go to the \"Firewalling\" option on the left side and click the \"default\" security group. Here you can create a firewall rule that lets HTTP traffic in. Please click the \"New Rule\" button in the top right hand corner and add a rule with the following options: Type: Ingress Protocol: TCP Source type: CIDR Source: 0.0.0.0/0 Port(s): 80 - 80 This will allow web traffic in from the whole internet. Networking know-how needed! For the purposes of this course it is assumed you have a basic understanding of computer networks, including IP addresses and CIDR masks. If you feel like you need a refresher, please give the Geek University CCNA course a quick read or ask us in the tutoring sessions. For later use please also add an SSH rule (port 22). Make sure that SSH is only allowed from your IP address. Opening SSH to the whole internet will incur a flood of hacking attempts that increase the log size needlessly. Alternatively, you can also move SSH to a different port. In the mean time your instance should be up and running. Please go to the \"Instances\" option and copy the IP address of your server. Open a new browser tab and insert the IP address there. If you did everything correctly an nginx welcome page should show up. If you now wish to log in to the server you can use the command prompt / console depending on your operating system. Please copy the command from the \"SSH command\" field in the interface into your command prompt. When you are prompted the password, insert the password. (Unlike on the graphical interfaces, an SSH login won't show you the stars for the characters you typed.) Setting up the CLI # Using the graphical interface is not the preferred way to interact with the cloud. In case of more complex cloud providers (Azure, etc) the user interface may not contain all features, or may be too complex to use. Command line interfaces provider a developer-friendly way to interact with the cloud provider. In case of Exoscale you will have to install the exo cli on your computer. Once you have it installed, add it to the PATH of your machine and then open a command prompt / terminal. In the terminal please type exo config to set up the configuration. During the setup you will be prompted for an API key which you can create in the Exoscale interface in the IAM menu . Once your exo CLI is configured you can create the firewall rules we created previously using the following commands: exo firewall add default --protocol tcp --port 80 exo firewall add default --protocol tcp --port 22 Provisioning an instance is similarly simple: > exo vm create test -d 10 -o micro -t \"Linux Ubuntu 20.04 LTS 64-bit\" Creating private SSH key Deploying \"test\" \u2826 done What to do now? 1. Connect to the machine exo ssh test 2. Put the following SSH configuration into \".ssh/config\" Host test HostName 194.182.174.84 User ubuntu IdentityFile C:\\Users\\janos\\AppData\\Roaming\\exoscale/instances/34df135d-8ff1-47b5-81a9-194c71233941/id_rsa Tip of the day: You're the sole owner of the private key. Be cautious with it. \"test\" in this example is the name of the instance. The benefit of using the exo cli is the automatic use of an SSH key. You can now use the exo ssh test command to SSH into your instance securely with an SSH key instead of a password. If you want to add a User Data you will have to store the script in a file and then pass the -f filename.sh option. Be careful! If you store the user data script on a Windows machine it will do so with Windows line endings. This will not work on Linux systems. Make sure to use an editor that can save the script with Linux line endings. Creating a virtual machine pool # Creating a virtual machine pool is very similar to creating an instance. Assuming you have saved your User Data script in a file called userdata.sh you can start a pool with 2 machines with the following command: exo instancepool create autoscaling \\ --service-offering Micro \\ --template \"Linux Ubuntu 20.04 LTS 64-bit\" \\ --zone at-vie-1 \\ --security-group default \\ --cloud-init userdata.sh \\ --disk 10 \\ --size 2 Destroying your instances # Once you are done with this exercise make sure to destroy your instances to save on the budget.","title":"1. IaaS"},{"location":"exercises/1-iaas/#starting_your_first_instance","text":"Once you log in to the Exoscale interface you will find several services on the left side: Compute, Storage, DNS, etc. Compute is the name for the IaaS platform discussed in lecture 2 . When you look at the Compute interface you will find left hand menu with several options: Instances, Instance pools, Templates, Firewalling, IP addresses, and so on. As a first exercise we will launch a single instance, so please open the Instances option and click the \"Add\" button on the interface. The \"New instance\" interface presents you with several options. Most importantly, you will select the operating system template, instance size and storage size. These options will be present on any cloud provider and affect the pricing. Instances on Exoscale, and on many other providers, are priced per-second for the instance size (Micro, Tiny, etc), and the storage is priced separately. When an instance is stopped, but not destroyed, only the storage price is charged when the account is not in trial . Since your account is in trial your instance is also charged-for when it is stopped. For the purposes of this exercise please select \"Linux Ubuntu 20.04 LTS 64-bit\", Micro, and a 10 GB disk. You can start the instance in any zone, but we recommend starting it in Vienna for latency reasons. The next option is the key pair. Key pairs are SSH keys used for passwordless authentication (more on that later). For now, please ignore the Keypair, Private Networks and IPv6 options. The next option is important: Security Groups. For now we will stick with the default security group, but remember this option for later as it will become important to building your architecture. As discussed in lecture 2 , security groups provide network firewalling. You can also ignore Anti-Affinity groups for now. Remember for later, anti-affinity groups are useful when you are building a redundant service and you don't want two instances to run on the same physical hardware. That's what anti-affinity groups guarantee. The last item in the list is User Data . Again, as discussed in in lecture 2 , User Data is a way to pass an initialization script to the virtual machine. For our exercise let's put a very simple Bash script here: #!/bin/bash set -e apt update apt install -y nginx Once you are done with the configuration, please click the \"Create\" button. The instance will now be installed and the initialization script will run. Please note the \"Password\" field. Since you have not provided an SSH key the password is the only way to log in to your instance now. Wait until the instance has booted and make a note of the password! While you wait for the installation script to run please go to the \"Firewalling\" option on the left side and click the \"default\" security group. Here you can create a firewall rule that lets HTTP traffic in. Please click the \"New Rule\" button in the top right hand corner and add a rule with the following options: Type: Ingress Protocol: TCP Source type: CIDR Source: 0.0.0.0/0 Port(s): 80 - 80 This will allow web traffic in from the whole internet. Networking know-how needed! For the purposes of this course it is assumed you have a basic understanding of computer networks, including IP addresses and CIDR masks. If you feel like you need a refresher, please give the Geek University CCNA course a quick read or ask us in the tutoring sessions. For later use please also add an SSH rule (port 22). Make sure that SSH is only allowed from your IP address. Opening SSH to the whole internet will incur a flood of hacking attempts that increase the log size needlessly. Alternatively, you can also move SSH to a different port. In the mean time your instance should be up and running. Please go to the \"Instances\" option and copy the IP address of your server. Open a new browser tab and insert the IP address there. If you did everything correctly an nginx welcome page should show up. If you now wish to log in to the server you can use the command prompt / console depending on your operating system. Please copy the command from the \"SSH command\" field in the interface into your command prompt. When you are prompted the password, insert the password. (Unlike on the graphical interfaces, an SSH login won't show you the stars for the characters you typed.)","title":"Starting your first instance"},{"location":"exercises/1-iaas/#setting_up_the_cli","text":"Using the graphical interface is not the preferred way to interact with the cloud. In case of more complex cloud providers (Azure, etc) the user interface may not contain all features, or may be too complex to use. Command line interfaces provider a developer-friendly way to interact with the cloud provider. In case of Exoscale you will have to install the exo cli on your computer. Once you have it installed, add it to the PATH of your machine and then open a command prompt / terminal. In the terminal please type exo config to set up the configuration. During the setup you will be prompted for an API key which you can create in the Exoscale interface in the IAM menu . Once your exo CLI is configured you can create the firewall rules we created previously using the following commands: exo firewall add default --protocol tcp --port 80 exo firewall add default --protocol tcp --port 22 Provisioning an instance is similarly simple: > exo vm create test -d 10 -o micro -t \"Linux Ubuntu 20.04 LTS 64-bit\" Creating private SSH key Deploying \"test\" \u2826 done What to do now? 1. Connect to the machine exo ssh test 2. Put the following SSH configuration into \".ssh/config\" Host test HostName 194.182.174.84 User ubuntu IdentityFile C:\\Users\\janos\\AppData\\Roaming\\exoscale/instances/34df135d-8ff1-47b5-81a9-194c71233941/id_rsa Tip of the day: You're the sole owner of the private key. Be cautious with it. \"test\" in this example is the name of the instance. The benefit of using the exo cli is the automatic use of an SSH key. You can now use the exo ssh test command to SSH into your instance securely with an SSH key instead of a password. If you want to add a User Data you will have to store the script in a file and then pass the -f filename.sh option. Be careful! If you store the user data script on a Windows machine it will do so with Windows line endings. This will not work on Linux systems. Make sure to use an editor that can save the script with Linux line endings.","title":"Setting up the CLI"},{"location":"exercises/1-iaas/#creating_a_virtual_machine_pool","text":"Creating a virtual machine pool is very similar to creating an instance. Assuming you have saved your User Data script in a file called userdata.sh you can start a pool with 2 machines with the following command: exo instancepool create autoscaling \\ --service-offering Micro \\ --template \"Linux Ubuntu 20.04 LTS 64-bit\" \\ --zone at-vie-1 \\ --security-group default \\ --cloud-init userdata.sh \\ --disk 10 \\ --size 2","title":"Creating a virtual machine pool"},{"location":"exercises/1-iaas/#destroying_your_instances","text":"Once you are done with this exercise make sure to destroy your instances to save on the budget.","title":"Destroying your instances"},{"location":"exercises/2-terraform/","text":"Terraform In the previous exercise we have discussed how to set up an infrastructure manually. For the purposes of this lecture Terraform is our tool of choice when it comes to cloud automation. There are, of course, many other open source tools like Ansible , or provider-dependent tools like AWS CloudFormation . We specifically chose Terraform because it is provider-independent and can be used to provision cloud resources with a wide range of providers . You can download Terraform to your own computer. As it is a program written in Go you can simply unpack it and use it. We recommend adding it to your PATH environment variable for easy access. Since Terraform is capable of using multiple files we also recommend using an IDE or an editor with a directory listing function. The Intellij IDEA Community Edition has a plugin for Terraform that also offers code completion. At it's core Terraform reads all *.tf files in a directory and merges them together. This provides an efficient way of structuring a project. Each Terraform file contains a number of data and resource sections which describe the cloud environment. For example, you could create the Exoscale instance from the previous exercise using the following Terraform code: data \"exoscale_compute_template\" \"ubuntu\" { zone = \"at-vie-1\" name = \"Linux Ubuntu 20.04 LTS 64-bit\" } resource \"exoscale_compute\" \"mymachine\" { zone = \"at-vie-1\" display_name = \"test\" template_id = data.exoscale_compute_template.ubuntu.id size = \"Micro\" disk_size = 10 key_pair = \"\" affinity_groups = [] security_groups = [\"default\"] user_data = <<EOF #!/bin/bash set -e apt update apt install -y nginx EOF } This exercise will guide you through the basics of using Terraform to provision cloud servers. Tip The entire source code for this exercise is available on GitHub . Setting up the Exoscale provider # Before you can begin you will need to configure your cloud provider. In our case that will be Exoscale, so our first piece of code in a file called provider.tf will be as follows: terraform { required_providers { exoscale = { source = \"terraform-providers/exoscale\" } } } provider \"exoscale\" { key = \"EXO...\" secret = \"...\" } This configures the Exoscale provider. We can make sure the provider loads correctly by running terraform init in the directory of the project. Tip You need to run terraform init every time the provider configuration changes. The above example is not ideal as it contains hard-coded credentials. As any code Terraform should be stored in a code versioning system such as Git so having hard-coded credentials is not ideal. Instead, let's create a separate variable section: variable \"exoscale_key\" { description = \"The Exoscale API key\" type = string } variable \"exoscale_secret\" { description = \"The Exoscale API secret\" type = string } provider \"exoscale\" { key = var.exoscale_key secret = \"${var.exoscale_secret}\" } As you can see variables and other resources can be referenced in Terraform. This is important as it lets us write reusable and modular cloud manifests. Variables can be referenced in two formats: either by directly writing their name, or by including them in a string with the dollar sign. Creating a virtual machine # Unlike in our previous exercise , we will take a little more elaborate route and actually create our own security group this time. We start by creating a file called sg.tf to hold the following security group configuration: resource \"exoscale_security_group\" \"sg\" { name = \"exercise-2\" } We will add the rules in a separate step: resource \"exoscale_security_group_rule\" \"http\" { security_group_id = exoscale_security_group.sg.id type = \"INGRESS\" protocol = \"tcp\" cidr = \"0.0.0.0/0\" start_port = 80 end_port = 80 } Once you have this small amount of code complete you can try and run the Terraform config: terraform apply This step will query you for the variables: $ terraform apply var.exoscale_key The Exoscale API key Enter a value: EXO... var.exoscale_secret The Exoscale API secret Enter a value: ... If you wish to automate this step you can create a file called terraform.tfvars with the following content: exoscale_key = \"EXO...\" exoscale_secret = \"...\" After inserting the variables Terraform will present you with a plan. It is very important that you always read this plan as it is a list of things Terraform intends to do. This may very well include destroying and re-creating a server! In our case the plan looks like this: Resource actions are indicated with the following symbols: + create Terraform will perform the following actions: # exoscale_security_group.sg will be created + resource \"exoscale_security_group\" \"sg\" { + id = (known after apply) + name = \"exercise-2\" } # exoscale_security_group_rule.http will be created + resource \"exoscale_security_group_rule\" \"http\" { + cidr = \"0.0.0.0/0\" + end_port = 80 + id = (known after apply) + protocol = \"tcp\" + security_group = (known after apply) + security_group_id = (known after apply) + start_port = 80 + type = \"INGRESS\" + user_security_group = (known after apply) } Plan: 2 to add, 0 to change, 0 to destroy. This looks good, so you can enter \"yes\" to have Terraform execute the plan. exoscale_security_group.sg: Creating... exoscale_security_group.sg: Creation complete after 1s [id=beac3948-dc6f-44a9-89cb-d3ff8ea1c319] exoscale_security_group_rule.http: Creating... exoscale_security_group_rule.http: Creation complete after 3s [id=33b86aea-a64e-4708-82c8-43e88242f73f] Apply complete! Resources: 2 added, 0 changed, 0 destroyed. Sweet! Now, on to creating a virtual machine: data \"exoscale_compute_template\" \"ubuntu\" { zone = \"at-vie-1\" name = \"Linux Ubuntu 20.04 LTS 64-bit\" } resource \"exoscale_compute\" \"mymachine\" { zone = \"at-vie-1\" display_name = \"test\" template_id = data.exoscale_compute_template.ubuntu.id size = \"Micro\" disk_size = 10 key_pair = \"\" affinity_groups = [] security_groups = [exoscale_security_group.sg.name] user_data = <<EOF #!/bin/bash set -e apt update apt install -y nginx EOF } As you can see we are referencing the previously-created security groups by name. This reference is important as Terraform attempts to execute resource creation in parallel and uses variables to determine which resources depend on each other. In other words, if you do not use a variable reference but hard-code the name Terraform will not be able to execute the instructions in the correct order. Tip You can force Terraform to destroy and recreate a resource by using terraform taint exoscale_compute.ubuntu . About your state file # You may have noticed that a new file called terraform.tfstate appeared in your project folder. This file is very important as it contains links to the cloud resources you created. If you lose it Terraform will not know what servers it already created and attempt to re-create everything. Also, do not put this file in Git! It contains sensitive information like passwords! Destroying resources # Once you are done with this exercise please run terraform destroy to tear down the infrastructure you just created.","title":"2. Terraform"},{"location":"exercises/2-terraform/#setting_up_the_exoscale_provider","text":"Before you can begin you will need to configure your cloud provider. In our case that will be Exoscale, so our first piece of code in a file called provider.tf will be as follows: terraform { required_providers { exoscale = { source = \"terraform-providers/exoscale\" } } } provider \"exoscale\" { key = \"EXO...\" secret = \"...\" } This configures the Exoscale provider. We can make sure the provider loads correctly by running terraform init in the directory of the project. Tip You need to run terraform init every time the provider configuration changes. The above example is not ideal as it contains hard-coded credentials. As any code Terraform should be stored in a code versioning system such as Git so having hard-coded credentials is not ideal. Instead, let's create a separate variable section: variable \"exoscale_key\" { description = \"The Exoscale API key\" type = string } variable \"exoscale_secret\" { description = \"The Exoscale API secret\" type = string } provider \"exoscale\" { key = var.exoscale_key secret = \"${var.exoscale_secret}\" } As you can see variables and other resources can be referenced in Terraform. This is important as it lets us write reusable and modular cloud manifests. Variables can be referenced in two formats: either by directly writing their name, or by including them in a string with the dollar sign.","title":"Setting up the Exoscale provider"},{"location":"exercises/2-terraform/#creating_a_virtual_machine","text":"Unlike in our previous exercise , we will take a little more elaborate route and actually create our own security group this time. We start by creating a file called sg.tf to hold the following security group configuration: resource \"exoscale_security_group\" \"sg\" { name = \"exercise-2\" } We will add the rules in a separate step: resource \"exoscale_security_group_rule\" \"http\" { security_group_id = exoscale_security_group.sg.id type = \"INGRESS\" protocol = \"tcp\" cidr = \"0.0.0.0/0\" start_port = 80 end_port = 80 } Once you have this small amount of code complete you can try and run the Terraform config: terraform apply This step will query you for the variables: $ terraform apply var.exoscale_key The Exoscale API key Enter a value: EXO... var.exoscale_secret The Exoscale API secret Enter a value: ... If you wish to automate this step you can create a file called terraform.tfvars with the following content: exoscale_key = \"EXO...\" exoscale_secret = \"...\" After inserting the variables Terraform will present you with a plan. It is very important that you always read this plan as it is a list of things Terraform intends to do. This may very well include destroying and re-creating a server! In our case the plan looks like this: Resource actions are indicated with the following symbols: + create Terraform will perform the following actions: # exoscale_security_group.sg will be created + resource \"exoscale_security_group\" \"sg\" { + id = (known after apply) + name = \"exercise-2\" } # exoscale_security_group_rule.http will be created + resource \"exoscale_security_group_rule\" \"http\" { + cidr = \"0.0.0.0/0\" + end_port = 80 + id = (known after apply) + protocol = \"tcp\" + security_group = (known after apply) + security_group_id = (known after apply) + start_port = 80 + type = \"INGRESS\" + user_security_group = (known after apply) } Plan: 2 to add, 0 to change, 0 to destroy. This looks good, so you can enter \"yes\" to have Terraform execute the plan. exoscale_security_group.sg: Creating... exoscale_security_group.sg: Creation complete after 1s [id=beac3948-dc6f-44a9-89cb-d3ff8ea1c319] exoscale_security_group_rule.http: Creating... exoscale_security_group_rule.http: Creation complete after 3s [id=33b86aea-a64e-4708-82c8-43e88242f73f] Apply complete! Resources: 2 added, 0 changed, 0 destroyed. Sweet! Now, on to creating a virtual machine: data \"exoscale_compute_template\" \"ubuntu\" { zone = \"at-vie-1\" name = \"Linux Ubuntu 20.04 LTS 64-bit\" } resource \"exoscale_compute\" \"mymachine\" { zone = \"at-vie-1\" display_name = \"test\" template_id = data.exoscale_compute_template.ubuntu.id size = \"Micro\" disk_size = 10 key_pair = \"\" affinity_groups = [] security_groups = [exoscale_security_group.sg.name] user_data = <<EOF #!/bin/bash set -e apt update apt install -y nginx EOF } As you can see we are referencing the previously-created security groups by name. This reference is important as Terraform attempts to execute resource creation in parallel and uses variables to determine which resources depend on each other. In other words, if you do not use a variable reference but hard-code the name Terraform will not be able to execute the instructions in the correct order. Tip You can force Terraform to destroy and recreate a resource by using terraform taint exoscale_compute.ubuntu .","title":"Creating a virtual machine"},{"location":"exercises/2-terraform/#about_your_state_file","text":"You may have noticed that a new file called terraform.tfstate appeared in your project folder. This file is very important as it contains links to the cloud resources you created. If you lose it Terraform will not know what servers it already created and attempt to re-create everything. Also, do not put this file in Git! It contains sensitive information like passwords!","title":"About your state file"},{"location":"exercises/2-terraform/#destroying_resources","text":"Once you are done with this exercise please run terraform destroy to tear down the infrastructure you just created.","title":"Destroying resources"},{"location":"exercises/3-containers/","text":"Containers As described in lecture 4 containers are a staple of packaging applications in a self-contained fashion. Linux containers contain everything a Linux application needs to run on a Linux kernel in an isolated fashion. This exercise will run you through the basics of packaging an application in a container and running it on a Linux server. This guide is an abridged version of \u201cGetting started with Docker for the Inquisitive Mind\u201d by Janos Pasztor, one of the lecturers on this course. We highly recommend reading the original version for more detail. Installing Docker # One of the most popular small-scale container runtimes, and indeed the first to popularize build recipes is Docker . Docker can be installed on Linux as well as Windows and MacOS, but the latter two have several issues a user needs to work around so we recommend using an Ubuntu Linux 20.04 virtual machine for the purposes of this exercise. To install Docker you can either follow the installation guide or use the convenience script to do so: curl -fsSL https://get.docker.com -o get-docker.sh sudo sh get-docker.sh Please note that this script is not recommended for production use, but will be accepted in a solution to the project work . Creating a basic Dockerfile # In order to package an application you can create a Dockerfile containing the instructions to install the dependencies and package an application. This file consists of the following format: COMMAND PARAMETERS The first command is the FROM command. This command specifies which base image to use, for example: FROM ubuntu:20.04 As mentioned before, containers are not virtual machines. This command only pulls in the userland piece of an Ubuntu system. It can be used to install libraries and packages required for an application to run. As this file has now been created we can build our container image as follows: docker build -t myimagename . Warning The dot at the end is important! It tells Docker to look for the Dockerfile in the current directory. Now that the image is built it can be launched: docker run -ti myimagename Tip The -ti parameter is only required for interactive sessions. For non-interactive sessions it is not required. Inside the container you can do operations just as you normally would. For example, you could run apt update , then apt install mc , and finally mc to launch the midnight commander. Once you are finished with the interactive session you can use the exit command or Ctrl + D to exit. However, as discussed in lecture 4 the main benefit containers bring to the table is reusability. If you treat a container like you would treat a virtual machine, updating, installing things manually, you lose the main benefit of using containers. Let's extend our Dockerfile to install the nginx webserver: FROM ubuntu:20.04 ENV DEBIAN_FRONTEND = noninteractive RUN apt update RUN apt install nginx There are two new commands here: the ENV and the RUN command. The ENV command instructs Docker to set an environment variable. The DEBIAN_FRONTEND=noninteractive environment variable instructs the apt and dpkg packaging utilities in Ubuntu to run in non-interactive mode and not ask questions. Instead, these commands will use default values. The RUN command simply runs the specified command. If you run the docker build . command now the build will fail: ... After this operation, 60.8 MB of additional disk space will be used. Do you want to continue? [Y/n] Abort. The command '/bin/sh -c apt install nginx' returned a non-zero code: 1 As you can see, the apt install command is asking for permission to continue with the installation. This teaches us an important aspect of containerization: the Dockerfile must contain only commands that run automatically . Let's fix the file: ... RUN apt install -y nginx The -y is specific to apt and means that the user wishes to answer yes to everything. OK, so let's run the build again: docker build -t mynginx . ... ---> 85e4def234e1 Successfully built 85e4def234e1 So, we run our image: docker run -ti -p 80 :80 mynginx The -p flag tells Docker to map the port 80 of the container to the port 80 of the host. This will allow you to access the service on the IP address of your server. However, we didn't tell Docker to run nginx, so we receive a shell and nothing else. Sure enough, we can start nginx manually, but that's not what we want. Before we exit our container let's find out where nginx is located: which nginx This will print the full path of the nginx binary which we will need in a moment. Let's exit the container with Ctrl + D . In order to make Docker actually run nginx we need to employ the CMD command in our Dockerfile : CMD [ \"/usr/sbin/nginx\" ] A word about CMD # The CMD command has a second form: CMD nginx parameters here . In this form Docker cannot pass the parameters directly to the underlying execv call that will ultimately launch the application. execv expects all parameters in a separated list. The binary itself has to be passed as a full path as the first parameter. To transform the CMD parameter Docker launches a shell in the container, for example Bash. This shell will then parse the parameters and ultimately launch the desired program. This may seem like a reasonable approach at first, but can cause problems. The first process in a container is responsible for handling signals , such as the TERM signal that indicates that a process should stop gracefully. Bash does not do this. Instead, Bash will ignore the signal and Docker will wait for 30 seconds and then forcefully kill the container. This is the same reason why you need to be careful when running a shell script in your CMD . If you want to do that you must either handle signals correctly, or launch your subsequent program with the exec stanza. The exec stanza replaces the current shell with the new program instead of launching a child process: #!/bin/bash set -e # Do some initialization here exec /usr/sbin/nginx There is also a second command in Docker called ENTRYPOINT . The ENTRYPOINT command allows you to specify an additional program that acts as a wrapper for CMD . Let's say you have the CMD from above, and you specify the ENTRYPOINT of /init.sh . In this case the full command that will be launched is /init.sh /usr/sbin/nginx . In other words, the contents of CMD are passed to the ENTRYPOINT . Getting nginx running # Back to the nginx example. If you launch the container you will see that it exits immediately. As mentioned before, the first process in a container has a special role. This special role is not only to handle signals, but also extends to running the container. If the first process exits the container will stop. In our case, nginx is doing something called daemonization . Daemonization on Linux means that a process launches a second copy of itself and exits from the first copy in order to give back control over the console to the user. In a container we want exactly the opposite: we do not want nginx to daemonize. This is achieved by setting the daemon off; parameter in the configuration file, or passing it via the command line: CMD [ \"/usr/sbin/nginx\" , \"-g\" , \"daemon off;\" ] If you now build and run the container you will see that it stays up and you can access the web service on the IP address of your machine running Docker. Applying configuration files # Having a container with only nginx in it is not very useful, so let's see how we can put some content on the web server. First, let's figure out where the document root of our web server is. We do this by overriding the CMD for out container and launching a shell in it: docker run -ti mynginx /bin/bash This will land us in a shell inside a new container. In the container we can search for the document root: grep -nr 'root ' /etc/nginx | grep -v '#' This will give us the following result: /etc/nginx/sites-available/default:41: root /var/www/html ; So, the document root is in /var/www/html . Let's create an index.html file which we will copy to the container in the next step: <h1>Hello world!</h1> Copying files to the container: COPY index.html /var/www/html/index.html If you rebuild and relaunch the container you will see that the default nginx page is replaced by our Hello world! . Before we proceed, let's document that our container exposes a service on port 80: EXPOSE 80 This is not strictly necessary and only serves a documentation purpose, but it is nevertheless useful. Building a software in a container # To summarize: in order to containerize our application we need to install the dependencies of your application (e.g. a web server), then copy your compiled application in your container, and then set your CMD to launch your application. You also have the option to compile your application inside the container. To avoid creating extremely large containers with the build tools we will use multistage builds . Let's create a very simple Go application in a file called main.go : package main import ( \"fmt\" \"log\" \"net/http\" ) func handler(w http.ResponseWriter, r *http.Request) { fmt.Fprintf(w, \"Hello world!\") } func main() { http.HandleFunc(\"/\", handler) log.Fatal(http.ListenAndServe(\":8080\", nil)) } This program will launch a simple web server on port 8080. Let's also add a go.mod file to enable Go modules: module github.com/yourusername/smaple go 1.14 Let's create a Dockerfile that compiles this application: FROM alpine AS build RUN apk add --update go RUN mkdir -p /srv/myapp WORKDIR /srv/myapp COPY main.go . COPY go.mod . ENV CGO_ENABLED = 0 RUN go build -tags netgo -a -v main.go RUN chmod +x main In the next stage we will copy our compiled application from the first stage and run it: FROM alpine AS run RUN apk add --no-cache libc6-compat COPY --from = build /srv/myapp/main /app CMD [ \"/app\" ] EXPOSE 8080 Finally, let's run it: docker build -t myapp . docker run -d -p 8080:8080 myapp There you go! That's your app built and running in a single file. You can see a few additional commands above, feel free to look them up in the official documentation . Service discovery # Docker has a built-in service discovery model. Containers can address each other by their names as long as you specify the --name parameter when creating or running the container, and the containers are attached to the same network. Let's take the following example: docker run --name a -ti ubuntu docker run --name b -ti ubuntu docker network create test docker network connect test a docker network connect test b In this case both A and B containers will be able to talk to each other over the network with their respective names. Alternatively, you can also provide the --network option to docker run Volumes # One last item we need to talk about are volumes . Containers are designed to be immutable . When a new version comes along we simply destroy the existing container and create a new one. Some services like database, however, need to store data in a persistent fashion. That's what we need volumes for. In the container world we can mount volumes directly from the host machine, or we can mount a network-based storage as discussed in the lectures. In our example we will showcase how to mount a volume from a local folder using Docker. Assuming we want to launch a webserver we can mount the document root from our previous example: docker run -d -p 80:80 -v /srv/www:/var/www/html mynginx In this case the /srv/www folder of the host machine will be mounted in /var/www/html inside the container. This can be used to persist data, but also during development when files change frequently. Security hardening # One more aspect of containerization we need to talk about is security. With our configuration above our container is running as root. While the container has a security boundary, running as root can still present a security risk. In fact, enterprise Kubernetes setups like OpenShift do not allow containers to run as root. Let's change our Go container so it doesn't run as root: ... FROM alpine AS run RUN addgroup -S --gid 1000 app && adduser -S app --uid 1000 -G app COPY --from = build /srv/myapp/main /app CMD [ \"/app\" ] USER 1000:1000 EXPOSE 8080 Note that we are specifying the user ID and group ID to run as in numeric form ( 1000 ). The USER command allows user names to be passed as well, but in a hardened Kubernetes setup this will not be accepted.","title":"3. Containers"},{"location":"exercises/3-containers/#installing_docker","text":"One of the most popular small-scale container runtimes, and indeed the first to popularize build recipes is Docker . Docker can be installed on Linux as well as Windows and MacOS, but the latter two have several issues a user needs to work around so we recommend using an Ubuntu Linux 20.04 virtual machine for the purposes of this exercise. To install Docker you can either follow the installation guide or use the convenience script to do so: curl -fsSL https://get.docker.com -o get-docker.sh sudo sh get-docker.sh Please note that this script is not recommended for production use, but will be accepted in a solution to the project work .","title":"Installing Docker"},{"location":"exercises/3-containers/#creating_a_basic_dockerfile","text":"In order to package an application you can create a Dockerfile containing the instructions to install the dependencies and package an application. This file consists of the following format: COMMAND PARAMETERS The first command is the FROM command. This command specifies which base image to use, for example: FROM ubuntu:20.04 As mentioned before, containers are not virtual machines. This command only pulls in the userland piece of an Ubuntu system. It can be used to install libraries and packages required for an application to run. As this file has now been created we can build our container image as follows: docker build -t myimagename . Warning The dot at the end is important! It tells Docker to look for the Dockerfile in the current directory. Now that the image is built it can be launched: docker run -ti myimagename Tip The -ti parameter is only required for interactive sessions. For non-interactive sessions it is not required. Inside the container you can do operations just as you normally would. For example, you could run apt update , then apt install mc , and finally mc to launch the midnight commander. Once you are finished with the interactive session you can use the exit command or Ctrl + D to exit. However, as discussed in lecture 4 the main benefit containers bring to the table is reusability. If you treat a container like you would treat a virtual machine, updating, installing things manually, you lose the main benefit of using containers. Let's extend our Dockerfile to install the nginx webserver: FROM ubuntu:20.04 ENV DEBIAN_FRONTEND = noninteractive RUN apt update RUN apt install nginx There are two new commands here: the ENV and the RUN command. The ENV command instructs Docker to set an environment variable. The DEBIAN_FRONTEND=noninteractive environment variable instructs the apt and dpkg packaging utilities in Ubuntu to run in non-interactive mode and not ask questions. Instead, these commands will use default values. The RUN command simply runs the specified command. If you run the docker build . command now the build will fail: ... After this operation, 60.8 MB of additional disk space will be used. Do you want to continue? [Y/n] Abort. The command '/bin/sh -c apt install nginx' returned a non-zero code: 1 As you can see, the apt install command is asking for permission to continue with the installation. This teaches us an important aspect of containerization: the Dockerfile must contain only commands that run automatically . Let's fix the file: ... RUN apt install -y nginx The -y is specific to apt and means that the user wishes to answer yes to everything. OK, so let's run the build again: docker build -t mynginx . ... ---> 85e4def234e1 Successfully built 85e4def234e1 So, we run our image: docker run -ti -p 80 :80 mynginx The -p flag tells Docker to map the port 80 of the container to the port 80 of the host. This will allow you to access the service on the IP address of your server. However, we didn't tell Docker to run nginx, so we receive a shell and nothing else. Sure enough, we can start nginx manually, but that's not what we want. Before we exit our container let's find out where nginx is located: which nginx This will print the full path of the nginx binary which we will need in a moment. Let's exit the container with Ctrl + D . In order to make Docker actually run nginx we need to employ the CMD command in our Dockerfile : CMD [ \"/usr/sbin/nginx\" ]","title":"Creating a basic Dockerfile"},{"location":"exercises/3-containers/#a_word_about_cmd","text":"The CMD command has a second form: CMD nginx parameters here . In this form Docker cannot pass the parameters directly to the underlying execv call that will ultimately launch the application. execv expects all parameters in a separated list. The binary itself has to be passed as a full path as the first parameter. To transform the CMD parameter Docker launches a shell in the container, for example Bash. This shell will then parse the parameters and ultimately launch the desired program. This may seem like a reasonable approach at first, but can cause problems. The first process in a container is responsible for handling signals , such as the TERM signal that indicates that a process should stop gracefully. Bash does not do this. Instead, Bash will ignore the signal and Docker will wait for 30 seconds and then forcefully kill the container. This is the same reason why you need to be careful when running a shell script in your CMD . If you want to do that you must either handle signals correctly, or launch your subsequent program with the exec stanza. The exec stanza replaces the current shell with the new program instead of launching a child process: #!/bin/bash set -e # Do some initialization here exec /usr/sbin/nginx There is also a second command in Docker called ENTRYPOINT . The ENTRYPOINT command allows you to specify an additional program that acts as a wrapper for CMD . Let's say you have the CMD from above, and you specify the ENTRYPOINT of /init.sh . In this case the full command that will be launched is /init.sh /usr/sbin/nginx . In other words, the contents of CMD are passed to the ENTRYPOINT .","title":"A word about CMD"},{"location":"exercises/3-containers/#getting_nginx_running","text":"Back to the nginx example. If you launch the container you will see that it exits immediately. As mentioned before, the first process in a container has a special role. This special role is not only to handle signals, but also extends to running the container. If the first process exits the container will stop. In our case, nginx is doing something called daemonization . Daemonization on Linux means that a process launches a second copy of itself and exits from the first copy in order to give back control over the console to the user. In a container we want exactly the opposite: we do not want nginx to daemonize. This is achieved by setting the daemon off; parameter in the configuration file, or passing it via the command line: CMD [ \"/usr/sbin/nginx\" , \"-g\" , \"daemon off;\" ] If you now build and run the container you will see that it stays up and you can access the web service on the IP address of your machine running Docker.","title":"Getting nginx running"},{"location":"exercises/3-containers/#applying_configuration_files","text":"Having a container with only nginx in it is not very useful, so let's see how we can put some content on the web server. First, let's figure out where the document root of our web server is. We do this by overriding the CMD for out container and launching a shell in it: docker run -ti mynginx /bin/bash This will land us in a shell inside a new container. In the container we can search for the document root: grep -nr 'root ' /etc/nginx | grep -v '#' This will give us the following result: /etc/nginx/sites-available/default:41: root /var/www/html ; So, the document root is in /var/www/html . Let's create an index.html file which we will copy to the container in the next step: <h1>Hello world!</h1> Copying files to the container: COPY index.html /var/www/html/index.html If you rebuild and relaunch the container you will see that the default nginx page is replaced by our Hello world! . Before we proceed, let's document that our container exposes a service on port 80: EXPOSE 80 This is not strictly necessary and only serves a documentation purpose, but it is nevertheless useful.","title":"Applying configuration files"},{"location":"exercises/3-containers/#building_a_software_in_a_container","text":"To summarize: in order to containerize our application we need to install the dependencies of your application (e.g. a web server), then copy your compiled application in your container, and then set your CMD to launch your application. You also have the option to compile your application inside the container. To avoid creating extremely large containers with the build tools we will use multistage builds . Let's create a very simple Go application in a file called main.go : package main import ( \"fmt\" \"log\" \"net/http\" ) func handler(w http.ResponseWriter, r *http.Request) { fmt.Fprintf(w, \"Hello world!\") } func main() { http.HandleFunc(\"/\", handler) log.Fatal(http.ListenAndServe(\":8080\", nil)) } This program will launch a simple web server on port 8080. Let's also add a go.mod file to enable Go modules: module github.com/yourusername/smaple go 1.14 Let's create a Dockerfile that compiles this application: FROM alpine AS build RUN apk add --update go RUN mkdir -p /srv/myapp WORKDIR /srv/myapp COPY main.go . COPY go.mod . ENV CGO_ENABLED = 0 RUN go build -tags netgo -a -v main.go RUN chmod +x main In the next stage we will copy our compiled application from the first stage and run it: FROM alpine AS run RUN apk add --no-cache libc6-compat COPY --from = build /srv/myapp/main /app CMD [ \"/app\" ] EXPOSE 8080 Finally, let's run it: docker build -t myapp . docker run -d -p 8080:8080 myapp There you go! That's your app built and running in a single file. You can see a few additional commands above, feel free to look them up in the official documentation .","title":"Building a software in a container"},{"location":"exercises/3-containers/#service_discovery","text":"Docker has a built-in service discovery model. Containers can address each other by their names as long as you specify the --name parameter when creating or running the container, and the containers are attached to the same network. Let's take the following example: docker run --name a -ti ubuntu docker run --name b -ti ubuntu docker network create test docker network connect test a docker network connect test b In this case both A and B containers will be able to talk to each other over the network with their respective names. Alternatively, you can also provide the --network option to docker run","title":"Service discovery"},{"location":"exercises/3-containers/#volumes","text":"One last item we need to talk about are volumes . Containers are designed to be immutable . When a new version comes along we simply destroy the existing container and create a new one. Some services like database, however, need to store data in a persistent fashion. That's what we need volumes for. In the container world we can mount volumes directly from the host machine, or we can mount a network-based storage as discussed in the lectures. In our example we will showcase how to mount a volume from a local folder using Docker. Assuming we want to launch a webserver we can mount the document root from our previous example: docker run -d -p 80:80 -v /srv/www:/var/www/html mynginx In this case the /srv/www folder of the host machine will be mounted in /var/www/html inside the container. This can be used to persist data, but also during development when files change frequently.","title":"Volumes"},{"location":"exercises/3-containers/#security_hardening","text":"One more aspect of containerization we need to talk about is security. With our configuration above our container is running as root. While the container has a security boundary, running as root can still present a security risk. In fact, enterprise Kubernetes setups like OpenShift do not allow containers to run as root. Let's change our Go container so it doesn't run as root: ... FROM alpine AS run RUN addgroup -S --gid 1000 app && adduser -S app --uid 1000 -G app COPY --from = build /srv/myapp/main /app CMD [ \"/app\" ] USER 1000:1000 EXPOSE 8080 Note that we are specifying the user ID and group ID to run as in numeric form ( 1000 ). The USER command allows user names to be passed as well, but in a hardened Kubernetes setup this will not be accepted.","title":"Security hardening"},{"location":"exercises/4-prometheus/","text":"Prometheus Prometheus is the de-facto standard for monitoring in the cloud age. In a classic monitoring scenario the monitoring system has a fixed set of IP addresses. As the cloud is dynamic in its nature a fixed list of IP addresses is not sufficient. Prometheus brings with it a modular approach. It employs service discovery to find the IP addresses and ports of the services it is supposed to gather information from. It then connects these services (e.g. a metrics server on each of your web servers) and collects any metrics they expose. These metrics are gathered in a time-series database and can be queried with a specialized query language called PromQL . Running Prometheus # The easiest way to run Prometheus is using a container. As a first step you need to create a configuration file. Let's say, you are putting this in /srv/prometheus.yml : global : scrape_interval : 15s scrape_configs : - job_name : 'prometheus' scrape_interval : 5s static_configs : - targets : [ 'localhost:9090' ] This configuration file will tell Prometheus to gather its own metrics. Having this configuration file, we can launch Prometheus: docker run \\ -d \\ -p 9090 :9090 \\ -v /srv/prometheus.yml:/etc/prometheus/prometheus.yml \\ prom/prometheus Warning This will run Prometheus on your host machine on port 9000. Prometheus itself does not have any authentication. Make sure to secure your server with security groups as needed. After running the previous command you can now enter the IP address of your server into your browser and access Prometheus on port 9000. The interface looks like this: Querying Prometheus # Prometheus is running and gathering its own metrics. You can check the successful metrics scraping by navigating to Status \u2192 Targets. You can also visualize the gathered metrics from the Graph interface by clicking the - insert metric at cursor - dropdown. For example, you can add the prometheus_http_requests_total metric to show the HTTP requests Prometheus has received on each endpoint: You can also summarize the HTTP requests by using the sum(prometheus_http_requests_total) formula: As you can see, this graph is ever-growing. This is because the graph type is a counter . Counters always increase. Let's get the requests per minute by applying the rate() function: sum(rate(prometheus_http_requests_total[1m])) You can read more about the available query functions in PromQL in the official documentation . Configuring a node exporter # Prometheus in and of itself does not do anything. It needs to read from an exporter. The easiest and probably most useful exporter to start with is the node exporter . The node exporter exposes the basic metrics of a machine, such as CPU and memory usage. Let's start by launching the node exporter on the same host as Prometheus: docker run -d \\ --net = \"host\" \\ --pid = \"host\" \\ -v \"/:/host:ro,rslave\" \\ quay.io/prometheus/node-exporter \\ --path.rootfs = /host In the next step you can configure the public IP of your Prometheus server to be scraped for metrics in your prometheus.yml : global : scrape_interval : 15s scrape_configs : - job_name : 'prometheus' scrape_interval : 5s static_configs : - targets : [ 'localhost:9090' ] - job_name : Monitoring Server Node Exporter static_configs : - targets : - '1.2.3.4:9100' Note The localhost:9090 and the 1.2.3.4:9100 are not the same! Remember, Prometheus runs in a container and has its own network stack. The node exporter on the other hand is running with --net=\"host\" so it shares the network stack of the host operating system. Let's restart Prometheus and then check Status \u2192 Targets to see if the node exporter shows up as a target. We can also look at the CPU metrics of our monitoring host: node_cpu_seconds_total As you can see, the CPU query results in several counters. This is the result of how CPU metrics gathering works. These separate metrics ( user , system , steal , softirq , nice , irq , iowait , and idle ) always add up to 100%. Let's query the CPU usage we actually care about: the CPU time used by our applications and our kernel. This can be done by filtering out the idle state and then comparing it to the whole: sum(rate(node_cpu_seconds_total{mode!=\"idle\"}[1m])) / sum(rate(node_cpu_seconds_total[1m])) Dynamic scraping # There is a wide range of service discovery options available . In our case we will implement the file SD . The file service discovery periodically checks a file for changes and reads the IP addresses in that file. Let's extend our configuration my the file SD configuration: global : scrape_interval : 15s scrape_configs : - job_name : 'prometheus' scrape_interval : 5s static_configs : - targets : [ 'localhost:9090' ] - job_name : Monitoring Server Node Exporter static_configs : - targets : - '1.2.3.4:9100' - job_name : Custom file_sd_configs : - files : - /custom_servers.json refresh_interval : 10s To make this work you will have to mount an additional volume for Prometheus: docker run \\ -d \\ -p 9090 :9090 \\ -v /srv/custom_servers.json:/custom_servers.json \\ -v /srv/prometheus.yml:/etc/prometheus/prometheus.yml \\ prom/prometheus Finally, let's place the following file in /srv/custom_servers.json on the host: [ { \"targets\" : [ \"1.2.3.4:9100\" ] } ] You can enter the IP address of a second server in this file to enable scraping and restart Prometheus this one last time to enable loading this file. When you change this file in the future Prometheus will automatically pick up the changes. Prometheus will connect the second server and fetch the metrics from that server, which you should then be able to query. Let's repeat the PromQL query from the example above to get the CPU usage. You will see that it won't give you two graphs. That's because it will summarize the CPU usage. Let's group it by the IP address: sum by (instance) (rate(node_cpu_seconds_total{mode!=\"idle\"}[1m])) / sum by (instance) (rate(node_cpu_seconds_total[1m])) The only thing left to do is to write an automatic cron job that fetches the IP address list from the cloud provider (Exoscale in our case) and populates this file automatically. Tip Prometheus would be able to do the alerting behavior required for the project work itself without Grafana using the Alert Manager . This is also a valid and accepted alternative, but for the ease of configuration we are going to demonstrate Grafana in the next exercise .","title":"4. Prometheus"},{"location":"exercises/4-prometheus/#running_prometheus","text":"The easiest way to run Prometheus is using a container. As a first step you need to create a configuration file. Let's say, you are putting this in /srv/prometheus.yml : global : scrape_interval : 15s scrape_configs : - job_name : 'prometheus' scrape_interval : 5s static_configs : - targets : [ 'localhost:9090' ] This configuration file will tell Prometheus to gather its own metrics. Having this configuration file, we can launch Prometheus: docker run \\ -d \\ -p 9090 :9090 \\ -v /srv/prometheus.yml:/etc/prometheus/prometheus.yml \\ prom/prometheus Warning This will run Prometheus on your host machine on port 9000. Prometheus itself does not have any authentication. Make sure to secure your server with security groups as needed. After running the previous command you can now enter the IP address of your server into your browser and access Prometheus on port 9000. The interface looks like this:","title":"Running Prometheus"},{"location":"exercises/4-prometheus/#querying_prometheus","text":"Prometheus is running and gathering its own metrics. You can check the successful metrics scraping by navigating to Status \u2192 Targets. You can also visualize the gathered metrics from the Graph interface by clicking the - insert metric at cursor - dropdown. For example, you can add the prometheus_http_requests_total metric to show the HTTP requests Prometheus has received on each endpoint: You can also summarize the HTTP requests by using the sum(prometheus_http_requests_total) formula: As you can see, this graph is ever-growing. This is because the graph type is a counter . Counters always increase. Let's get the requests per minute by applying the rate() function: sum(rate(prometheus_http_requests_total[1m])) You can read more about the available query functions in PromQL in the official documentation .","title":"Querying Prometheus"},{"location":"exercises/4-prometheus/#configuring_a_node_exporter","text":"Prometheus in and of itself does not do anything. It needs to read from an exporter. The easiest and probably most useful exporter to start with is the node exporter . The node exporter exposes the basic metrics of a machine, such as CPU and memory usage. Let's start by launching the node exporter on the same host as Prometheus: docker run -d \\ --net = \"host\" \\ --pid = \"host\" \\ -v \"/:/host:ro,rslave\" \\ quay.io/prometheus/node-exporter \\ --path.rootfs = /host In the next step you can configure the public IP of your Prometheus server to be scraped for metrics in your prometheus.yml : global : scrape_interval : 15s scrape_configs : - job_name : 'prometheus' scrape_interval : 5s static_configs : - targets : [ 'localhost:9090' ] - job_name : Monitoring Server Node Exporter static_configs : - targets : - '1.2.3.4:9100' Note The localhost:9090 and the 1.2.3.4:9100 are not the same! Remember, Prometheus runs in a container and has its own network stack. The node exporter on the other hand is running with --net=\"host\" so it shares the network stack of the host operating system. Let's restart Prometheus and then check Status \u2192 Targets to see if the node exporter shows up as a target. We can also look at the CPU metrics of our monitoring host: node_cpu_seconds_total As you can see, the CPU query results in several counters. This is the result of how CPU metrics gathering works. These separate metrics ( user , system , steal , softirq , nice , irq , iowait , and idle ) always add up to 100%. Let's query the CPU usage we actually care about: the CPU time used by our applications and our kernel. This can be done by filtering out the idle state and then comparing it to the whole: sum(rate(node_cpu_seconds_total{mode!=\"idle\"}[1m])) / sum(rate(node_cpu_seconds_total[1m]))","title":"Configuring a node exporter"},{"location":"exercises/4-prometheus/#dynamic_scraping","text":"There is a wide range of service discovery options available . In our case we will implement the file SD . The file service discovery periodically checks a file for changes and reads the IP addresses in that file. Let's extend our configuration my the file SD configuration: global : scrape_interval : 15s scrape_configs : - job_name : 'prometheus' scrape_interval : 5s static_configs : - targets : [ 'localhost:9090' ] - job_name : Monitoring Server Node Exporter static_configs : - targets : - '1.2.3.4:9100' - job_name : Custom file_sd_configs : - files : - /custom_servers.json refresh_interval : 10s To make this work you will have to mount an additional volume for Prometheus: docker run \\ -d \\ -p 9090 :9090 \\ -v /srv/custom_servers.json:/custom_servers.json \\ -v /srv/prometheus.yml:/etc/prometheus/prometheus.yml \\ prom/prometheus Finally, let's place the following file in /srv/custom_servers.json on the host: [ { \"targets\" : [ \"1.2.3.4:9100\" ] } ] You can enter the IP address of a second server in this file to enable scraping and restart Prometheus this one last time to enable loading this file. When you change this file in the future Prometheus will automatically pick up the changes. Prometheus will connect the second server and fetch the metrics from that server, which you should then be able to query. Let's repeat the PromQL query from the example above to get the CPU usage. You will see that it won't give you two graphs. That's because it will summarize the CPU usage. Let's group it by the IP address: sum by (instance) (rate(node_cpu_seconds_total{mode!=\"idle\"}[1m])) / sum by (instance) (rate(node_cpu_seconds_total[1m])) The only thing left to do is to write an automatic cron job that fetches the IP address list from the cloud provider (Exoscale in our case) and populates this file automatically. Tip Prometheus would be able to do the alerting behavior required for the project work itself without Grafana using the Alert Manager . This is also a valid and accepted alternative, but for the ease of configuration we are going to demonstrate Grafana in the next exercise .","title":"Dynamic scraping"},{"location":"exercises/5-grafana/","text":"Grafana Grafana is a common way to visualize information from multiple source systems, including Prometheus . It offers a user friendly way to create graphs, alters, and display metrics. Running Grafana # Like Prometheus before, Grafana can be run in a container: docker run -d \\ -p 3000:3000 \\ grafana/grafana This will launch Grafana on port 3000 of your node. Setting up Grafana with Prometheus # Grafana is an aggregator for information from many different systems. Data sources can be configured in the Configuration \u2192 Data sources menu on the left side. When adding a Prometheus data source you simply have to provide Grafana with your Prometheus URL. If you did not deploy any additional changes in the previous exercise you can leave everything else on default settings. Creating a CPU graph # Once you have the data source set up you can create a new dashboard and add a panel. The panel can have different visualization types, but will always draw its data from a query in the bottom box. Make sure to select the Prometheus data source and you will be presented with an option to enter your PromQL query. In addition, you can also provide sophisticated legend information, such as `` to list the instance name. On the right hand side you can adjust various display options, for example what range and display format the Axis' should have in the graph. Creating an alert # In the same interface as above you can create an alert. The alert will allow you to set an evaluation period to determine how often the rule is evaluated. You can also set the For field to allow a grace period before the alert triggers. The conditions give you a way to create a flexible set of rules when an alert should be triggered. You can also set up notifications to send the alert to various notification channels. This functionality can be used to trigger the scaling behavior required in the project work. Setting up a notification channel # Before you can set up an alert notification you have to create a notification channel. This can be done from the left hand menu by going to Alerting \u2192 Notification channels. For the purposes of the project work the notification type should be \"webhook\". The URL should point to your custom web server that triggers Exoscale to increase the instance pool size. Finally, you should also set up the \"Send reminders\" option to keep triggering the scale up/down behavior if the CPU usage is still high/low. Deploying Grafana in an automated fashion # Since our project work revolves around Terraform we need a way to deploy Grafana with all settings in an automated fashion. Thankfully, the Grafana developers have thought of this and provided us with a way to do that . Provisioning data sources # To provision data sources we must place or mount the data source configuration file in the /etc/grafana/provisioning/datasources/ directory inside the Grafana container in YAML format. For Prometheus this data source could look like this: apiVersion : 1 datasources : - name : Prometheus type : prometheus access : proxy orgId : 1 url : http://prometheus:9090 version : 1 editable : false Provisioning a notification channel # Notification channels can also be provisioned by placing the appropriate YAML file in /etc/grafana/provisioning/notifiers/ : notifiers : - name : Scale up type : webhook uid : scale-up org_id : 1 is_default : false send_reminder : true disable_resolve_message : true frequency : \"2m\" settings : autoResolve : true httpMethod : \"POST\" severity : \"critical\" uploadImage : false url : \"http://autoscaler:8090/up\" Note, that the uid field of the notifier matters as this will be referenced from the dashboard. Provisioning dashboards # Provisioning dashboards is slightly more complex. As a first step we must tell Grafana to look in a certain directory for the dashboard configuration file. Grafana will periodically check this directory. This can be done from a YAML file in the /etc/grafana/provisioning/dashboards/ directory: apiVersion : 1 providers : - name : 'Home' orgId : 1 folder : '' type : file updateIntervalSeconds : 10 options : path : /etc/grafana/dashboards We can then place the dashboard JSON file in the specified directory. The easiest way to create a JSON file is to manually create it in Grafana and then copy the JSON from the Grafana dashboard. Keep in mind that you may have to adjust the JSON manually to match your notification channel uid values.","title":"5. Grafana"},{"location":"exercises/5-grafana/#running_grafana","text":"Like Prometheus before, Grafana can be run in a container: docker run -d \\ -p 3000:3000 \\ grafana/grafana This will launch Grafana on port 3000 of your node.","title":"Running Grafana"},{"location":"exercises/5-grafana/#setting_up_grafana_with_prometheus","text":"Grafana is an aggregator for information from many different systems. Data sources can be configured in the Configuration \u2192 Data sources menu on the left side. When adding a Prometheus data source you simply have to provide Grafana with your Prometheus URL. If you did not deploy any additional changes in the previous exercise you can leave everything else on default settings.","title":"Setting up Grafana with Prometheus"},{"location":"exercises/5-grafana/#creating_a_cpu_graph","text":"Once you have the data source set up you can create a new dashboard and add a panel. The panel can have different visualization types, but will always draw its data from a query in the bottom box. Make sure to select the Prometheus data source and you will be presented with an option to enter your PromQL query. In addition, you can also provide sophisticated legend information, such as `` to list the instance name. On the right hand side you can adjust various display options, for example what range and display format the Axis' should have in the graph.","title":"Creating a CPU graph"},{"location":"exercises/5-grafana/#creating_an_alert","text":"In the same interface as above you can create an alert. The alert will allow you to set an evaluation period to determine how often the rule is evaluated. You can also set the For field to allow a grace period before the alert triggers. The conditions give you a way to create a flexible set of rules when an alert should be triggered. You can also set up notifications to send the alert to various notification channels. This functionality can be used to trigger the scaling behavior required in the project work.","title":"Creating an alert"},{"location":"exercises/5-grafana/#setting_up_a_notification_channel","text":"Before you can set up an alert notification you have to create a notification channel. This can be done from the left hand menu by going to Alerting \u2192 Notification channels. For the purposes of the project work the notification type should be \"webhook\". The URL should point to your custom web server that triggers Exoscale to increase the instance pool size. Finally, you should also set up the \"Send reminders\" option to keep triggering the scale up/down behavior if the CPU usage is still high/low.","title":"Setting up a notification channel"},{"location":"exercises/5-grafana/#deploying_grafana_in_an_automated_fashion","text":"Since our project work revolves around Terraform we need a way to deploy Grafana with all settings in an automated fashion. Thankfully, the Grafana developers have thought of this and provided us with a way to do that .","title":"Deploying Grafana in an automated fashion"},{"location":"exercises/5-grafana/#provisioning_data_sources","text":"To provision data sources we must place or mount the data source configuration file in the /etc/grafana/provisioning/datasources/ directory inside the Grafana container in YAML format. For Prometheus this data source could look like this: apiVersion : 1 datasources : - name : Prometheus type : prometheus access : proxy orgId : 1 url : http://prometheus:9090 version : 1 editable : false","title":"Provisioning data sources"},{"location":"exercises/5-grafana/#provisioning_a_notification_channel","text":"Notification channels can also be provisioned by placing the appropriate YAML file in /etc/grafana/provisioning/notifiers/ : notifiers : - name : Scale up type : webhook uid : scale-up org_id : 1 is_default : false send_reminder : true disable_resolve_message : true frequency : \"2m\" settings : autoResolve : true httpMethod : \"POST\" severity : \"critical\" uploadImage : false url : \"http://autoscaler:8090/up\" Note, that the uid field of the notifier matters as this will be referenced from the dashboard.","title":"Provisioning a notification channel"},{"location":"exercises/5-grafana/#provisioning_dashboards","text":"Provisioning dashboards is slightly more complex. As a first step we must tell Grafana to look in a certain directory for the dashboard configuration file. Grafana will periodically check this directory. This can be done from a YAML file in the /etc/grafana/provisioning/dashboards/ directory: apiVersion : 1 providers : - name : 'Home' orgId : 1 folder : '' type : file updateIntervalSeconds : 10 options : path : /etc/grafana/dashboards We can then place the dashboard JSON file in the specified directory. The easiest way to create a JSON file is to manually create it in Grafana and then copy the JSON from the Grafana dashboard. Keep in mind that you may have to adjust the JSON manually to match your notification channel uid values.","title":"Provisioning dashboards"},{"location":"exercises/6-api-integration/","text":"API integration Cloud providers offer a wide range of tools to interact with their API's. Most importantly, every cloud provider offers a command line tool. With larger cloud providers this is often the better choice compared to their web interface. The cloud provider for the project work, Exoscale, offers a CLI for all major platforms . Tip The source code for this exercise is available on GitHub . Using the CLI # When basic scripting is required the CLI may entirely be enough for the task. For example, you can query the list of instances in an instace pool as follows: exo instancepool show INSTANCE-POOL-NAME --output-format json You can use this command to generate your service discovery JSON file. Similarly, you could change the instance pool size: exo instancepool update test --size 5 Using Go # When it comes to creating a web server the CLI solution usually goes out the window because it is quite hard to create a webserver that calls a shell script. (It involves creating a CGI script, which is an easy way to also create a security hole.) Go being a popular language for the cloud world many cloud providers offer an SDK for this language. It also has a built-in library for creating web servers which makes it ideal to receive webhooks and react on them. As a first step for this exercise let's create a file called go.mod to enable Go module support. (If you are unfamiliar with Go you really, really don't want to write Go code without this.) module github.com/yourname/go-example go 1.14 require github.com/exoscale/egoscale v0.27.0 As you can see, we already pulled in the egoscale library. This is the SDK for Exoscale in Go. Let's create a simple main.go that increases the size of an instance pool: package main import ( \"context\" \"flag\" \"log\" \"github.com/exoscale/egoscale\" ) func main () { zoneId , err := egoscale . ParseUUID ( \"zone-id-here\" ) if err != nil { log . Fatalf ( \"invalid zone ID (%v)\" , err ) } poolId , err := egoscale . ParseUUID ( \"instance-pool-id-here\" ) if err != nil { log . Fatalf ( \"invalid pool ID (%v)\" , err ) } // Create a new client client := egoscale . NewClient ( \"https://api.exoscale.ch/v1/\" , \"api-key-here\" , \"secret-here\" ) ctx := context . Background () //Request the current size of the instance pool resp , err := client . RequestWithContext ( ctx , egoscale . GetInstancePool { ZoneID : zoneId , ID : poolId , }) response := resp .( egoscale . GetInstancePoolResponse ) if len ( response . InstancePools ) == 0 { log . Fatalf ( \"instance pool not found\" ) } else if len ( response . InstancePools ) > 1 { //This should never happen log . Fatalf ( \"more than one instance pool returned\" ) } instancePool := response . InstancePools [ 0 ] //Resize the instance pool _ , err = client . RequestWithContext ( ctx , egoscale . ScaleInstancePool { ZoneID : zoneId , ID : poolId , Size : instancePool . Size + 1 , }) if err != nil { log . Fatalf ( \"Failed to increase instance pool size (%v)\" , err ) } } This little program will increase the instance pool size by 1. You can run it with go run main.go . Python # A second popular language for cloud programming is Python. Exoscale also has an SDK for Python . Note The Python library for Exoscale currently does not work on Windows . First, let's create a requirements.txt : exoscale Then we can run pip install -r requirements.txt and then implement this simple program: import exoscale if __name__ == \"__main__\" : exo = exoscale . Exoscale ( api_key = \"api-key-here\" , api_secret = \"api-secret-here\" , config_file = \"\" ) zone = exo . compute . get_zone ( \"zone-name-here\" ) ip = exo . compute . get_instance_pool ( \"instance-pool-id-here\" , zone = zone ) ip . scale ( ip . size + 1 ) That's it! Other languages # Like every cloud provider, Exoscale provides a well documented API . There are third party SDK's available for several languages: Java Javascript C# Warning We have not tested these SDKs. We recommend sticking to the officially supported languages and may not be able to help if you work with the unofficial libraries.","title":"6. API integration"},{"location":"exercises/6-api-integration/#using_the_cli","text":"When basic scripting is required the CLI may entirely be enough for the task. For example, you can query the list of instances in an instace pool as follows: exo instancepool show INSTANCE-POOL-NAME --output-format json You can use this command to generate your service discovery JSON file. Similarly, you could change the instance pool size: exo instancepool update test --size 5","title":"Using the CLI"},{"location":"exercises/6-api-integration/#using_go","text":"When it comes to creating a web server the CLI solution usually goes out the window because it is quite hard to create a webserver that calls a shell script. (It involves creating a CGI script, which is an easy way to also create a security hole.) Go being a popular language for the cloud world many cloud providers offer an SDK for this language. It also has a built-in library for creating web servers which makes it ideal to receive webhooks and react on them. As a first step for this exercise let's create a file called go.mod to enable Go module support. (If you are unfamiliar with Go you really, really don't want to write Go code without this.) module github.com/yourname/go-example go 1.14 require github.com/exoscale/egoscale v0.27.0 As you can see, we already pulled in the egoscale library. This is the SDK for Exoscale in Go. Let's create a simple main.go that increases the size of an instance pool: package main import ( \"context\" \"flag\" \"log\" \"github.com/exoscale/egoscale\" ) func main () { zoneId , err := egoscale . ParseUUID ( \"zone-id-here\" ) if err != nil { log . Fatalf ( \"invalid zone ID (%v)\" , err ) } poolId , err := egoscale . ParseUUID ( \"instance-pool-id-here\" ) if err != nil { log . Fatalf ( \"invalid pool ID (%v)\" , err ) } // Create a new client client := egoscale . NewClient ( \"https://api.exoscale.ch/v1/\" , \"api-key-here\" , \"secret-here\" ) ctx := context . Background () //Request the current size of the instance pool resp , err := client . RequestWithContext ( ctx , egoscale . GetInstancePool { ZoneID : zoneId , ID : poolId , }) response := resp .( egoscale . GetInstancePoolResponse ) if len ( response . InstancePools ) == 0 { log . Fatalf ( \"instance pool not found\" ) } else if len ( response . InstancePools ) > 1 { //This should never happen log . Fatalf ( \"more than one instance pool returned\" ) } instancePool := response . InstancePools [ 0 ] //Resize the instance pool _ , err = client . RequestWithContext ( ctx , egoscale . ScaleInstancePool { ZoneID : zoneId , ID : poolId , Size : instancePool . Size + 1 , }) if err != nil { log . Fatalf ( \"Failed to increase instance pool size (%v)\" , err ) } } This little program will increase the instance pool size by 1. You can run it with go run main.go .","title":"Using Go"},{"location":"exercises/6-api-integration/#python","text":"A second popular language for cloud programming is Python. Exoscale also has an SDK for Python . Note The Python library for Exoscale currently does not work on Windows . First, let's create a requirements.txt : exoscale Then we can run pip install -r requirements.txt and then implement this simple program: import exoscale if __name__ == \"__main__\" : exo = exoscale . Exoscale ( api_key = \"api-key-here\" , api_secret = \"api-secret-here\" , config_file = \"\" ) zone = exo . compute . get_zone ( \"zone-name-here\" ) ip = exo . compute . get_instance_pool ( \"instance-pool-id-here\" , zone = zone ) ip . scale ( ip . size + 1 ) That's it!","title":"Python"},{"location":"exercises/6-api-integration/#other_languages","text":"Like every cloud provider, Exoscale provides a well documented API . There are third party SDK's available for several languages: Java Javascript C# Warning We have not tested these SDKs. We recommend sticking to the officially supported languages and may not be able to help if you work with the unofficial libraries.","title":"Other languages"},{"location":"glossary/","text":"API # Application Program Interfaces are methods for programs to exchange data without the involvement of humans. APIs often involve a schematic description of how the data looks like and where it needs to be sent. Today many APIs are based on HTTP . Container # Containers are discussed in lecture 4 and provide a way to package an application with its runtime requirement in a self-contained manner. Containers also provide limited security isolation. Container Orchestrator # A container orchestrator is responsible for managing containers across multiple virtual or physical servers. When a server fails or a container crashes the orchestrator is responsible for scheduling the container on a different server. Some orchestrators also manage integration with the cloud. Orchestrators are discussed in lecture 4 CDN # A content delivery network replicates content across the globe to bring the content closer to the end user. CDNs are discussed in lecture 3 . DBaaS # Databases as a Service are discussed in lecture 3 providing managed databases to a cloud customer without needing to manage a server, or the database system. Docker # Docker is one of the earliest modern containerization systems. Docker was largely responsible for promoting the immutable infrastructure concept in containers. Docker also contains the Docker Swarm orchestrator, a very simple system to manage containers across multiple machines. Docker is discussed in lecture 4 and exercise 3 . FaaS # Functions as a Service are a concept discussed in lecture 3 that allow a developer to run program code without needing to manage a server or a runtime environment. Grafana # Grafana is an open source graphing dashboard. It is discussed in exercise 5 . HTTP # The Hypertext Transfer Protocol is the protocol that powers the world wide web allowing for the easy up- and download of data. With HTTP each file (resource) has a unique URL on a server and can be linked. IaaS # Infrastructure as a Service is discussed in lecture 2 and provides virtual machines and related services to cloud customers. IOMMU # The IOMMU is the memory management unit that virtualizes the Direct Memory Access (DMA) requests from I/O components and separates them from each other to prevent attacks over Thunderbolt/USB. Istio # Istio is a service mesh used in conjunction with Kubernetes to facilitate microservices. Istio is discussed in lecture 5 Kubernetes # Kubernetes is an advanced, and thus complex container orchestrator. It not only manages containers, it also contains a large number of cloud integrations for storage, networking, load balancers, autoscaling, and more. Kubernetes is discussed in lecture 4 Load balancer # Load balancers provide either network or application level traffic distribution across multiple servers. They are discussed in lecture 2 and lecture 3 Microservices # Microservices are a concept discussed in lecture 5 for creating multiple small applications working together across the network. MMU # The Memory Management Unit is the component of the CPU that translates virtual memory addresses to real addresses. It is used to separate applications from each other. PaaS # Platform as a Service is a collection of services discussed in lecture 3 that give a developer the ability to deploy an application without needing to manage IaaS servers. Prometheus # Prometheus is an open source, cloud native metrics collection system (time series database). It is discussed in lecture 5 and exercise 4 . Rack # A closet with standardized mounts for servers. Router # A network device that forwards layer 3 (IP) packets between separate networks. Switch # A network device that forwards Ethernet frames (packets) between devices. It does not perform layer 3 (IP) routing.","title":"A-Z"},{"location":"glossary/#api","text":"Application Program Interfaces are methods for programs to exchange data without the involvement of humans. APIs often involve a schematic description of how the data looks like and where it needs to be sent. Today many APIs are based on HTTP .","title":"API"},{"location":"glossary/#container","text":"Containers are discussed in lecture 4 and provide a way to package an application with its runtime requirement in a self-contained manner. Containers also provide limited security isolation.","title":"Container"},{"location":"glossary/#container_orchestrator","text":"A container orchestrator is responsible for managing containers across multiple virtual or physical servers. When a server fails or a container crashes the orchestrator is responsible for scheduling the container on a different server. Some orchestrators also manage integration with the cloud. Orchestrators are discussed in lecture 4","title":"Container Orchestrator"},{"location":"glossary/#cdn","text":"A content delivery network replicates content across the globe to bring the content closer to the end user. CDNs are discussed in lecture 3 .","title":"CDN"},{"location":"glossary/#dbaas","text":"Databases as a Service are discussed in lecture 3 providing managed databases to a cloud customer without needing to manage a server, or the database system.","title":"DBaaS"},{"location":"glossary/#docker","text":"Docker is one of the earliest modern containerization systems. Docker was largely responsible for promoting the immutable infrastructure concept in containers. Docker also contains the Docker Swarm orchestrator, a very simple system to manage containers across multiple machines. Docker is discussed in lecture 4 and exercise 3 .","title":"Docker"},{"location":"glossary/#faas","text":"Functions as a Service are a concept discussed in lecture 3 that allow a developer to run program code without needing to manage a server or a runtime environment.","title":"FaaS"},{"location":"glossary/#grafana","text":"Grafana is an open source graphing dashboard. It is discussed in exercise 5 .","title":"Grafana"},{"location":"glossary/#http","text":"The Hypertext Transfer Protocol is the protocol that powers the world wide web allowing for the easy up- and download of data. With HTTP each file (resource) has a unique URL on a server and can be linked.","title":"HTTP"},{"location":"glossary/#iaas","text":"Infrastructure as a Service is discussed in lecture 2 and provides virtual machines and related services to cloud customers.","title":"IaaS"},{"location":"glossary/#iommu","text":"The IOMMU is the memory management unit that virtualizes the Direct Memory Access (DMA) requests from I/O components and separates them from each other to prevent attacks over Thunderbolt/USB.","title":"IOMMU"},{"location":"glossary/#istio","text":"Istio is a service mesh used in conjunction with Kubernetes to facilitate microservices. Istio is discussed in lecture 5","title":"Istio"},{"location":"glossary/#kubernetes","text":"Kubernetes is an advanced, and thus complex container orchestrator. It not only manages containers, it also contains a large number of cloud integrations for storage, networking, load balancers, autoscaling, and more. Kubernetes is discussed in lecture 4","title":"Kubernetes"},{"location":"glossary/#load_balancer","text":"Load balancers provide either network or application level traffic distribution across multiple servers. They are discussed in lecture 2 and lecture 3","title":"Load balancer"},{"location":"glossary/#microservices","text":"Microservices are a concept discussed in lecture 5 for creating multiple small applications working together across the network.","title":"Microservices"},{"location":"glossary/#mmu","text":"The Memory Management Unit is the component of the CPU that translates virtual memory addresses to real addresses. It is used to separate applications from each other.","title":"MMU"},{"location":"glossary/#paas","text":"Platform as a Service is a collection of services discussed in lecture 3 that give a developer the ability to deploy an application without needing to manage IaaS servers.","title":"PaaS"},{"location":"glossary/#prometheus","text":"Prometheus is an open source, cloud native metrics collection system (time series database). It is discussed in lecture 5 and exercise 4 .","title":"Prometheus"},{"location":"glossary/#rack","text":"A closet with standardized mounts for servers.","title":"Rack"},{"location":"glossary/#router","text":"A network device that forwards layer 3 (IP) packets between separate networks.","title":"Router"},{"location":"glossary/#switch","text":"A network device that forwards Ethernet frames (packets) between devices. It does not perform layer 3 (IP) routing.","title":"Switch"},{"location":"grading/","text":"Grade construction # 55% written examination (individual). 45% project work (15% per sprint). Project work grading # You will incur a -15% penalty if your Exoscale account runs out of budget from the project work only. Make sure you monitor your Exoscale account. Sprint 1: # 5% for having a running instance pool and a network load balancer, running the designated load generator. 10% for implementing the instance pool and network load balancer setup in Terraform. Sprint 2: # 5% for having a working Prometheus and Grafana installation that monitors the instances in the instance pool. 5% for automating the installation procedure of Prometheus and Grafana via Terraform. 5% for implementing the Prometheus service discovery yourself. (Your solution must be in a different programming language than the provided example, or must be substantially different.) Sprint 3: # 5% for having a working autoscaling behavior. 5% for having a complete automated setup of the instance pools, NLB, monitoring server via Terraform. 5% for implementing the autoscaling web service yourself. (Your solution must be in a different programming language than the provided example, or must be substantially different.) Marks # Positive grade: Minimum 60% score in written exam plus 60% of overall score required. 1 2 3 4 5 90% \u2014 100% 80% \u2014 89% 70% \u2014 79% 60% \u2014 69% <60%","title":"Grading"},{"location":"grading/#grade_construction","text":"55% written examination (individual). 45% project work (15% per sprint).","title":"Grade construction"},{"location":"grading/#project_work_grading","text":"You will incur a -15% penalty if your Exoscale account runs out of budget from the project work only. Make sure you monitor your Exoscale account.","title":"Project work grading"},{"location":"grading/#sprint_1","text":"5% for having a running instance pool and a network load balancer, running the designated load generator. 10% for implementing the instance pool and network load balancer setup in Terraform.","title":"Sprint 1:"},{"location":"grading/#sprint_2","text":"5% for having a working Prometheus and Grafana installation that monitors the instances in the instance pool. 5% for automating the installation procedure of Prometheus and Grafana via Terraform. 5% for implementing the Prometheus service discovery yourself. (Your solution must be in a different programming language than the provided example, or must be substantially different.)","title":"Sprint 2:"},{"location":"grading/#sprint_3","text":"5% for having a working autoscaling behavior. 5% for having a complete automated setup of the instance pools, NLB, monitoring server via Terraform. 5% for implementing the autoscaling web service yourself. (Your solution must be in a different programming language than the provided example, or must be substantially different.)","title":"Sprint 3:"},{"location":"grading/#marks","text":"Positive grade: Minimum 60% score in written exam plus 60% of overall score required. 1 2 3 4 5 90% \u2014 100% 80% \u2014 89% 70% \u2014 79% 60% \u2014 69% <60%","title":"Marks"},{"location":"help/","text":"Everybody needs help, and since this is a new situation we have prepared a few channels where you can get help. Before you ask for help # Make sure you strip the code you have a problem with of all unnecessary code, comments, etc. Make sure you upload your code to GitHub so others can take a look at it. Make sure you think through and describe your problem in more detail as you would in a conversation, as digitally it may be harder to follow. Where to ask for help # The primary avenue of getting help is on the Slack channel for this course . Join the bi-weekly online consultation sessions. If you need to send us something, please do so on the email addresses on the introduction page .","title":"Getting help"},{"location":"help/#before_you_ask_for_help","text":"Make sure you strip the code you have a problem with of all unnecessary code, comments, etc. Make sure you upload your code to GitHub so others can take a look at it. Make sure you think through and describe your problem in more detail as you would in a conversation, as digitally it may be harder to follow.","title":"Before you ask for help"},{"location":"help/#where_to_ask_for_help","text":"The primary avenue of getting help is on the Slack channel for this course . Join the bi-weekly online consultation sessions. If you need to send us something, please do so on the email addresses on the introduction page .","title":"Where to ask for help"},{"location":"lectures/","text":"Lecture 1: Introduction # Cloud building components IaaS, PaaS, SaaS Public vs. Private Cloud Basic \u201chosting\u201d service vs. managed service Business Models: Overbooking, Pay-per-Use, Standardization & Automation Benefits/Risks/Regulations: Scalability, Privacy, GDPR, Safe Harbor/Privacy Shield Go to lecture \u00bb Lecture 2: IaaS introduction # The history and inner workings of virtualization Virtual machines as a Service Storage in the Cloud Networking in the Cloud Go to lecture \u00bb Lecture 3: Beyond IaaS # Application load balancers CDNs Object Storage Databases as a Service (DBaaS) Functions as a Service (FaaS / Lambda) Containers as a Service (CaaS) Stream processing Deployment pipelines Go to lecture \u00bb Lecture 4: Containers # Containers vs. Virtual Machines The Dockerfile format Container runtimes Container orchestrators Container networking Go to lecture \u00bb Lecture 5: Cloud-native software development # 12-factor Apps Microservices Service meshes Frameworks and tools Go to lecture \u00bb","title":"Overview"},{"location":"lectures/#lecture_1_introduction","text":"Cloud building components IaaS, PaaS, SaaS Public vs. Private Cloud Basic \u201chosting\u201d service vs. managed service Business Models: Overbooking, Pay-per-Use, Standardization & Automation Benefits/Risks/Regulations: Scalability, Privacy, GDPR, Safe Harbor/Privacy Shield Go to lecture \u00bb","title":"Lecture 1: Introduction"},{"location":"lectures/#lecture_2_iaas_introduction","text":"The history and inner workings of virtualization Virtual machines as a Service Storage in the Cloud Networking in the Cloud Go to lecture \u00bb","title":"Lecture 2: IaaS introduction"},{"location":"lectures/#lecture_3_beyond_iaas","text":"Application load balancers CDNs Object Storage Databases as a Service (DBaaS) Functions as a Service (FaaS / Lambda) Containers as a Service (CaaS) Stream processing Deployment pipelines Go to lecture \u00bb","title":"Lecture 3: Beyond IaaS"},{"location":"lectures/#lecture_4_containers","text":"Containers vs. Virtual Machines The Dockerfile format Container runtimes Container orchestrators Container networking Go to lecture \u00bb","title":"Lecture 4: Containers"},{"location":"lectures/#lecture_5_cloud-native_software_development","text":"12-factor Apps Microservices Service meshes Frameworks and tools Go to lecture \u00bb","title":"Lecture 5: Cloud-native software development"},{"location":"lectures/1-cloud-intro/","text":"Download PPTX \ud83d\udcbb Download MP3 \ud83c\udfa7 Watch Video \ud83c\udfac Introduction to the Cloud As the popular saying goes: The cloud is just somebody else's computer. There is no magic, just using the cloud does not magically solve problems we are having with a traditional infrastructure. This lecture will teach you how a cloud is built and what the typical services are that they offer as well as the pitfalls which may come with such setups. What is a Server? # In a hurry? Servers: Nowadays either x86 or ARM architecture. Redundant, hot-swap hardware (power supply, fans, etc). Flat build profile for rack mounts. Out-Of-Bounds management interface for monitoring and remote management. Power consumption is a concern. Buying servers can take a long time and present an up-front investment. In older times, servers were completely different from the machines we used for regular work. Not just in weight and form factor, but in architecture. The landscape would stretch from SPARC and Power architectures to the x86 architecture we use in our PCs. Over time, however, the x86 architecture took over to the point where a server today, from an architectural standpoint, is exactly the same as the machine you are using right now. This means that you can copy the machine you are using onto a server and chances are it will run without any modification. The only notable exception is the rise of the ARM architecture which is popular in phones and tables and is known for its low power consumption. In recent years there has been a small but noticeable trend to run ARM in a datacenter. At home it may not matter if your computer uses 200 or 300 watts of power. In a datacenter, at the scale of hundreds, thousands or tens of thousands of servers, saving even a few percent of power will translate to huge cost savings. The difference in build is, however, quite apparent. While some servers, mainly built for office use, have the standard \u201ctower\u201d build, most servers have a flat profile designed to be mounted in racks as displayed on the picture. Since most servers are not high enough to take full size expansion cards (graphics cards, network cards, etc), servers may contain a separate removable component for these usually called a riser. An HP Proliant G5 server pulled out of its rack. Source: Wikipedia Server racks are standardized closets that have mounting screws for rails that allow pulling servers out even mid operation and replace components while the server is running. Servers also come with a high level of redundancy. While you may have a single power supply in your home computer, servers typically have two that are able to take power from different power inputs. This makes sure that the server keeps running even if one of the two power supplies fail, or if one of the two power inputs goes down. Also in contrast to your home setup these servers contain an Out-Of-Bounds Management interface that allows remote management of servers even when they are turned off. The hardware components built into the server report their health status to this OOB management interface which allows for the simultaneous monitoring of thousands of machines. Note This is a fairly generic description of servers. Different vendors may choose to leave out certain features of their more \u201cbudget\u201d line of servers or call certain components differently. HP, for example, calls their OOBM \u201cIntegrated Lights Out\u201d, Dell \u201cDRAC - Dell Remote Access Control\u201d, etc. When it comes to purchasing servers, larger companies tend to go with the same components for a longer period of time and they also buy support from the vendor. This sometimes includes hardware replacement done entirely by the vendor in the datacenter without the need for the customer to have staff on site. However, purchasing a specific set of components or ordering larger quantities of servers presents a logistics challenge and can sometimes take up to 3-4 months. Buying hardware is also an up-front investment which is hard to justify when demands change rapidly. What components are redundant in a server? Power supply CPU RAM Fan OOBM What is the purpose of an OOBM? Remotely manage a server Receive hardware malfunction alerts The Anatomy of a Datacenter # In a hurry? Datacenter components: Racks to house servers. Larger customers have cages for their racks. Cabling under the floor. Redundant cooling, fire suppression systems and power supply. Some datacenters provide internet connectivity. Eco friendliness is becoming a factor. Since the cloud is just somebody else's computer, that computer needs to be hosted somewhere. Servers are almost exclusively hosted in datacenters. Let's take a look at what is involved in running a datacenter. First of all, as mentioned above, most servers are going to be rack-mounted, so you need a bunch of racks. These racks are installed in rows, often with a fake floor to allow for cabling to go under the floor. A datacenter with racks. Source: Wikipedia Since servers produce a lot of heat, a datacenter also requires cooling. There are a variety of ways to solve cooling, some are more \u201cgreen\u201d than others. Some datacenters, for example, opt to install a \u201ccold aisles\u201d where the cold air is pumped between two rack rows and is pushed through the racks to cool the servers. Apart from cooling, datacenters also require automated fire suppression systems simply because of the amount of electricity going through. Datacenters usually go with a non-destructive fire suppression system such as lowering the oxygen content of the air enough to stop the fire. All critical systems in a datacenter (power, cooling, fire suppression systems) are usually built in a redundant fashion because the loss of either of those systems will potentially mean a complete shutdown for the datacenter. Datacenter operators usually have further contingency plans in place, too, such as a UPS (battery) system, diesel generator, fuel truck on standby, hotline to the fire department, etc. to make sure the datacenter can keep its required uptime. On the networking side of things, matters get slightly more complicated. Some datacenter providers also offer you the ability to use their network uplink (also redundant), but larger customers will prefer to host their own networking equipment and negotiate their own internet uplink contracts. Since there is no generic rule for how datacenters handle this, we will dispense with a description. It is also worth noting that larger customers (banks, cloud providers, etc) usually prefer to have their own racks in a separated gated area called a \u201ccage\u201d to which they control access. The Anatomy of the Internet # In a hurry? Internet: IP ranges are advertised using BGP. Providers connect direcly or using internet exchanges. 16 global providers form the backbone of the internet (tier 1). Once the physical infrastructure is set up there is also the question of how to connect to the Internet. As mentioned before, networks can be very complicated and there is no one size fits all solution. Smaller customers will typically use the network infrastructure provided by the datacenter while larger customers will host their own network equipment. Again, generally speaking, racks will be equipped with a Top-of-Rack switch to provide layer 2 (Ethernet) connectivity between servers. Several ToR may have interconnects between each other and are usually connected to one or more routers. Routers provide layer 3 (IP) routing to other customers in the same datacenter, internet exchange , or may be connected via dedicated fiber to another provider. Note If you are not familiar with computer networks we recommend giving the Geek University CCNA course a quick read . While you will not need everything, you will have to understand how IP addresses, netmasks, etc work in order to pass this course. Providers on the internet exchange data about which network they are hosting using the Border Gateway Protocol . Each provider's router announces the IP address ranges they are hosting to their peer providers, who in turn forward these announcements in an aggregated form to other providers. Providers have agreements with each other, or with an Internet Exchange, about exchanging a certain amount of traffic. These agreements may be paid if the traffic is very asymmetric or one provider is larger than the other. Alternatively providers can come to an arrangement to exchange traffic for free. Internet exchanges facilitate the exchange between many providers for a modest fee allowing cost-effective exchange of data. Depending on the exchange the rules are different. Local exchanges, for example, may only allow advertising local (in-country) addresses, while others are built for a specific purpose. Generally speaking, providers can be classified into 3 categories. Tier 1 providers are the global players that are present on every continent. They form the backbone of the Internet. At the time of writing there are 16 such networks . Tier 2 are the providers who are directly connected to the tier 1 providers, while tier 3 is everyone else. What IP addresses does a CIDR of 1.2.3.4/0 cover? Only the 1.2.3.4 IP address All IP addresses from 1.2.3.0 to 1.2.3.255 All IP addresses from 1.2.0.0 to 1.2.255.255 All IP addresses from 1.0.0.0 to 1.255.255.255 All IP addresses Is 192.168.2.1 in the network range 192.168.1.0/24 Yes No Given there are 3 providers A, B, and C. B is connected to A and C, but A and C are not directly connected. Which of the following is true? A and C directly exchange IP address range information via BGP. A and B directly exchange IP address range information via BGP. B and C directly exchange IP address range information via BGP. A and C cannot communicate. A and C can communicate. A forwards IP address range information from B to C A forwards IP address range information from C to B B forwards IP address range information from A to C B forwards IP address range information from C to A C forwards IP address range information from A to B C forwards IP address range information from B to A Software Stack # In a hurry? Software stack: Virtualization. Operating system. Application runtime. Application. The purpose of all this is, of course, to run an application. Each server hosts an operating system which is responsible for managing the hardware. Operating systems provide a simplified API for applications to do hardware-related operations such as dealing with files or talking to the network. This part of the operating system is called the kernel. Other parts form the userland. The userland includes user applications such as a logging software. Specifically on Linux and Unix systems the userland also contains a package manager used to install other software. Modern x86 server CPUs (and some desktop CPUs) also have a number of features that help with virtualization. Virtualization lets the server administrator run multiple guest operating systems efficiently and share the server resources between them. Did you know? You can find out if an Intel CPU supports hardware virtualization by looking for the VT-x feature on the Intel ARK . Unfortunately AMD does not have an easy to use list but you can look for the AMD-V feature on AMD CPUs. Note Virtualization is different from containerization (which we will talk about later) in that with virtualization each guest operating system has its own kernel whereas containers share a kernel between them. There is one more important aspect of finally getting an application to run: the runtime environment. Except for a few rare occasions applications need a runtime environment. If the application is compiled to machine code they still need so-called shared libraries. Shared libraries are common across multiple applications and can be installed and updated independently from the application itself. This makes for a more efficient update process, but also means that the right set of libraries need to be installed for applications. If the applications are written higher level languages like Java, Javascript, PHP, etc. they need the appropriate runtime environment for that language. One notable exception to the runtime environment requirement is the programming language Go . Go compiles everything normally located in libraries into a single binary along with the application. This makes it exceptionally simple to deploy Go applications into containers. The Cloud # In a hurry? Typical cloud features: API Dynamic scaling Can be classified into IaaS, PaaS and SaaS IaaS service offerings typically include: Virtualization. Network infrastructure. Everything required to run the above. PaaS service offerings typically include a managed service ready to be consumed by a developer. SaaS service offerings typically include a managed service ready to be consumed by a non-technical end user. All of the previously discussed things were available before the \u201ccloud\u201d. You could pay a provider to give you access to a virtual machine where you could run your applications. What changed with the cloud, however, is the fact that you no longer had to write a support ticket and everything became self service. The cloud age started with an infamous e-mail from Jeff Bezos to his engineers in 2002 forcing them to use APIs to exchange data between teams. The exact e-mail is no longer available but it went along these lines: 1) All teams will henceforth expose their data and functionality through service interfaces. 2) Teams must communicate with each other through these interfaces. 3) There will be no other form of interprocess communication allowed: no direct linking, no direct reads of another team\u2019s data store, no shared-memory model, no back-doors whatsoever. The only communication allowed is via service interface calls over the network. 4) It doesn\u2019t matter what technology is used. HTTP, Corba, Pubsub, custom protocols \u2014 doesn\u2019t matter. 5) All service interfaces, without exception, must be designed from the ground up to be externalizable. That is to say, the team must plan and design to be able to expose the interface to developers in the outside world. No exceptions. 6) Anyone who doesn\u2019t do this will be fired. This marked the beginning of Amazon Web Services the first and also the most successful public cloud offering. The first public release of AWS was in 2004 with SQS their message queue service, and got completely overhauled in 2006 where the Elastic Compute (EC2) and the Simple Storage Service (S3) service made its first public appearance. The APIs provided by cloud providers allow for a large amount of flexibility. If new servers are needed, they can be launched within a few minutes. If there are too many servers, they can be deleted. The same goes for other services: with the API (and the appropriate billing model) comes flexibility to adapt to change. The other factor that makes it easy to adapt to change is of course the fact that these services are managed. The cloud customer doesn't need to hire a team of engineers to build a database service, for example, it can be consumed without knowing how exactly the database is set up. If you want a consice description of what cloud computing is the US National Institute for Standards and Technologies has described cloud computing in publication 800-145 as follows: Cloud computing is a model for enabling ubiquitous, convenient, on-demand network access to a shared pool of configurable computing resources (e.g., networks, servers, storage, applications, and services) that can be rapidly provisioned and released with minimal management effort or service provider interaction. In more detail, NIST has defined the following characteristics: On-demand self-service. Broad network access. Resource pooling. Rapid elasticity. Measured service. Generally speaking, cloud services can be classified into three categories: Infrastructure as a Service (IaaS) providing virtual machines and network infrastructure, Platform as a Service (PaaS) offering services for developers to use, and Software as a Service (SaaS) offering end-user services. This is also described in the above NIST publication. Note SaaS will not be discussed in this course. Which of the following can be considered cloud providers? An in-house team that provides you with virtual machines when requested via ticket. A company that providers virtual machines as a service that are billed on a per-month basis. A company that offers a databases as a service on a monthly basis. An in-house service where you and other in-house teams can create virtual machines via an API or user interface automatically. A company that provides video conversion on a per-video price upon upload. When determining if a company is a true cloud provider which of the following do you look for? An API and/or user interface Can I create resources (servers, etc) immediately? Are my resources billed based on usage or per-minute/second Infrastructure as a Service (IaaS) # The most basic of cloud service offerings is IaaS. IaaS means that the cloud provider will manage the infrastructure used by the customer. Infrastructure in this sense means the ability to manage (provision, start,stop) virtual machines. In very rare cases some providers also offer \u201cbare metal\u201d machines in this fashion. However, most bare metal providers do not offer a true IaaS as machines cannot be ordered using an API, have an up-front fee and are billed on a monthly basis. IaaS also includes the network connectivity to the Internet. Almost all IaaS providers also offer a built-in firewall, virtual private networks (VPC) that can be used to connect virtual machines together without sending the traffic over the Internet, and other network services. Note that network services can differ greatly. For example, some providers implement private networks on a regional basis while other providers offer private networks that can be used to connect virtual machines that are located in different regions. Which of the following do you absolutely need to use an IaaS service? DevOps engineers, Linux or Windows system administrators Developers Neither Managed Services (PaaS) # In a hurry? Managed services: The provider is typically responsible for provisioning, backups, monitoring and restoring the service if needed. Low entry cost. Little in-house know-how required. Vendor lock-in for non-standard services. If problems arise they can be hard to debug. Apart from offering virtual machines and network infrastructure as a service many cloud providers also offer additional services that are typically used by developers such as managed databases. These services are managed in the sense that the developer does not need to run the operating system and the database software itself. However, the developer has to configure the database such that it works according to the needs of the application. The provider takes over duties like installing the service, making backups, monitoring, and restoring the service in case of an outage. Did you know? There are purely PaaS providers that do not offer IaaS, only a developer-friendly platform to run software on. Your run of the mill average PHP web hosting provider does qualify if you can order the service using an API. One notable example that rose to prominence is Heroku . The pricing of these managed services varies greatly but they usually follow the cloud model. Most cloud providers offer at least a low cost or even free entry version that has a small markup on top of the IaaS costs. The massive benefit of using these managed services is, of course, that you do not need to have in-house knowledge about how to operate them. You don't need an in-house database operator, you \u201cjust\u201d need to know how to set up the database in the cloud. This lets your company focus on the core business they are trying to build and outsource the know-how required to build these services on top of the IaaS layer. As you might imagine managed services also have downsides. Most importantly they present a clear vendor lock-in. This means that if you want to move to a different cloud provider you will have a hard time doing so if you are using one of the more specialized services. In other words the level of standardization across providers matters. It is also worth mentioning that managed services tend to work well for a large number of customers but a few number of customers can run into hard to debug problems. This debugging difficulty arises out of the inherent opacity of the services: you, the customer, don't see what's happening on the IaaS layer. If a database, for example, fills the available bandwidth may not be notified and are left guessing why your database misbehaves. Which of the following do you absolutely need to use a PaaS service? DevOps engineers, Linux or Windows system administrators Developers Neither What is a vendor lock-in? A contractual clause that forces you to use a certain cloud provider for a specified time. A technological incompatibility or hurdle that makes moving to a different provider hard. A cloud user uses virtual machines, network load balancers, network firewalls. Which is this? IaaS PaaS SaaS A cloud user's developers deploy their application purely as functions as a service and managed databases. Which is this? IaaS PaaS SaaS A cloud user uses a cloud offering using only the web interface. Which is this? IaaS PaaS SaaS Business Models # In a hurry? Billing models: IaaS is typically priced per-second based on the instance size. PaaS can be priced similar to IaaS but also per-request or data volume. Data volume is typically priced based on usage. Some providers charge for data volume even for internal traffic. The flexibility of cloud providers comes from their usage-based pricing. This can vary depending on the type of service used. For example, virtual machines (IaaS) is typically priced on a per-second basis. When you start a virtual machine for only 5 minutes you will only pay for 5 minutes of runtime. This lets you optimize the usage costs of the cloud if you use automation to start and stop machines or services on demand. Typically the cost savings are realized in these scenarios: On-off usage # This usage type is typical for batch processing. Machines are only started when there is a workload (e.g. video conversion, machine learning training job, etc.) Growth usage # Projects that see a growth curve often opt to use a cloud provider as well since buying hardware in larger quantities typically takes 2-4 months. Sudden spike usage # Cloud providers can also be useful if a service encounters a sudden load spike. This is typically the case for webshops around Black Friday and other sales events. Periodic spike usage # Almost every service has usage spikes depending on the time of day. This can be used to scale the service up and down. Per-request billing # IaaS services are typically priced based on an allocated amount of resources determined at the start of the virtual machines. Some PaaS services also use this billing model. (Typically Databases as a Service.) Other PaaS services often opt for a per-request or a data volume-based billing approach. Cost planning with the cloud # Cost planning is an important part of the job of a cloud architect. Depending on the billing model the cloud provider adopts this can be fairly simple to almost impossible. As a rule of thumb the billing model of larger cloud providers (AWS, Azure, Google Cloud) is more complex than smaller cloud providers (DigitalOcean, Exoscale, Upcloud, etc). It is worth noting that data transfer is typically charged based on volume. Some cloud providers even charge for internal traffic between availability zones. Cost-comparison with on premises systems # One of the most important cases for cost analysis will be the comparison with on premises architecture. It is quite common to create a cost comparison where the investment required for a hardware purchase is compared with the cloud cost over 3 or 5 years. These comparisons can be quite misleading because they often don't contain any cost attribution for the required work and they also do not use the advantages of the cloud in terms of scaling. Without these factors a hardware purchase will almost always be cheaper when calculated over 5 years and be roughly equal when compared over 3 years. Private vs. Public Cloud # In a hurry? Private cloud: Hosted on-premises or in the public cloud using only private connections. Large cloud providers offer their services \u201cin a box\u201d to host yourself. Most cloud applications will reside on a public cloud and be accessible from the Internet. However, for some use cases access over the public Internet is not desirable or even strictly forbidden by laws or regulations. In other cases companies may decide that it is their policy that certain systems must never be connected to the public Internet, or only connect via a self-hosted gateway. In these cases a private cloud is desirable to minimize the risk of cross-contamination or data exposure due to other tenants being on the same infrastructure. Such cross-contamination is not uncommon, for example in recent years there have been a litany of CPU bugs such as Meltdown and Spectre . A special form of a private cloud is a cloud offering that is dedicated to a special set of consumers, such as a research cloud with special hardware, or discounted rates for several research organizations or universities. The NIST classifies this as a community cloud . Public cloud providers have extended their offers to address these concerns. Their service offerings now include dedicated hypervisors to mitigate CPU bugs, the ability to connect the cloud over a non-Internet connection (e.g. MPLS tunnel) , or accessing the cloud API's from a private network . Some cloud providers even went as far offering a self-hosted setup where the public cloud offering can be brought on-premises. These features, especially the connectivity-related features, also allow for the creation of a hybrid cloud where a traditional infrastructure can be connected to the cloud. This gives a company the ability to leverage the flexibility of the cloud but also keep static / legacy systems for financial or engineering reasons. While the definitions outlined by NIST and other organizations in this matter are clear, real life rarely is. In a real-world scenario you will find a wide range of mixes comprised from all of the above. To you as a cloud engineer it is more important to understand the benefits and drawbacks of each so you can make a good recommendation depending on the use case. What are the advantages of a public cloud? Large amount of available resources Public clouds automatically solve scaling problems They are always cheaper than private clouds What are the advantages of a private cloud? Greater protection against hardware-bound attacks Easier interconnect with an on-premises environment They are always hosted on-premises Automation # In a hurry? Automation: Documents how a cloud is set up. Gives a reproducible environment. Allows for spinning up multiple copies of the same environment. Not all tools are equal. Some tools allow for manual changes after running them (e.g. Ansible), others don't (e.g. Terraform). As you can see from the previous sections cloud computing doesn't necessarily make it simpler to deploy an application. In this regard setting up a server and not documenting it is the same as setting up a cloud and not documenting it. In the end in both cases modifications will be hard to carry out later since details of the implementation are lost. It will also be hard to create a near-identical copy of the environment for development, testing, etc. purposes. This is partially due to the lack of documentation, and partially because of the manual work involved. In the past this problem was addressed by cloning virtual machines which was an approach with limited success as customizations were hard to make. The rise of API's brought a much welcome change: automation tools such as Puppet, Ansible and Terraform not only automate the installation of a certain software but also document how the setup works. When engineers first go into automation, they tend to reserve a part of the work to be done manually. For example they might use Ansible to create virtual machines and install some basic tools and install the rest of the software stack by hand. This approach is workable where the software stack cannot be automated but should be avoided for software where this is not the case as the lack of automation usually also means a lack of documentation as described above. Automation tools are also not equally suited for each task. Terraform, for example, expects full control of the infrastructure that is created by it. Manual installation steps afterwards are not supported. This approach gives Terraform the advantage that it can also remove the infrastructure it creates. Removing the infrastructure is paramount when there are multiple temporary environments deployed such as a dev or testing environment. Ansible, on the other hand, allows for manual changes but does not automatically implement a tear-down procedure for the environment that has been created. This makes Ansible environments, and Ansible code harder to test and maintain, but more suited for environments where traditional IT is still a concern. When would you use Ansible? I can use Terraform only once I already have my virtual machines installed. I can use Ansible to partially manage a server and manually configure other parts. I can use Ansible to fully manage my cloud environment. When would you use Terraform? I can use it to install servers and then install the software manually. I can use Terraform to implement immutable infrastructure. I can use Terraform only install software within a virtual machine. Regulation # In a hurry? GDPR: Applies to all companies, world-wide, that handle personal data of EU citizens. Companies must keep track of why and how data is handled. Data subjects have wide ranging, but not unlimited rights to request information, correction and deletion of data. Data breaches have to be reported and may carry a fine. The right of deletion means that backups have to be set up in such a way that data can be deleted. CLOUD Act: US authorities can access data stored by US companies in Europe. DMCA: Copyright infringements can be solved by sending a DMCA takedown notice to the provider. Providers have to restore the content if the uploader sends a counter-notification. CDA Section 230: Shields providers from liability if their systems host illegal content which they don't know about. Privacy Shield and SCC: Privacy Shield was recently abolished and instead cloud providers must now add Standard Contract Clauses to their Terms of Service. The SCC favored by most cloud providers puts the legal liability for any data braches due to US law enforcement requests on the cloud customer (you). General Data Protection Regulation (G.D.P.R., EU) # The GDPR (or DSGVO in German-speaking countries) is the general overhaul of privacy protections in the EU. The GDPR replaces much of the previously country-specific privacy protections present in the EU. Jurisdiction # The GDPR applies to all companies that deal with the data of EU citizens around the globe. This is made possible by trade agreements previously already in place. All companies that handle the data of EU citizens even if the companies are not located in the EU. Structure # Data subject: the person whose data is being handled. Personal identifiable data: (PI) data that makes it possible to uniquely identify a data subject. PI can also be created when previously non-PI data is combined to build a profile. Data controller: The company that has a relationship with the data subject and is given the data for a certain task. Data processor: A company that processes data on behalf of the data controller. The data processor must have a data processing agreement (DPA) with the data controller. IaaS and PaaS cloud providers are data processors. Purposes of data processing # One new limitation the GDPR brings to privacy regulation is the fact that there are fundamental limits to data processing. You, the data controller, cannot simply collect data for one purpose and then use that same data for another purpose. The purposes to which data is collected have to be clearly defined and in several cases the data subject has to provide their explicit consent. (In other words it is not enough to add a text to the signup form that says that they agree to everything all at once.) Legal grounds to data processing are detailed in Article 6 of the GDPR . These are: If the data subject has given their explicit consent to process their data for a specific purpose. If the processing is necessary to perform a contract with the data subject. In other words you do not need additional consent if you are processing data in order to fulfill a service to the data subject. To perform a legal obligation . For example, you have to keep an archive of your invoices to show in case of a tax audit. To protect the vial interests of the data subject or another natural person . To perform a task which is in the public interest , or if an official authority has been vested in the data controller. For the purposes of a legitimate interest of the data controller (with exceptions). This might seem like a catch-all but courts have defined legitimate interests very narrowly. The data controller must show that the legitimate interest exists and cannot be achieved in any other way than with the data processing in question. One good example for the legitimate interests clause would be a webshop that is collecting data about their visitors in order to spot fraudulent orders. Data subjects' rights # Chapter 3 of the GDPR deals with the rights of the data subject. These are: The right to be informed. This right lets the data subject request several pieces of information: The purposes of processing. The categories of personal data concerned. Who received their data, in particular recipients in third countries or international organisations. How long the data will be stored. The right to request modification, deletion or restricting the processing of the personal data in question. The right to log a complaint. The source of the personal data if not provided by the data subject themselves. If and how automated decision-making, profiling is taking place. The right to fix incorrect data. The right of deletion. \u201cThe right to be forgotten.\u201d This right puts several architectural limits on cloud systems as, for example, backups must be built in such a way that deletion requests are repeated after a restore. Note that this right is not without limits, legal obligations, for example, override it. The right to restrict processing. If a deletion cannot be requested the data subject can request that their data should only be processed to the purposes that are strictly required. The right to data portability. The data subject has to be provided a copy of their data in a machine-readable form. The right to object automated individual decision-making and profiling. Data breaches # Chapter IV Section 2 is probably the first legislation around the world that explicitly requires security of data processing from companies that handle personal data. Article 33 of this chapter specifically requires that data breaches must be disclosed to the supervisory authorities. Supervisory authorities in turn have the right to impose fines up to 4% or 20.000.000 \u20ac of the global revenue of a company. This means that companies can no longer sweep data breaches under the rug and must spend resources to secure their data processing. In the scope of the cloud this means that our cloud environment has to be set up in a secure way to avoid data breaches. Clarifying Lawful Overseas Use of Data Act (C.L.O.U.D., 2018, USA) # The CLOUD Act, or House Rule 4943 is the latest addition to the US legislation pertaining to cloud providers. This act says that a cloud provider must hand over data to US authorities if requested even if that data is stored in a different country. This act has been widely criticized and is, according to several legal scholars, in contradiction of the GDPR. The large US cloud providers have opened up subsidiaries in the EU in order to try and shield their european customers from this act and a few purely EU cloud providers have also capitalized on this. It remains to be seen how effective this move is. Digital Millennium Copyright Act (D.M.C.A., 1998, USA) # The Digital Millenium Copyright Act clarifies how copyright works in the USA. Since the USA is part of the WIPO copyright treaty and a significant amount of providers are based in the USA nowadays all countries align themselves with the DMCA when it comes to dealing with online copyright infringement. Title II of the DMCA creates a safe harbor for online service providers against copyright infringement committed by their users. However, they have to remove infringing content as soon as they are properly notified of it via a DMCA takedown notice. The author of a DMCA takedown notice must swear by the penalty of perjury that they are or are acting on behalf of the copyright owner. (If they were to make this pledge in bad faith they could end up in prison.) If the DMCA takedown notice has been sent erroneously the original uploader is free to send a counter-notification, also swearing under the penalty of perjury. In this case the provider notifies the original sender and restores the content. The original sender then has to take the uploader to court to pursue the matter further. Communications Decency Act, Section 230 (C.D.A., 1996, USA) # This section of of the CDA is very short: No provider or user of an interactive computer service shall be treated as the publisher or speaker of any information provided by another information content provider. To someone not well versed in US legal lingo this may be sound like gibberish, but it is, in fact, one of the most important pieces of legislation pertaining to operating platforms in the cloud. In US legal history the speaker or publisher of certain information is responsible for the information being provided. Before the CDA any provider hosting illegal content would be responsible for said content. The CDA changed that by shielding providers from liability. This included web hosting providers as well as newspapers that had a user content section. The CDA is not limitless, providers still have to remove infringing content if notified. They may also employ moderation proactively. The fall of Privacy Shield and Standard Contract Clauses (2020, EU-US) # The Privacy Shield agreement (2016) was the successor to the failed International Safe Harbor Privacy Principles framework and was intend to protect the privacy of EU citizens when using US services or their data is transferred to the USA. The Privacy Shield has been enhanced in 2017 by the EU\u2013US Umbrella Agreement which fixed many of the issues with the Privacy Shield. Unfortunately for cloud users, Privacy Shield has been invalidated by the CJEU . This decision means that you, as a European cloud user, can no longer automatically use a US-governed cloud provider without accepting liability. Instead, US data processors have to use one of three standard contract clauses approved by the EU. The first SCC is Decision 2001/497/EC which states that both parties (you as the cloud user, and the cloud provider) are both jointly and severely liable for any data breaches. The second SCC is Decision 2004/915/EC is more business friendly and merely makes sure that both the European party and the non-European party uphold their obligation to handle data with due care and implement protection measures. Shared liability would only apply if the terms of the SCC were breached. While this is a step forward, it is worth noting that this puts the responsibility on the cloud customer if the cloud provider breaches the terms by, for example, handing over data to US authorities covertly. The third SCC is Decision 2010/87/EU and was adopted by Google and AWS , for example. It creates a safety net for the cloud provider by only requiring them to disclose requests by law enforcement if that would not put criminal liability on them. It also puts any liability for damages on the cloud user unless they, for example, went bankrupt. In that case the cloud provider is only responsible for their own operations, not those of the cloud user. In order words, you the cloud user are responsible for any breaches of data due to US law enforcement requests . Who is subject to the GDPR? Any company located in the EU. Any company dealing with privately identifiable information of EU citizens world-wide. Any company world-wide. Does an EU citizen have an absolute right to have their data deleted? Yes No As a EU-based company can you use US cloud providers to store privately identifiable information? Yes, under the Privacy Shield. Yes, under the Standard Contract Clauses. Yes, but only if the data is stored in an EU datacenter. No. You are operating a mailing list and someone contacts you to that they want their PII removed. What are your options? It is enough if I remove the mails sent by that individual. I have to redact or delete all information regarding that persons identity. I do not have to remove anything as archives fall under an exception. You are operating a webshop and a customer contacts you to remove all information about them. What do you have to remove? I have to them from my marketing mailing list. I have to their user account. I have to delete their invoices.","title":"1. Introduction to the Cloud"},{"location":"lectures/1-cloud-intro/#what_is_a_server","text":"In a hurry? Servers: Nowadays either x86 or ARM architecture. Redundant, hot-swap hardware (power supply, fans, etc). Flat build profile for rack mounts. Out-Of-Bounds management interface for monitoring and remote management. Power consumption is a concern. Buying servers can take a long time and present an up-front investment. In older times, servers were completely different from the machines we used for regular work. Not just in weight and form factor, but in architecture. The landscape would stretch from SPARC and Power architectures to the x86 architecture we use in our PCs. Over time, however, the x86 architecture took over to the point where a server today, from an architectural standpoint, is exactly the same as the machine you are using right now. This means that you can copy the machine you are using onto a server and chances are it will run without any modification. The only notable exception is the rise of the ARM architecture which is popular in phones and tables and is known for its low power consumption. In recent years there has been a small but noticeable trend to run ARM in a datacenter. At home it may not matter if your computer uses 200 or 300 watts of power. In a datacenter, at the scale of hundreds, thousands or tens of thousands of servers, saving even a few percent of power will translate to huge cost savings. The difference in build is, however, quite apparent. While some servers, mainly built for office use, have the standard \u201ctower\u201d build, most servers have a flat profile designed to be mounted in racks as displayed on the picture. Since most servers are not high enough to take full size expansion cards (graphics cards, network cards, etc), servers may contain a separate removable component for these usually called a riser. An HP Proliant G5 server pulled out of its rack. Source: Wikipedia Server racks are standardized closets that have mounting screws for rails that allow pulling servers out even mid operation and replace components while the server is running. Servers also come with a high level of redundancy. While you may have a single power supply in your home computer, servers typically have two that are able to take power from different power inputs. This makes sure that the server keeps running even if one of the two power supplies fail, or if one of the two power inputs goes down. Also in contrast to your home setup these servers contain an Out-Of-Bounds Management interface that allows remote management of servers even when they are turned off. The hardware components built into the server report their health status to this OOB management interface which allows for the simultaneous monitoring of thousands of machines. Note This is a fairly generic description of servers. Different vendors may choose to leave out certain features of their more \u201cbudget\u201d line of servers or call certain components differently. HP, for example, calls their OOBM \u201cIntegrated Lights Out\u201d, Dell \u201cDRAC - Dell Remote Access Control\u201d, etc. When it comes to purchasing servers, larger companies tend to go with the same components for a longer period of time and they also buy support from the vendor. This sometimes includes hardware replacement done entirely by the vendor in the datacenter without the need for the customer to have staff on site. However, purchasing a specific set of components or ordering larger quantities of servers presents a logistics challenge and can sometimes take up to 3-4 months. Buying hardware is also an up-front investment which is hard to justify when demands change rapidly. What components are redundant in a server? Power supply CPU RAM Fan OOBM What is the purpose of an OOBM? Remotely manage a server Receive hardware malfunction alerts","title":"What is a Server?"},{"location":"lectures/1-cloud-intro/#the_anatomy_of_a_datacenter","text":"In a hurry? Datacenter components: Racks to house servers. Larger customers have cages for their racks. Cabling under the floor. Redundant cooling, fire suppression systems and power supply. Some datacenters provide internet connectivity. Eco friendliness is becoming a factor. Since the cloud is just somebody else's computer, that computer needs to be hosted somewhere. Servers are almost exclusively hosted in datacenters. Let's take a look at what is involved in running a datacenter. First of all, as mentioned above, most servers are going to be rack-mounted, so you need a bunch of racks. These racks are installed in rows, often with a fake floor to allow for cabling to go under the floor. A datacenter with racks. Source: Wikipedia Since servers produce a lot of heat, a datacenter also requires cooling. There are a variety of ways to solve cooling, some are more \u201cgreen\u201d than others. Some datacenters, for example, opt to install a \u201ccold aisles\u201d where the cold air is pumped between two rack rows and is pushed through the racks to cool the servers. Apart from cooling, datacenters also require automated fire suppression systems simply because of the amount of electricity going through. Datacenters usually go with a non-destructive fire suppression system such as lowering the oxygen content of the air enough to stop the fire. All critical systems in a datacenter (power, cooling, fire suppression systems) are usually built in a redundant fashion because the loss of either of those systems will potentially mean a complete shutdown for the datacenter. Datacenter operators usually have further contingency plans in place, too, such as a UPS (battery) system, diesel generator, fuel truck on standby, hotline to the fire department, etc. to make sure the datacenter can keep its required uptime. On the networking side of things, matters get slightly more complicated. Some datacenter providers also offer you the ability to use their network uplink (also redundant), but larger customers will prefer to host their own networking equipment and negotiate their own internet uplink contracts. Since there is no generic rule for how datacenters handle this, we will dispense with a description. It is also worth noting that larger customers (banks, cloud providers, etc) usually prefer to have their own racks in a separated gated area called a \u201ccage\u201d to which they control access.","title":"The Anatomy of a Datacenter"},{"location":"lectures/1-cloud-intro/#the_anatomy_of_the_internet","text":"In a hurry? Internet: IP ranges are advertised using BGP. Providers connect direcly or using internet exchanges. 16 global providers form the backbone of the internet (tier 1). Once the physical infrastructure is set up there is also the question of how to connect to the Internet. As mentioned before, networks can be very complicated and there is no one size fits all solution. Smaller customers will typically use the network infrastructure provided by the datacenter while larger customers will host their own network equipment. Again, generally speaking, racks will be equipped with a Top-of-Rack switch to provide layer 2 (Ethernet) connectivity between servers. Several ToR may have interconnects between each other and are usually connected to one or more routers. Routers provide layer 3 (IP) routing to other customers in the same datacenter, internet exchange , or may be connected via dedicated fiber to another provider. Note If you are not familiar with computer networks we recommend giving the Geek University CCNA course a quick read . While you will not need everything, you will have to understand how IP addresses, netmasks, etc work in order to pass this course. Providers on the internet exchange data about which network they are hosting using the Border Gateway Protocol . Each provider's router announces the IP address ranges they are hosting to their peer providers, who in turn forward these announcements in an aggregated form to other providers. Providers have agreements with each other, or with an Internet Exchange, about exchanging a certain amount of traffic. These agreements may be paid if the traffic is very asymmetric or one provider is larger than the other. Alternatively providers can come to an arrangement to exchange traffic for free. Internet exchanges facilitate the exchange between many providers for a modest fee allowing cost-effective exchange of data. Depending on the exchange the rules are different. Local exchanges, for example, may only allow advertising local (in-country) addresses, while others are built for a specific purpose. Generally speaking, providers can be classified into 3 categories. Tier 1 providers are the global players that are present on every continent. They form the backbone of the Internet. At the time of writing there are 16 such networks . Tier 2 are the providers who are directly connected to the tier 1 providers, while tier 3 is everyone else. What IP addresses does a CIDR of 1.2.3.4/0 cover? Only the 1.2.3.4 IP address All IP addresses from 1.2.3.0 to 1.2.3.255 All IP addresses from 1.2.0.0 to 1.2.255.255 All IP addresses from 1.0.0.0 to 1.255.255.255 All IP addresses Is 192.168.2.1 in the network range 192.168.1.0/24 Yes No Given there are 3 providers A, B, and C. B is connected to A and C, but A and C are not directly connected. Which of the following is true? A and C directly exchange IP address range information via BGP. A and B directly exchange IP address range information via BGP. B and C directly exchange IP address range information via BGP. A and C cannot communicate. A and C can communicate. A forwards IP address range information from B to C A forwards IP address range information from C to B B forwards IP address range information from A to C B forwards IP address range information from C to A C forwards IP address range information from A to B C forwards IP address range information from B to A","title":"The Anatomy of the Internet"},{"location":"lectures/1-cloud-intro/#software_stack","text":"In a hurry? Software stack: Virtualization. Operating system. Application runtime. Application. The purpose of all this is, of course, to run an application. Each server hosts an operating system which is responsible for managing the hardware. Operating systems provide a simplified API for applications to do hardware-related operations such as dealing with files or talking to the network. This part of the operating system is called the kernel. Other parts form the userland. The userland includes user applications such as a logging software. Specifically on Linux and Unix systems the userland also contains a package manager used to install other software. Modern x86 server CPUs (and some desktop CPUs) also have a number of features that help with virtualization. Virtualization lets the server administrator run multiple guest operating systems efficiently and share the server resources between them. Did you know? You can find out if an Intel CPU supports hardware virtualization by looking for the VT-x feature on the Intel ARK . Unfortunately AMD does not have an easy to use list but you can look for the AMD-V feature on AMD CPUs. Note Virtualization is different from containerization (which we will talk about later) in that with virtualization each guest operating system has its own kernel whereas containers share a kernel between them. There is one more important aspect of finally getting an application to run: the runtime environment. Except for a few rare occasions applications need a runtime environment. If the application is compiled to machine code they still need so-called shared libraries. Shared libraries are common across multiple applications and can be installed and updated independently from the application itself. This makes for a more efficient update process, but also means that the right set of libraries need to be installed for applications. If the applications are written higher level languages like Java, Javascript, PHP, etc. they need the appropriate runtime environment for that language. One notable exception to the runtime environment requirement is the programming language Go . Go compiles everything normally located in libraries into a single binary along with the application. This makes it exceptionally simple to deploy Go applications into containers.","title":"Software Stack"},{"location":"lectures/1-cloud-intro/#the_cloud","text":"In a hurry? Typical cloud features: API Dynamic scaling Can be classified into IaaS, PaaS and SaaS IaaS service offerings typically include: Virtualization. Network infrastructure. Everything required to run the above. PaaS service offerings typically include a managed service ready to be consumed by a developer. SaaS service offerings typically include a managed service ready to be consumed by a non-technical end user. All of the previously discussed things were available before the \u201ccloud\u201d. You could pay a provider to give you access to a virtual machine where you could run your applications. What changed with the cloud, however, is the fact that you no longer had to write a support ticket and everything became self service. The cloud age started with an infamous e-mail from Jeff Bezos to his engineers in 2002 forcing them to use APIs to exchange data between teams. The exact e-mail is no longer available but it went along these lines: 1) All teams will henceforth expose their data and functionality through service interfaces. 2) Teams must communicate with each other through these interfaces. 3) There will be no other form of interprocess communication allowed: no direct linking, no direct reads of another team\u2019s data store, no shared-memory model, no back-doors whatsoever. The only communication allowed is via service interface calls over the network. 4) It doesn\u2019t matter what technology is used. HTTP, Corba, Pubsub, custom protocols \u2014 doesn\u2019t matter. 5) All service interfaces, without exception, must be designed from the ground up to be externalizable. That is to say, the team must plan and design to be able to expose the interface to developers in the outside world. No exceptions. 6) Anyone who doesn\u2019t do this will be fired. This marked the beginning of Amazon Web Services the first and also the most successful public cloud offering. The first public release of AWS was in 2004 with SQS their message queue service, and got completely overhauled in 2006 where the Elastic Compute (EC2) and the Simple Storage Service (S3) service made its first public appearance. The APIs provided by cloud providers allow for a large amount of flexibility. If new servers are needed, they can be launched within a few minutes. If there are too many servers, they can be deleted. The same goes for other services: with the API (and the appropriate billing model) comes flexibility to adapt to change. The other factor that makes it easy to adapt to change is of course the fact that these services are managed. The cloud customer doesn't need to hire a team of engineers to build a database service, for example, it can be consumed without knowing how exactly the database is set up. If you want a consice description of what cloud computing is the US National Institute for Standards and Technologies has described cloud computing in publication 800-145 as follows: Cloud computing is a model for enabling ubiquitous, convenient, on-demand network access to a shared pool of configurable computing resources (e.g., networks, servers, storage, applications, and services) that can be rapidly provisioned and released with minimal management effort or service provider interaction. In more detail, NIST has defined the following characteristics: On-demand self-service. Broad network access. Resource pooling. Rapid elasticity. Measured service. Generally speaking, cloud services can be classified into three categories: Infrastructure as a Service (IaaS) providing virtual machines and network infrastructure, Platform as a Service (PaaS) offering services for developers to use, and Software as a Service (SaaS) offering end-user services. This is also described in the above NIST publication. Note SaaS will not be discussed in this course. Which of the following can be considered cloud providers? An in-house team that provides you with virtual machines when requested via ticket. A company that providers virtual machines as a service that are billed on a per-month basis. A company that offers a databases as a service on a monthly basis. An in-house service where you and other in-house teams can create virtual machines via an API or user interface automatically. A company that provides video conversion on a per-video price upon upload. When determining if a company is a true cloud provider which of the following do you look for? An API and/or user interface Can I create resources (servers, etc) immediately? Are my resources billed based on usage or per-minute/second","title":"The Cloud"},{"location":"lectures/1-cloud-intro/#infrastructure_as_a_service_iaas","text":"The most basic of cloud service offerings is IaaS. IaaS means that the cloud provider will manage the infrastructure used by the customer. Infrastructure in this sense means the ability to manage (provision, start,stop) virtual machines. In very rare cases some providers also offer \u201cbare metal\u201d machines in this fashion. However, most bare metal providers do not offer a true IaaS as machines cannot be ordered using an API, have an up-front fee and are billed on a monthly basis. IaaS also includes the network connectivity to the Internet. Almost all IaaS providers also offer a built-in firewall, virtual private networks (VPC) that can be used to connect virtual machines together without sending the traffic over the Internet, and other network services. Note that network services can differ greatly. For example, some providers implement private networks on a regional basis while other providers offer private networks that can be used to connect virtual machines that are located in different regions. Which of the following do you absolutely need to use an IaaS service? DevOps engineers, Linux or Windows system administrators Developers Neither","title":"Infrastructure as a Service (IaaS)"},{"location":"lectures/1-cloud-intro/#managed_services_paas","text":"In a hurry? Managed services: The provider is typically responsible for provisioning, backups, monitoring and restoring the service if needed. Low entry cost. Little in-house know-how required. Vendor lock-in for non-standard services. If problems arise they can be hard to debug. Apart from offering virtual machines and network infrastructure as a service many cloud providers also offer additional services that are typically used by developers such as managed databases. These services are managed in the sense that the developer does not need to run the operating system and the database software itself. However, the developer has to configure the database such that it works according to the needs of the application. The provider takes over duties like installing the service, making backups, monitoring, and restoring the service in case of an outage. Did you know? There are purely PaaS providers that do not offer IaaS, only a developer-friendly platform to run software on. Your run of the mill average PHP web hosting provider does qualify if you can order the service using an API. One notable example that rose to prominence is Heroku . The pricing of these managed services varies greatly but they usually follow the cloud model. Most cloud providers offer at least a low cost or even free entry version that has a small markup on top of the IaaS costs. The massive benefit of using these managed services is, of course, that you do not need to have in-house knowledge about how to operate them. You don't need an in-house database operator, you \u201cjust\u201d need to know how to set up the database in the cloud. This lets your company focus on the core business they are trying to build and outsource the know-how required to build these services on top of the IaaS layer. As you might imagine managed services also have downsides. Most importantly they present a clear vendor lock-in. This means that if you want to move to a different cloud provider you will have a hard time doing so if you are using one of the more specialized services. In other words the level of standardization across providers matters. It is also worth mentioning that managed services tend to work well for a large number of customers but a few number of customers can run into hard to debug problems. This debugging difficulty arises out of the inherent opacity of the services: you, the customer, don't see what's happening on the IaaS layer. If a database, for example, fills the available bandwidth may not be notified and are left guessing why your database misbehaves. Which of the following do you absolutely need to use a PaaS service? DevOps engineers, Linux or Windows system administrators Developers Neither What is a vendor lock-in? A contractual clause that forces you to use a certain cloud provider for a specified time. A technological incompatibility or hurdle that makes moving to a different provider hard. A cloud user uses virtual machines, network load balancers, network firewalls. Which is this? IaaS PaaS SaaS A cloud user's developers deploy their application purely as functions as a service and managed databases. Which is this? IaaS PaaS SaaS A cloud user uses a cloud offering using only the web interface. Which is this? IaaS PaaS SaaS","title":"Managed Services (PaaS)"},{"location":"lectures/1-cloud-intro/#business_models","text":"In a hurry? Billing models: IaaS is typically priced per-second based on the instance size. PaaS can be priced similar to IaaS but also per-request or data volume. Data volume is typically priced based on usage. Some providers charge for data volume even for internal traffic. The flexibility of cloud providers comes from their usage-based pricing. This can vary depending on the type of service used. For example, virtual machines (IaaS) is typically priced on a per-second basis. When you start a virtual machine for only 5 minutes you will only pay for 5 minutes of runtime. This lets you optimize the usage costs of the cloud if you use automation to start and stop machines or services on demand. Typically the cost savings are realized in these scenarios:","title":"Business Models"},{"location":"lectures/1-cloud-intro/#on-off_usage","text":"This usage type is typical for batch processing. Machines are only started when there is a workload (e.g. video conversion, machine learning training job, etc.)","title":"On-off usage"},{"location":"lectures/1-cloud-intro/#growth_usage","text":"Projects that see a growth curve often opt to use a cloud provider as well since buying hardware in larger quantities typically takes 2-4 months.","title":"Growth usage"},{"location":"lectures/1-cloud-intro/#sudden_spike_usage","text":"Cloud providers can also be useful if a service encounters a sudden load spike. This is typically the case for webshops around Black Friday and other sales events.","title":"Sudden spike usage"},{"location":"lectures/1-cloud-intro/#periodic_spike_usage","text":"Almost every service has usage spikes depending on the time of day. This can be used to scale the service up and down.","title":"Periodic spike usage"},{"location":"lectures/1-cloud-intro/#per-request_billing","text":"IaaS services are typically priced based on an allocated amount of resources determined at the start of the virtual machines. Some PaaS services also use this billing model. (Typically Databases as a Service.) Other PaaS services often opt for a per-request or a data volume-based billing approach.","title":"Per-request billing"},{"location":"lectures/1-cloud-intro/#cost_planning_with_the_cloud","text":"Cost planning is an important part of the job of a cloud architect. Depending on the billing model the cloud provider adopts this can be fairly simple to almost impossible. As a rule of thumb the billing model of larger cloud providers (AWS, Azure, Google Cloud) is more complex than smaller cloud providers (DigitalOcean, Exoscale, Upcloud, etc). It is worth noting that data transfer is typically charged based on volume. Some cloud providers even charge for internal traffic between availability zones.","title":"Cost planning with the cloud"},{"location":"lectures/1-cloud-intro/#cost-comparison_with_on_premises_systems","text":"One of the most important cases for cost analysis will be the comparison with on premises architecture. It is quite common to create a cost comparison where the investment required for a hardware purchase is compared with the cloud cost over 3 or 5 years. These comparisons can be quite misleading because they often don't contain any cost attribution for the required work and they also do not use the advantages of the cloud in terms of scaling. Without these factors a hardware purchase will almost always be cheaper when calculated over 5 years and be roughly equal when compared over 3 years.","title":"Cost-comparison with on premises systems"},{"location":"lectures/1-cloud-intro/#private_vs_public_cloud","text":"In a hurry? Private cloud: Hosted on-premises or in the public cloud using only private connections. Large cloud providers offer their services \u201cin a box\u201d to host yourself. Most cloud applications will reside on a public cloud and be accessible from the Internet. However, for some use cases access over the public Internet is not desirable or even strictly forbidden by laws or regulations. In other cases companies may decide that it is their policy that certain systems must never be connected to the public Internet, or only connect via a self-hosted gateway. In these cases a private cloud is desirable to minimize the risk of cross-contamination or data exposure due to other tenants being on the same infrastructure. Such cross-contamination is not uncommon, for example in recent years there have been a litany of CPU bugs such as Meltdown and Spectre . A special form of a private cloud is a cloud offering that is dedicated to a special set of consumers, such as a research cloud with special hardware, or discounted rates for several research organizations or universities. The NIST classifies this as a community cloud . Public cloud providers have extended their offers to address these concerns. Their service offerings now include dedicated hypervisors to mitigate CPU bugs, the ability to connect the cloud over a non-Internet connection (e.g. MPLS tunnel) , or accessing the cloud API's from a private network . Some cloud providers even went as far offering a self-hosted setup where the public cloud offering can be brought on-premises. These features, especially the connectivity-related features, also allow for the creation of a hybrid cloud where a traditional infrastructure can be connected to the cloud. This gives a company the ability to leverage the flexibility of the cloud but also keep static / legacy systems for financial or engineering reasons. While the definitions outlined by NIST and other organizations in this matter are clear, real life rarely is. In a real-world scenario you will find a wide range of mixes comprised from all of the above. To you as a cloud engineer it is more important to understand the benefits and drawbacks of each so you can make a good recommendation depending on the use case. What are the advantages of a public cloud? Large amount of available resources Public clouds automatically solve scaling problems They are always cheaper than private clouds What are the advantages of a private cloud? Greater protection against hardware-bound attacks Easier interconnect with an on-premises environment They are always hosted on-premises","title":"Private vs. Public Cloud"},{"location":"lectures/1-cloud-intro/#automation","text":"In a hurry? Automation: Documents how a cloud is set up. Gives a reproducible environment. Allows for spinning up multiple copies of the same environment. Not all tools are equal. Some tools allow for manual changes after running them (e.g. Ansible), others don't (e.g. Terraform). As you can see from the previous sections cloud computing doesn't necessarily make it simpler to deploy an application. In this regard setting up a server and not documenting it is the same as setting up a cloud and not documenting it. In the end in both cases modifications will be hard to carry out later since details of the implementation are lost. It will also be hard to create a near-identical copy of the environment for development, testing, etc. purposes. This is partially due to the lack of documentation, and partially because of the manual work involved. In the past this problem was addressed by cloning virtual machines which was an approach with limited success as customizations were hard to make. The rise of API's brought a much welcome change: automation tools such as Puppet, Ansible and Terraform not only automate the installation of a certain software but also document how the setup works. When engineers first go into automation, they tend to reserve a part of the work to be done manually. For example they might use Ansible to create virtual machines and install some basic tools and install the rest of the software stack by hand. This approach is workable where the software stack cannot be automated but should be avoided for software where this is not the case as the lack of automation usually also means a lack of documentation as described above. Automation tools are also not equally suited for each task. Terraform, for example, expects full control of the infrastructure that is created by it. Manual installation steps afterwards are not supported. This approach gives Terraform the advantage that it can also remove the infrastructure it creates. Removing the infrastructure is paramount when there are multiple temporary environments deployed such as a dev or testing environment. Ansible, on the other hand, allows for manual changes but does not automatically implement a tear-down procedure for the environment that has been created. This makes Ansible environments, and Ansible code harder to test and maintain, but more suited for environments where traditional IT is still a concern. When would you use Ansible? I can use Terraform only once I already have my virtual machines installed. I can use Ansible to partially manage a server and manually configure other parts. I can use Ansible to fully manage my cloud environment. When would you use Terraform? I can use it to install servers and then install the software manually. I can use Terraform to implement immutable infrastructure. I can use Terraform only install software within a virtual machine.","title":"Automation"},{"location":"lectures/1-cloud-intro/#regulation","text":"In a hurry? GDPR: Applies to all companies, world-wide, that handle personal data of EU citizens. Companies must keep track of why and how data is handled. Data subjects have wide ranging, but not unlimited rights to request information, correction and deletion of data. Data breaches have to be reported and may carry a fine. The right of deletion means that backups have to be set up in such a way that data can be deleted. CLOUD Act: US authorities can access data stored by US companies in Europe. DMCA: Copyright infringements can be solved by sending a DMCA takedown notice to the provider. Providers have to restore the content if the uploader sends a counter-notification. CDA Section 230: Shields providers from liability if their systems host illegal content which they don't know about. Privacy Shield and SCC: Privacy Shield was recently abolished and instead cloud providers must now add Standard Contract Clauses to their Terms of Service. The SCC favored by most cloud providers puts the legal liability for any data braches due to US law enforcement requests on the cloud customer (you).","title":"Regulation"},{"location":"lectures/1-cloud-intro/#general_data_protection_regulation_gdpr_eu","text":"The GDPR (or DSGVO in German-speaking countries) is the general overhaul of privacy protections in the EU. The GDPR replaces much of the previously country-specific privacy protections present in the EU.","title":"General Data Protection Regulation (G.D.P.R., EU)"},{"location":"lectures/1-cloud-intro/#jurisdiction","text":"The GDPR applies to all companies that deal with the data of EU citizens around the globe. This is made possible by trade agreements previously already in place. All companies that handle the data of EU citizens even if the companies are not located in the EU.","title":"Jurisdiction"},{"location":"lectures/1-cloud-intro/#structure","text":"Data subject: the person whose data is being handled. Personal identifiable data: (PI) data that makes it possible to uniquely identify a data subject. PI can also be created when previously non-PI data is combined to build a profile. Data controller: The company that has a relationship with the data subject and is given the data for a certain task. Data processor: A company that processes data on behalf of the data controller. The data processor must have a data processing agreement (DPA) with the data controller. IaaS and PaaS cloud providers are data processors.","title":"Structure"},{"location":"lectures/1-cloud-intro/#purposes_of_data_processing","text":"One new limitation the GDPR brings to privacy regulation is the fact that there are fundamental limits to data processing. You, the data controller, cannot simply collect data for one purpose and then use that same data for another purpose. The purposes to which data is collected have to be clearly defined and in several cases the data subject has to provide their explicit consent. (In other words it is not enough to add a text to the signup form that says that they agree to everything all at once.) Legal grounds to data processing are detailed in Article 6 of the GDPR . These are: If the data subject has given their explicit consent to process their data for a specific purpose. If the processing is necessary to perform a contract with the data subject. In other words you do not need additional consent if you are processing data in order to fulfill a service to the data subject. To perform a legal obligation . For example, you have to keep an archive of your invoices to show in case of a tax audit. To protect the vial interests of the data subject or another natural person . To perform a task which is in the public interest , or if an official authority has been vested in the data controller. For the purposes of a legitimate interest of the data controller (with exceptions). This might seem like a catch-all but courts have defined legitimate interests very narrowly. The data controller must show that the legitimate interest exists and cannot be achieved in any other way than with the data processing in question. One good example for the legitimate interests clause would be a webshop that is collecting data about their visitors in order to spot fraudulent orders.","title":"Purposes of data processing"},{"location":"lectures/1-cloud-intro/#data_subjects_rights","text":"Chapter 3 of the GDPR deals with the rights of the data subject. These are: The right to be informed. This right lets the data subject request several pieces of information: The purposes of processing. The categories of personal data concerned. Who received their data, in particular recipients in third countries or international organisations. How long the data will be stored. The right to request modification, deletion or restricting the processing of the personal data in question. The right to log a complaint. The source of the personal data if not provided by the data subject themselves. If and how automated decision-making, profiling is taking place. The right to fix incorrect data. The right of deletion. \u201cThe right to be forgotten.\u201d This right puts several architectural limits on cloud systems as, for example, backups must be built in such a way that deletion requests are repeated after a restore. Note that this right is not without limits, legal obligations, for example, override it. The right to restrict processing. If a deletion cannot be requested the data subject can request that their data should only be processed to the purposes that are strictly required. The right to data portability. The data subject has to be provided a copy of their data in a machine-readable form. The right to object automated individual decision-making and profiling.","title":"Data subjects' rights"},{"location":"lectures/1-cloud-intro/#data_breaches","text":"Chapter IV Section 2 is probably the first legislation around the world that explicitly requires security of data processing from companies that handle personal data. Article 33 of this chapter specifically requires that data breaches must be disclosed to the supervisory authorities. Supervisory authorities in turn have the right to impose fines up to 4% or 20.000.000 \u20ac of the global revenue of a company. This means that companies can no longer sweep data breaches under the rug and must spend resources to secure their data processing. In the scope of the cloud this means that our cloud environment has to be set up in a secure way to avoid data breaches.","title":"Data breaches"},{"location":"lectures/1-cloud-intro/#clarifying_lawful_overseas_use_of_data_act_cloud_2018_usa","text":"The CLOUD Act, or House Rule 4943 is the latest addition to the US legislation pertaining to cloud providers. This act says that a cloud provider must hand over data to US authorities if requested even if that data is stored in a different country. This act has been widely criticized and is, according to several legal scholars, in contradiction of the GDPR. The large US cloud providers have opened up subsidiaries in the EU in order to try and shield their european customers from this act and a few purely EU cloud providers have also capitalized on this. It remains to be seen how effective this move is.","title":"Clarifying Lawful Overseas Use of Data Act (C.L.O.U.D., 2018, USA)"},{"location":"lectures/1-cloud-intro/#digital_millennium_copyright_act_dmca_1998_usa","text":"The Digital Millenium Copyright Act clarifies how copyright works in the USA. Since the USA is part of the WIPO copyright treaty and a significant amount of providers are based in the USA nowadays all countries align themselves with the DMCA when it comes to dealing with online copyright infringement. Title II of the DMCA creates a safe harbor for online service providers against copyright infringement committed by their users. However, they have to remove infringing content as soon as they are properly notified of it via a DMCA takedown notice. The author of a DMCA takedown notice must swear by the penalty of perjury that they are or are acting on behalf of the copyright owner. (If they were to make this pledge in bad faith they could end up in prison.) If the DMCA takedown notice has been sent erroneously the original uploader is free to send a counter-notification, also swearing under the penalty of perjury. In this case the provider notifies the original sender and restores the content. The original sender then has to take the uploader to court to pursue the matter further.","title":"Digital Millennium Copyright Act (D.M.C.A., 1998, USA)"},{"location":"lectures/1-cloud-intro/#communications_decency_act_section_230_cda_1996_usa","text":"This section of of the CDA is very short: No provider or user of an interactive computer service shall be treated as the publisher or speaker of any information provided by another information content provider. To someone not well versed in US legal lingo this may be sound like gibberish, but it is, in fact, one of the most important pieces of legislation pertaining to operating platforms in the cloud. In US legal history the speaker or publisher of certain information is responsible for the information being provided. Before the CDA any provider hosting illegal content would be responsible for said content. The CDA changed that by shielding providers from liability. This included web hosting providers as well as newspapers that had a user content section. The CDA is not limitless, providers still have to remove infringing content if notified. They may also employ moderation proactively.","title":"Communications Decency Act, Section 230 (C.D.A., 1996, USA)"},{"location":"lectures/1-cloud-intro/#the_fall_of_privacy_shield_and_standard_contract_clauses_2020_eu-us","text":"The Privacy Shield agreement (2016) was the successor to the failed International Safe Harbor Privacy Principles framework and was intend to protect the privacy of EU citizens when using US services or their data is transferred to the USA. The Privacy Shield has been enhanced in 2017 by the EU\u2013US Umbrella Agreement which fixed many of the issues with the Privacy Shield. Unfortunately for cloud users, Privacy Shield has been invalidated by the CJEU . This decision means that you, as a European cloud user, can no longer automatically use a US-governed cloud provider without accepting liability. Instead, US data processors have to use one of three standard contract clauses approved by the EU. The first SCC is Decision 2001/497/EC which states that both parties (you as the cloud user, and the cloud provider) are both jointly and severely liable for any data breaches. The second SCC is Decision 2004/915/EC is more business friendly and merely makes sure that both the European party and the non-European party uphold their obligation to handle data with due care and implement protection measures. Shared liability would only apply if the terms of the SCC were breached. While this is a step forward, it is worth noting that this puts the responsibility on the cloud customer if the cloud provider breaches the terms by, for example, handing over data to US authorities covertly. The third SCC is Decision 2010/87/EU and was adopted by Google and AWS , for example. It creates a safety net for the cloud provider by only requiring them to disclose requests by law enforcement if that would not put criminal liability on them. It also puts any liability for damages on the cloud user unless they, for example, went bankrupt. In that case the cloud provider is only responsible for their own operations, not those of the cloud user. In order words, you the cloud user are responsible for any breaches of data due to US law enforcement requests . Who is subject to the GDPR? Any company located in the EU. Any company dealing with privately identifiable information of EU citizens world-wide. Any company world-wide. Does an EU citizen have an absolute right to have their data deleted? Yes No As a EU-based company can you use US cloud providers to store privately identifiable information? Yes, under the Privacy Shield. Yes, under the Standard Contract Clauses. Yes, but only if the data is stored in an EU datacenter. No. You are operating a mailing list and someone contacts you to that they want their PII removed. What are your options? It is enough if I remove the mails sent by that individual. I have to redact or delete all information regarding that persons identity. I do not have to remove anything as archives fall under an exception. You are operating a webshop and a customer contacts you to remove all information about them. What do you have to remove? I have to them from my marketing mailing list. I have to their user account. I have to delete their invoices.","title":"The fall of Privacy Shield and Standard Contract Clauses (2020, EU-US)"},{"location":"lectures/2-iaas/","text":"Download PPTX \ud83d\udcbb Download MP3 \ud83c\udfa7 Watch Video \ud83c\udfac Infrastructure as a Service Infrastructure as a Service, or IaaS is a service offering by most cloud providers that provides virtual machines and the accompanying infrastructure as a service. This lecture will discuss the details of how an IaaS service is built. Virtual machines # In a hurry? Modern CPUs have several operation modes: Ring 3 (unprivileged) runs the application Ring 0 runs the operating system kernel Ring -1 runs the hypervisor managing serveral kernels Ring -2 runs the Intel Management Engine Other components are responsible for virtualizing other hardware components. For example, the IOMMU is responsible for virtualizing direct memory access between applications and hardware components. Virtualization is a surprisingly old technology. The first virtualized system was the IBM System/370 mainframe with the VM/370 operating system in 1972. The system was different from how we understand virtualization today, but the goal was the same: separate workloads from each other. When you think about mainframes you have to consider that these machines were very expensive and machine time was a scarce resource. Most programs back in those days were batch jobs . They processed a large set of data at once and then terminated. Initially CPUs in personal computers did not have application separation. The x86 line of Intel CPUs only received the protected mode feature with the 80286 in 1982. The operating system (MS/DOS) would run in in real mode and applications could then switch into the new mode to isolate applications from each other. One such application making use of the new mode was Windows that ran on top of MS/DOS. Protected mode introduced the concept of rings in the CPU. The operating system kernel would run in ring 0, device drivers would run in ring 1 and 2 while applications would run in ring 3. The lower ring number meant the higher privilege level. Note Device drivers today typically run on ring 0 instead of 1 or 2. This ring system allowed the operating system to restrict the higher ring numbers from accessing certain functions or memory locations. However, most applications in the day would violate the restrictions of protected mode and could not run in the new mode. Note If you try and set up a really old computer game like Commander Keen in DOSBox you will realize that you have to provide the game itself with very hardware-specific settings. You will, for example, have to provide details for your sound card. This is because the game itself incorporated sound card drivers for Sound Blaster 16 or Gravis Ultrasound cards. A game that would do this could not run in protected mode. To work around the problems with protected mode the 80386 successor introduced virtual mode . The new virtual 8086 mode (VM86) introduced a number of compatibility fixes to enable running old real mode programs in a multitasking environment such as Windows without problems. For instance the CPU would create a simulated virtual memory space the program could write to and translate the virtual addresses to physical addresses internally. It would also capture sensitive instructions and turn them over for control to the kernel. Note VM86 does not capture every instruction the application runs in virtual mode, only the sensitive CPU instructions. This enables legacy applications to run at a reasonable speed. In the mid 2000's CPUs became so powerful that it made sense to not only virtualize applications but whole operating systems including their kernel. This allowed multiple operating systems to run in parallel. However, without CPU support only software virtualization could be achieved. In other words early virtualization software had to simulate a CPU in ring 0 to the guest operating system. Some virtualization techniques, such as Xen required the guest operating system to run a modified kernel to facilitate them running in ring 3. Others employed a number of techniques we won't go into here. Hardware vendors, of course, followed suit. In 2005 Intel added the VT-x (Vanderpool) feature to its new Pentium 4 CPUs followed by AMDs SVM/AMD-V technology in 2006 in the Athlon 64, Athlon 64 X2, and Athlon 64 FX processors. VT-x and AMD-V added new ring -1 to accommodate hypervisors . This new ring allowed for separation between several operating systems running at ring 0. Later CPU releases added features such as Direct Input/Output virtualization , network virtualization or even graphics card virtualization. These features allowed for more efficient virtualization and sharing hardware devices between several virtual machines. Note Intel also introduced a ring -2 for the Intel Management Engine , a chip that functions as an OOBM in modern Intel chips. The ME runs its own operating system, a MINIX variant and has been the target of severe criticism for its secrecy and power over the machine. Several bugs have been found in the ME that let an attacker hide a malware inside the ME . Virtualization also gave rise to Infrastructure as a Service. AWS was the first service that offered virtual machines as a service starting in 2006 with a Xen-based offer. They not only offered virtual machines but they did so that a customer could order or cancel the service using an Application Programming Interface . This allowed customers to create virtual machines as they needed it and they were billed for it on an hourly basis. (Later on AWS and other cloud providers moved to a per-second billing.) The presence of an API makes the difference between IaaS and plain old virtual machines as a service. IaaS allows a customer to scale their application dynamically according to their current demand. What component of the software stack runs on Ring 3 in virtual mode? The application The kernel The hypervisor The management engine What component of the software stack runs on Ring 0 in virtual mode? The application The kernel The hypervisor The management engine What component of the software stack runs on Ring -1 in virtual mode? The application The kernel The hypervisor The management engine What component of the software stack runs on Ring -2 in virtual mode? The application The kernel The hypervisor The management engine What does virtualization mean? Every instruction by a virtual machine is captured by the kernel and translated. Critical instructions like memory operations are captured or translated by the kernel. Critical instructions like memory operations are captured or translated by the CPU and the hypervisor. Typical instance types # When the cloud became popular in the late 2000s several providers attempted to offer a service that was fully dynamic in their sizes. The customer could set how many GB of RAM they needed and how many CPU cores. However, this model has been phased out by most providers since it is difficult to manage such a dynamic environment. Instead most cloud providers nowadays opt to offer fixed machine sizes. To accommodate high-CPU and high RAM workloads there are several different instance types, typically: Shared CPU: These are small instances where a single CPU core is shared between multiple virtual machines, sometimes leading to high steal time . Sometimes this offering includes a burst capability (such as the Amazon T instances) where a VM can temporarily use more CPU. Standard, dedicated core CPU: These instance types receive one or more physical cores leading to a more stable performance without the ability to burst beyond their limits. High CPU: These instance types are usually hosted on physical servers that have a very high CPU to RAM ratio. Accordingly, the virtual machine offering includes more CPU than the standard offering. High RAM: This offering is the exact opposite of the high CPU offering. The machines on offer here include more RAM with very little CPU. Storage: These instance types contain large amounts of local storage (see below in the storage section). Hardware-specific: These instance types offer access to dedicated hardware such as graphics cards (GPUs) or FPGAs. Automation # In a hurry? Cloud-init allows for running a script, or other initial configuration on virtual machines on first boot. It is also responsible for managing password resets when desired. It can be used to fully automate the setup of a virtual machine. Terraform and Ansible are tools that interact with the cloud API to provision virtual machines programmatically. Ansible is also capable of running inside a virtual machine to configure the software within. Terraform requires full control of the machines it is managing and implements what's called immutable infrastructure. As discussed before, what makes an IaaS cloud provider a cloud provider is the fact that they offer an API to automate the provisioning and deprovisioning of virtual machines as needed. However, that's not all. Simply starting a virtual machine is not enough, the software needs to be installed in it. Initially this problem would be solved by creating templates for the operating system that launches. In larger cloud setups these templates included a pre-installed agent for configuration management that would report to a central service and fetch its manifest of software to install. Thankfully in the last decade a lot has happened and Cloud Init has established itself as a defacto standard in the IaaS world. Every cloud provider nowadays offers the ability to submit a user data field when creating a virtual machine. This user data field is read by Cloud Init (or its Windows alternative Cloudbase Init ) and is executed at the first start of the virtual machine. A DevOps engineer can simply inject a script that runs at the first start that takes care of all the installation steps required. Tools like Terraform or Ansible assist with managing the whole process of provisioning the virtual machines and supplying it with the correct user data script. What is the role of cloud-init? It initializes a cloud account. It creates a virtual machine. It runs initial machine configuration on a virtual machine. Virtual machine pools # In a hurry? Virtual machine pools automatically create and destroy machines to keep up a desired pool size. Some implementations also have autoscaling. One other use of user data are virtual machine pools. Each cloud provider adopts a different name for them, ranging from instance pools to autoscaling groups. The concept is the same everywhere: you supply the cloud with a configuration how you would like your virtual machines to look like and the cloud will take care that the given number of machines are always running. If a machine crashes or fails a health check the cloud deletes the machine and creates a new one. The number of machines in a pool can, of course, be changed either manually or in some cases automatically using rules for automatic scaling. Combined with the aforementioned user data this can be a very powerful tool to create a dynamically sized pool of machines and is the prime choice for creating a scalable architecture. These pools are often integrated with the various load-balancer offerings cloud providers have in their portfolio to direct traffic to the dynamic number of instances. Some cloud providers integrate them with their Functions as a Service offering as well allowing you to run a custom function whenever a machine starts or stops. This can be used to, for example, update your own service discovery database. Storage # In a hurry? Local disks offer affordable performance at the cost of redundancy. Network block storage offers resilience to machine failures, but costs more to ensure the same performance. Not all NBS implementations store data in a redundant fashion. Network file systems offer access from multiple virtual machines in parallel at the cost of performance. Object storage offers parallel access from multiple VMs and scalability at the cost of performance and consistency. Object storages are typically integrated on the application level rather than the OS level. When it comes to data storage virtual machines work exactly like your physical machine would: there is a physical disk (or multiple) that store the files. The difference is that in the cloud your virtual machine may make use of a distributed storage architecture instead of using a local disk. In a distributed storage system the data isn't stored on the machine that runs the virtual machine so a hardware failure on that machine will not cause a data loss. However, a distributed storage system is generally either slower or more expensive for the same performance by several magnitudes so using a local storage may still be needed for some use cases. When we talk about storage systems we are talking about two types: block devices and filesystems. On the physical disk data is stored in its raw form so the disk itself has no information about which data belongs to which file. Filesystems organize data into blocks of a fixed or dynamic size and then create a database (mapping table) of which file entry consists of which blocks of data. Keep in mind that the blocks of a single file may be distributed all over the whole disk randomly so that's something the filesystem must keep track of. Therefore we traditionally call raw disk devices block devices . Block devices are (with very few exceptions) only accessible from a single virtual or physical machine since otherwise the machines would have to synchronize their file system operations on that device. The only notable exception is GFS2 . While you can use GFS2 over a shared storage infrastructure if you have control over it cloud providers enforce a single-VM access policy. In other words, one block storage device can only ever be used by a single VM. Local Storage # As described above the simplest and most widely supported option to store data from your virtual machine is a disk that is locally attached to the physical machine running the VM. This option offers you the highest performance at a relatively low price point. The reason for that is that it is the simplest and cheapest to build. Some cloud providers offer disk redundancy ( RAID ) while others don't. At any rate a hardware failure of the physical machine means that your data may become unavailable for a period of time or may be completely lost. It is therefore very advisable to solve redundancy on top of the virtual machine, e.g. by building a replicated database setup. If, however, your database is replicated anyway you may no longer need the more expensive storage options and this can be a great way to save costs. Which of the following is provided by local storage? Fault-tolerance in the face of a machine failure. High IO performance. The ability to move the data volume to a different machine. The ability to access the data volume from several machines at once. Data consistency. Network Block Storage # Network block storage means a block storage that is delivered over the network. The network here can mean a traditional IP network or a dedicated Fibre Channel infrastructure. As described before block storage is, in general, single-VM only. You can't access the files stored on a block storage device from multiple virtual machines. Also note that Network Block Storage does not automatically come with redundancy. Some solutions, such as iSCSI simply offer the disk of one machine to another. More advanced ones like Ceph RBD or the cloud provider offerings such as EBS by Amazon , however, do offer redundancy. At any rate, using Network Block Storage does not absolve you from the duty to make backups and have a documented and tested disaster recovery strategy. Which of the following is provided by network block storage? Fault-tolerance in the face of a machine failure. High IO performance. The ability to move the data volume to a different machine. The ability to access the data volume from several machines at once. Data consistency. Network File Systems # In contrast to network block storage network file systems offer access to data not on a block level, but on a file level. Over the various network file system protocols machines using these file systems can open, read and write files, and even place locks on them. The filesystem has to keep track of which machine has which file open, or has locks on which file. When multiple machines edit the same file in parallel the filesystem has to ensure that these writes are consistent. This means that network file systems are either much slower than block-level access (e.g. NFS ) or require a great deal more CPU and RAM to keep track of the changes across the network (e.g. CephFS ). Some cloud providers also offer this, for example Amazon's EFS . Which of the following is provided by network filesystems? Fault-tolerance in the face of a machine failure. High IO performance. The ability to move the data volume to a different machine. The ability to access the data volume from several machines at once. Data consistency. Object storage # Object storage systems are similar to network file systems in that they deal with files rather than blocks. However, they do not have the same synchronization capabilities as network file systems. Files can generally only be read or written as a whole and they also don't have the ability to lock a file. While object storages technically can be used as a filesystem on an operating system level for example by using s3fs this is almost always a bad idea due to the exceptionally bad performance and stability issues. Operating system level integration should only be used as a last resort and object storages should be integrated on the application level. We will discuss object storage services in detail in our next lesson. Which of the following is provided by object storages? Fault-tolerance in the face of a machine failure. High IO performance. The ability to move the data volume to a different machine. The ability to access the data volume from several machines at once. Data consistency. Which storage type is Amazon's EBS? Local disk Network block storage Network filesystem Object storage Which storage type is Amazon's EFS? Local disk Network block storage Network filesystem Object storage Which storage type is Ceph RBD? Local disk Network block storage Network filesystem Object storage Which storage type is iSCSI? Local disk Network block storage Network filesystem Object storage Which storage type is S3? Local disk Network block storage Network filesystem Object storage Network # The next big topic concerning IaaS services is networks. Before we go into the cloud-aspect let's look at how the underlying infrastructure is built. As indicated in the first lecture it is strongly recommended that you familiarize yourself with the basics of computer networking, such as the Ethernet, IP and TCP protocols as you will need them to understand this section. So, let's get started. Imagine a data center from the first lecture. Your task is to build an IaaS cloud provider. You put your servers that will serve as your hosts for virtual machines in the racks. These servers will be connected to the Top-of-Rack switches (yes, two for redundancy) using 10 GBit/s network cables. The switches are themselves connected among each other and across racks with several 100 GBit/s. This sounds like a lot of bandwidth available but keep in mind that your virtual machines get assigned to the physical machines as capacity allows. There is no cloud provider that can guarantee the bandwidth or latency between two virtual machines. Generally cloud providers only state the theoretical bandwidth of the connection a virtual machine has to the switching fabric, but not the specific bandwidth between two distinct virtual machines. This is part of the reason why in the cloud scaling horizontally (adding more machines) is preferred rather than creating huge instances with lots of resources. Network architectures offered by cloud providers # When we look at the network offerings by cloud providers there are three types: Private-only network with NAT : This option is provided by the larger cloud providers such as AWS , Azure , GCP and IBM . This setup gives each virtual machine a private IP address on a private network only. When a public IP address is needed that public IP is handled by the gateway provided by the cloud provider and the incoming traffic is forwarded to the virtual machine on the private network using Destination NAT . Multiple private networks (VPC's) can be assigned to a virtual machine and they can work independently. Default public IP : This option is provided by smaller IaaS providers such as DigitalOcean , Exoscale , Hetzner , Linode , Upcloud , and Vultr . In this setup each virtual machine is attached to a public network and is directly assigned one public IP address. Optionally private networks can be attached as well but the first public IP generally cannot be removed as it is required for user data to work. Fully customizable: This setup allows the customer to design their network connectivity as they see fit. This setup is suitable for enterprise customers who want to move their on-premises setup into the cloud without changing their architecture (lift-and-shift). This option is offered by 1&1 IONOS . Note There are several other cloud providers which we have no information on, such as the Deutsche Telekom/Open Telekom Cloud , or the Alibaba Cloud . You can classify any cloud provider you come across into these categories. Note Out of group 2 it is worth mentioning that the services that are available on the public network (firewalls, load balancers) are often not available on private networks. Firewalling # IaaS providers often also offer network firewalls as a service, included in the platform. Firewalls generally have two rule types: INGRESS (from the Internet or other machines to the current VM) and EGRESS (from the current VM to everywhere else). Firewall providers often employ the concept of security groups . The implementation varies greatly, but in general security groups are a reusable set of rules that can be applied to a VM. For most cloud providers you will need to create an explicit rule allowing traffic to flow between two machines in the same security group. The advantage of security groups is that the rules can be made in such a way that they reference other security groups rather than specific IP addresses. For example, the database security group could be set to allow connections only from the appserver security group but not from anywhere else. This can help with the dynamic nature of the cloud since you do not need to hard-code the IP addresses of the application servers. What do security groups offer? Filtering based on IP address Filtering based on the requested service Filtering based on the requested domain name Filtering based on the requested subpage on a website Network load balancers # Network load balancers are an option some cloud providers offer. In contrast to Application Load Balancers they do not offer protocol decoding (such as routing requests to backends based on the requested web address), they only balance incoming connections to a pool of backends. Depending on the cloud provider in question network load balancers may or may not offer terminating encrypted connections (SSL/TLS), and may be bound to virtual machine pools. It is also cloud provider specific if load balancers are offered in private networks or not. When designing an architecture it is worth considering if the real IP address of the connecting client will be needed. If the backend needs to know the real IP address of the client and the network load balancer handles SSL/TLS termination that combination may not be suitable for the task unless a specific trick such as the proxy protocol from Digital Ocean . Network load balancers without SSL/TLS termination should, in general, make the client IP available to the backends. In order to make sure requests are not sent to faulty backends NLBs include a health check feature. This health check either simply opens a connection to the respective backends (TCP check) or requests a webpage from the backend (HTTP check). If the check fails the backend is removed from the rotation. When integrated with virtual machine pools they may automatically shut down and replace the faulty machine, but this is often not the case. It is on the operator to destroy faulty machines. When talking about load balancers an interesting question is the load balancing strategy. Most load balancers support either round robin (selecting the next backend in the list) or source hashing (routing the same connecting IP to the same backend). What do NLBs typically offer? Spreading incoming connections across multiple backend machines equally. Spreading incoming connections across multiple backend machines, sending connections from the same source to the same backend. Spreading incoming connections across multiple backend machines, based on the domain name. Spreading incoming connections across multiple backend machines, based on the subpage requested. Terminating encrypted connections so the backend doesn't have to (SSL/TLS offloading). VPNs, private interconnects, and routing services # While it seems convenient at first to use only the public network several organizations have security models that prevent accidental public exposure of services not only by implementing the appropriate firewalls (e.g. with security groups) but also by not having private services on the public internet at all. To connect these internal services you must be on a private network. However, this presents a problem when moving data between several, geographically distributed locations. Most companies don't own continent-spanning fiber channel backbones where they could simply create a private network without going on the internet. This means that most companies have to choose one of two methods if they want to create a private connectivity between locations (and the cloud): an MPLS tunnel or VPN . MPLS tunnels create a virtual connectivity that does not go on the Internet. While being expensive and slow to set up, it can offer a guaranteed bandwidth, latency and better security than a VPN. VPN's on the other hand create a virtual connectivity by sending data over the Internet in an encrypted form. Bandwidth or latency cannot be guaranteed, and there are several drawbacks (such as decreased MTU ) but it's a very affordable solution. Larger cloud providers tend to offer both options. MPLS is supported by the larger cloud providers ( AWS Direct connect , Azure Express Route , or Google Cloud Interconnect ) and also some smaller ones (e.g. Exoscale Private Connect ). VPN is also offered mostly by large providers ( AWS VPC VPN , Azure VPN , or Google Cloud VPN ). However, keep in mind that this VPN is a site-to-site VPN built on IPSec and requires a fixed IP address on your side as well. In other words you can't use this VPN to connect from your laptop to the cloud on the go. The only cloud service that offers a mobile device to cloud connectivity at the time of writing is Azure's Point-to-Site VPN . It is also worth noting that VPN's can be used to connect cloud providers together. What VPN type is offered by all major cloud providers? Site-to-site Device-to-site Device-to-device What VPN protocol is offered by all major cloud providers? OpenVPN IPsec SSTP L2TP PPTP What VPN type can IPsec offer by itself? Site-to-site Device-to-site Device-to-device DNS # The Domain Name Service is one of the services that are all but required for building an infrastructure. It provides domain name to IP address resolution, such as pointing your domain example.com to an IP address of your servers. There is a difference, however, between DNS services on offer. Some DNS services by cloud providers offer only simple resolution, other providers offer more advanced features. These features include being able to host the DNS service only on a private network without exposing it to the internet. More advanced features may include automatic DNS failover. This involves running regular health checks on your services and if your primary service fails the DNS service can automatically switch to the secondary IP. There are even services that offer advanced functionality such as routing traffic to different servers based on the geographic location of the client. This can be used for advanced builds such as building a custom CDN . CDNs are discussed in the next lecture . Monitoring # Some cloud providers offer includes basic monitoring, such as CPU or memory usage. Some providers are offering monitoring agents you can install on your virtual machine to get more data in the monitoring interface. With some cloud providers monitoring alerts can be integrated with virtual machine pools to provide automatic scaling, either automatically or using lambdas/FaaS, which we will talk about in the next lecture . Often times the monitoring facilities offered by cloud providers are not sufficient for keeping an application running and more detailed systems are needed. These will be discussed in greater detail in the lecture 5 .","title":"2. Infrastructure as a Service"},{"location":"lectures/2-iaas/#virtual_machines","text":"In a hurry? Modern CPUs have several operation modes: Ring 3 (unprivileged) runs the application Ring 0 runs the operating system kernel Ring -1 runs the hypervisor managing serveral kernels Ring -2 runs the Intel Management Engine Other components are responsible for virtualizing other hardware components. For example, the IOMMU is responsible for virtualizing direct memory access between applications and hardware components. Virtualization is a surprisingly old technology. The first virtualized system was the IBM System/370 mainframe with the VM/370 operating system in 1972. The system was different from how we understand virtualization today, but the goal was the same: separate workloads from each other. When you think about mainframes you have to consider that these machines were very expensive and machine time was a scarce resource. Most programs back in those days were batch jobs . They processed a large set of data at once and then terminated. Initially CPUs in personal computers did not have application separation. The x86 line of Intel CPUs only received the protected mode feature with the 80286 in 1982. The operating system (MS/DOS) would run in in real mode and applications could then switch into the new mode to isolate applications from each other. One such application making use of the new mode was Windows that ran on top of MS/DOS. Protected mode introduced the concept of rings in the CPU. The operating system kernel would run in ring 0, device drivers would run in ring 1 and 2 while applications would run in ring 3. The lower ring number meant the higher privilege level. Note Device drivers today typically run on ring 0 instead of 1 or 2. This ring system allowed the operating system to restrict the higher ring numbers from accessing certain functions or memory locations. However, most applications in the day would violate the restrictions of protected mode and could not run in the new mode. Note If you try and set up a really old computer game like Commander Keen in DOSBox you will realize that you have to provide the game itself with very hardware-specific settings. You will, for example, have to provide details for your sound card. This is because the game itself incorporated sound card drivers for Sound Blaster 16 or Gravis Ultrasound cards. A game that would do this could not run in protected mode. To work around the problems with protected mode the 80386 successor introduced virtual mode . The new virtual 8086 mode (VM86) introduced a number of compatibility fixes to enable running old real mode programs in a multitasking environment such as Windows without problems. For instance the CPU would create a simulated virtual memory space the program could write to and translate the virtual addresses to physical addresses internally. It would also capture sensitive instructions and turn them over for control to the kernel. Note VM86 does not capture every instruction the application runs in virtual mode, only the sensitive CPU instructions. This enables legacy applications to run at a reasonable speed. In the mid 2000's CPUs became so powerful that it made sense to not only virtualize applications but whole operating systems including their kernel. This allowed multiple operating systems to run in parallel. However, without CPU support only software virtualization could be achieved. In other words early virtualization software had to simulate a CPU in ring 0 to the guest operating system. Some virtualization techniques, such as Xen required the guest operating system to run a modified kernel to facilitate them running in ring 3. Others employed a number of techniques we won't go into here. Hardware vendors, of course, followed suit. In 2005 Intel added the VT-x (Vanderpool) feature to its new Pentium 4 CPUs followed by AMDs SVM/AMD-V technology in 2006 in the Athlon 64, Athlon 64 X2, and Athlon 64 FX processors. VT-x and AMD-V added new ring -1 to accommodate hypervisors . This new ring allowed for separation between several operating systems running at ring 0. Later CPU releases added features such as Direct Input/Output virtualization , network virtualization or even graphics card virtualization. These features allowed for more efficient virtualization and sharing hardware devices between several virtual machines. Note Intel also introduced a ring -2 for the Intel Management Engine , a chip that functions as an OOBM in modern Intel chips. The ME runs its own operating system, a MINIX variant and has been the target of severe criticism for its secrecy and power over the machine. Several bugs have been found in the ME that let an attacker hide a malware inside the ME . Virtualization also gave rise to Infrastructure as a Service. AWS was the first service that offered virtual machines as a service starting in 2006 with a Xen-based offer. They not only offered virtual machines but they did so that a customer could order or cancel the service using an Application Programming Interface . This allowed customers to create virtual machines as they needed it and they were billed for it on an hourly basis. (Later on AWS and other cloud providers moved to a per-second billing.) The presence of an API makes the difference between IaaS and plain old virtual machines as a service. IaaS allows a customer to scale their application dynamically according to their current demand. What component of the software stack runs on Ring 3 in virtual mode? The application The kernel The hypervisor The management engine What component of the software stack runs on Ring 0 in virtual mode? The application The kernel The hypervisor The management engine What component of the software stack runs on Ring -1 in virtual mode? The application The kernel The hypervisor The management engine What component of the software stack runs on Ring -2 in virtual mode? The application The kernel The hypervisor The management engine What does virtualization mean? Every instruction by a virtual machine is captured by the kernel and translated. Critical instructions like memory operations are captured or translated by the kernel. Critical instructions like memory operations are captured or translated by the CPU and the hypervisor.","title":"Virtual machines"},{"location":"lectures/2-iaas/#typical_instance_types","text":"When the cloud became popular in the late 2000s several providers attempted to offer a service that was fully dynamic in their sizes. The customer could set how many GB of RAM they needed and how many CPU cores. However, this model has been phased out by most providers since it is difficult to manage such a dynamic environment. Instead most cloud providers nowadays opt to offer fixed machine sizes. To accommodate high-CPU and high RAM workloads there are several different instance types, typically: Shared CPU: These are small instances where a single CPU core is shared between multiple virtual machines, sometimes leading to high steal time . Sometimes this offering includes a burst capability (such as the Amazon T instances) where a VM can temporarily use more CPU. Standard, dedicated core CPU: These instance types receive one or more physical cores leading to a more stable performance without the ability to burst beyond their limits. High CPU: These instance types are usually hosted on physical servers that have a very high CPU to RAM ratio. Accordingly, the virtual machine offering includes more CPU than the standard offering. High RAM: This offering is the exact opposite of the high CPU offering. The machines on offer here include more RAM with very little CPU. Storage: These instance types contain large amounts of local storage (see below in the storage section). Hardware-specific: These instance types offer access to dedicated hardware such as graphics cards (GPUs) or FPGAs.","title":"Typical instance types"},{"location":"lectures/2-iaas/#automation","text":"In a hurry? Cloud-init allows for running a script, or other initial configuration on virtual machines on first boot. It is also responsible for managing password resets when desired. It can be used to fully automate the setup of a virtual machine. Terraform and Ansible are tools that interact with the cloud API to provision virtual machines programmatically. Ansible is also capable of running inside a virtual machine to configure the software within. Terraform requires full control of the machines it is managing and implements what's called immutable infrastructure. As discussed before, what makes an IaaS cloud provider a cloud provider is the fact that they offer an API to automate the provisioning and deprovisioning of virtual machines as needed. However, that's not all. Simply starting a virtual machine is not enough, the software needs to be installed in it. Initially this problem would be solved by creating templates for the operating system that launches. In larger cloud setups these templates included a pre-installed agent for configuration management that would report to a central service and fetch its manifest of software to install. Thankfully in the last decade a lot has happened and Cloud Init has established itself as a defacto standard in the IaaS world. Every cloud provider nowadays offers the ability to submit a user data field when creating a virtual machine. This user data field is read by Cloud Init (or its Windows alternative Cloudbase Init ) and is executed at the first start of the virtual machine. A DevOps engineer can simply inject a script that runs at the first start that takes care of all the installation steps required. Tools like Terraform or Ansible assist with managing the whole process of provisioning the virtual machines and supplying it with the correct user data script. What is the role of cloud-init? It initializes a cloud account. It creates a virtual machine. It runs initial machine configuration on a virtual machine.","title":"Automation"},{"location":"lectures/2-iaas/#virtual_machine_pools","text":"In a hurry? Virtual machine pools automatically create and destroy machines to keep up a desired pool size. Some implementations also have autoscaling. One other use of user data are virtual machine pools. Each cloud provider adopts a different name for them, ranging from instance pools to autoscaling groups. The concept is the same everywhere: you supply the cloud with a configuration how you would like your virtual machines to look like and the cloud will take care that the given number of machines are always running. If a machine crashes or fails a health check the cloud deletes the machine and creates a new one. The number of machines in a pool can, of course, be changed either manually or in some cases automatically using rules for automatic scaling. Combined with the aforementioned user data this can be a very powerful tool to create a dynamically sized pool of machines and is the prime choice for creating a scalable architecture. These pools are often integrated with the various load-balancer offerings cloud providers have in their portfolio to direct traffic to the dynamic number of instances. Some cloud providers integrate them with their Functions as a Service offering as well allowing you to run a custom function whenever a machine starts or stops. This can be used to, for example, update your own service discovery database.","title":"Virtual machine pools"},{"location":"lectures/2-iaas/#storage","text":"In a hurry? Local disks offer affordable performance at the cost of redundancy. Network block storage offers resilience to machine failures, but costs more to ensure the same performance. Not all NBS implementations store data in a redundant fashion. Network file systems offer access from multiple virtual machines in parallel at the cost of performance. Object storage offers parallel access from multiple VMs and scalability at the cost of performance and consistency. Object storages are typically integrated on the application level rather than the OS level. When it comes to data storage virtual machines work exactly like your physical machine would: there is a physical disk (or multiple) that store the files. The difference is that in the cloud your virtual machine may make use of a distributed storage architecture instead of using a local disk. In a distributed storage system the data isn't stored on the machine that runs the virtual machine so a hardware failure on that machine will not cause a data loss. However, a distributed storage system is generally either slower or more expensive for the same performance by several magnitudes so using a local storage may still be needed for some use cases. When we talk about storage systems we are talking about two types: block devices and filesystems. On the physical disk data is stored in its raw form so the disk itself has no information about which data belongs to which file. Filesystems organize data into blocks of a fixed or dynamic size and then create a database (mapping table) of which file entry consists of which blocks of data. Keep in mind that the blocks of a single file may be distributed all over the whole disk randomly so that's something the filesystem must keep track of. Therefore we traditionally call raw disk devices block devices . Block devices are (with very few exceptions) only accessible from a single virtual or physical machine since otherwise the machines would have to synchronize their file system operations on that device. The only notable exception is GFS2 . While you can use GFS2 over a shared storage infrastructure if you have control over it cloud providers enforce a single-VM access policy. In other words, one block storage device can only ever be used by a single VM.","title":"Storage"},{"location":"lectures/2-iaas/#local_storage","text":"As described above the simplest and most widely supported option to store data from your virtual machine is a disk that is locally attached to the physical machine running the VM. This option offers you the highest performance at a relatively low price point. The reason for that is that it is the simplest and cheapest to build. Some cloud providers offer disk redundancy ( RAID ) while others don't. At any rate a hardware failure of the physical machine means that your data may become unavailable for a period of time or may be completely lost. It is therefore very advisable to solve redundancy on top of the virtual machine, e.g. by building a replicated database setup. If, however, your database is replicated anyway you may no longer need the more expensive storage options and this can be a great way to save costs. Which of the following is provided by local storage? Fault-tolerance in the face of a machine failure. High IO performance. The ability to move the data volume to a different machine. The ability to access the data volume from several machines at once. Data consistency.","title":"Local Storage"},{"location":"lectures/2-iaas/#network_block_storage","text":"Network block storage means a block storage that is delivered over the network. The network here can mean a traditional IP network or a dedicated Fibre Channel infrastructure. As described before block storage is, in general, single-VM only. You can't access the files stored on a block storage device from multiple virtual machines. Also note that Network Block Storage does not automatically come with redundancy. Some solutions, such as iSCSI simply offer the disk of one machine to another. More advanced ones like Ceph RBD or the cloud provider offerings such as EBS by Amazon , however, do offer redundancy. At any rate, using Network Block Storage does not absolve you from the duty to make backups and have a documented and tested disaster recovery strategy. Which of the following is provided by network block storage? Fault-tolerance in the face of a machine failure. High IO performance. The ability to move the data volume to a different machine. The ability to access the data volume from several machines at once. Data consistency.","title":"Network Block Storage"},{"location":"lectures/2-iaas/#network_file_systems","text":"In contrast to network block storage network file systems offer access to data not on a block level, but on a file level. Over the various network file system protocols machines using these file systems can open, read and write files, and even place locks on them. The filesystem has to keep track of which machine has which file open, or has locks on which file. When multiple machines edit the same file in parallel the filesystem has to ensure that these writes are consistent. This means that network file systems are either much slower than block-level access (e.g. NFS ) or require a great deal more CPU and RAM to keep track of the changes across the network (e.g. CephFS ). Some cloud providers also offer this, for example Amazon's EFS . Which of the following is provided by network filesystems? Fault-tolerance in the face of a machine failure. High IO performance. The ability to move the data volume to a different machine. The ability to access the data volume from several machines at once. Data consistency.","title":"Network File Systems"},{"location":"lectures/2-iaas/#object_storage","text":"Object storage systems are similar to network file systems in that they deal with files rather than blocks. However, they do not have the same synchronization capabilities as network file systems. Files can generally only be read or written as a whole and they also don't have the ability to lock a file. While object storages technically can be used as a filesystem on an operating system level for example by using s3fs this is almost always a bad idea due to the exceptionally bad performance and stability issues. Operating system level integration should only be used as a last resort and object storages should be integrated on the application level. We will discuss object storage services in detail in our next lesson. Which of the following is provided by object storages? Fault-tolerance in the face of a machine failure. High IO performance. The ability to move the data volume to a different machine. The ability to access the data volume from several machines at once. Data consistency. Which storage type is Amazon's EBS? Local disk Network block storage Network filesystem Object storage Which storage type is Amazon's EFS? Local disk Network block storage Network filesystem Object storage Which storage type is Ceph RBD? Local disk Network block storage Network filesystem Object storage Which storage type is iSCSI? Local disk Network block storage Network filesystem Object storage Which storage type is S3? Local disk Network block storage Network filesystem Object storage","title":"Object storage"},{"location":"lectures/2-iaas/#network","text":"The next big topic concerning IaaS services is networks. Before we go into the cloud-aspect let's look at how the underlying infrastructure is built. As indicated in the first lecture it is strongly recommended that you familiarize yourself with the basics of computer networking, such as the Ethernet, IP and TCP protocols as you will need them to understand this section. So, let's get started. Imagine a data center from the first lecture. Your task is to build an IaaS cloud provider. You put your servers that will serve as your hosts for virtual machines in the racks. These servers will be connected to the Top-of-Rack switches (yes, two for redundancy) using 10 GBit/s network cables. The switches are themselves connected among each other and across racks with several 100 GBit/s. This sounds like a lot of bandwidth available but keep in mind that your virtual machines get assigned to the physical machines as capacity allows. There is no cloud provider that can guarantee the bandwidth or latency between two virtual machines. Generally cloud providers only state the theoretical bandwidth of the connection a virtual machine has to the switching fabric, but not the specific bandwidth between two distinct virtual machines. This is part of the reason why in the cloud scaling horizontally (adding more machines) is preferred rather than creating huge instances with lots of resources.","title":"Network"},{"location":"lectures/2-iaas/#network_architectures_offered_by_cloud_providers","text":"When we look at the network offerings by cloud providers there are three types: Private-only network with NAT : This option is provided by the larger cloud providers such as AWS , Azure , GCP and IBM . This setup gives each virtual machine a private IP address on a private network only. When a public IP address is needed that public IP is handled by the gateway provided by the cloud provider and the incoming traffic is forwarded to the virtual machine on the private network using Destination NAT . Multiple private networks (VPC's) can be assigned to a virtual machine and they can work independently. Default public IP : This option is provided by smaller IaaS providers such as DigitalOcean , Exoscale , Hetzner , Linode , Upcloud , and Vultr . In this setup each virtual machine is attached to a public network and is directly assigned one public IP address. Optionally private networks can be attached as well but the first public IP generally cannot be removed as it is required for user data to work. Fully customizable: This setup allows the customer to design their network connectivity as they see fit. This setup is suitable for enterprise customers who want to move their on-premises setup into the cloud without changing their architecture (lift-and-shift). This option is offered by 1&1 IONOS . Note There are several other cloud providers which we have no information on, such as the Deutsche Telekom/Open Telekom Cloud , or the Alibaba Cloud . You can classify any cloud provider you come across into these categories. Note Out of group 2 it is worth mentioning that the services that are available on the public network (firewalls, load balancers) are often not available on private networks.","title":"Network architectures offered by cloud providers"},{"location":"lectures/2-iaas/#firewalling","text":"IaaS providers often also offer network firewalls as a service, included in the platform. Firewalls generally have two rule types: INGRESS (from the Internet or other machines to the current VM) and EGRESS (from the current VM to everywhere else). Firewall providers often employ the concept of security groups . The implementation varies greatly, but in general security groups are a reusable set of rules that can be applied to a VM. For most cloud providers you will need to create an explicit rule allowing traffic to flow between two machines in the same security group. The advantage of security groups is that the rules can be made in such a way that they reference other security groups rather than specific IP addresses. For example, the database security group could be set to allow connections only from the appserver security group but not from anywhere else. This can help with the dynamic nature of the cloud since you do not need to hard-code the IP addresses of the application servers. What do security groups offer? Filtering based on IP address Filtering based on the requested service Filtering based on the requested domain name Filtering based on the requested subpage on a website","title":"Firewalling"},{"location":"lectures/2-iaas/#network_load_balancers","text":"Network load balancers are an option some cloud providers offer. In contrast to Application Load Balancers they do not offer protocol decoding (such as routing requests to backends based on the requested web address), they only balance incoming connections to a pool of backends. Depending on the cloud provider in question network load balancers may or may not offer terminating encrypted connections (SSL/TLS), and may be bound to virtual machine pools. It is also cloud provider specific if load balancers are offered in private networks or not. When designing an architecture it is worth considering if the real IP address of the connecting client will be needed. If the backend needs to know the real IP address of the client and the network load balancer handles SSL/TLS termination that combination may not be suitable for the task unless a specific trick such as the proxy protocol from Digital Ocean . Network load balancers without SSL/TLS termination should, in general, make the client IP available to the backends. In order to make sure requests are not sent to faulty backends NLBs include a health check feature. This health check either simply opens a connection to the respective backends (TCP check) or requests a webpage from the backend (HTTP check). If the check fails the backend is removed from the rotation. When integrated with virtual machine pools they may automatically shut down and replace the faulty machine, but this is often not the case. It is on the operator to destroy faulty machines. When talking about load balancers an interesting question is the load balancing strategy. Most load balancers support either round robin (selecting the next backend in the list) or source hashing (routing the same connecting IP to the same backend). What do NLBs typically offer? Spreading incoming connections across multiple backend machines equally. Spreading incoming connections across multiple backend machines, sending connections from the same source to the same backend. Spreading incoming connections across multiple backend machines, based on the domain name. Spreading incoming connections across multiple backend machines, based on the subpage requested. Terminating encrypted connections so the backend doesn't have to (SSL/TLS offloading).","title":"Network load balancers"},{"location":"lectures/2-iaas/#vpns_private_interconnects_and_routing_services","text":"While it seems convenient at first to use only the public network several organizations have security models that prevent accidental public exposure of services not only by implementing the appropriate firewalls (e.g. with security groups) but also by not having private services on the public internet at all. To connect these internal services you must be on a private network. However, this presents a problem when moving data between several, geographically distributed locations. Most companies don't own continent-spanning fiber channel backbones where they could simply create a private network without going on the internet. This means that most companies have to choose one of two methods if they want to create a private connectivity between locations (and the cloud): an MPLS tunnel or VPN . MPLS tunnels create a virtual connectivity that does not go on the Internet. While being expensive and slow to set up, it can offer a guaranteed bandwidth, latency and better security than a VPN. VPN's on the other hand create a virtual connectivity by sending data over the Internet in an encrypted form. Bandwidth or latency cannot be guaranteed, and there are several drawbacks (such as decreased MTU ) but it's a very affordable solution. Larger cloud providers tend to offer both options. MPLS is supported by the larger cloud providers ( AWS Direct connect , Azure Express Route , or Google Cloud Interconnect ) and also some smaller ones (e.g. Exoscale Private Connect ). VPN is also offered mostly by large providers ( AWS VPC VPN , Azure VPN , or Google Cloud VPN ). However, keep in mind that this VPN is a site-to-site VPN built on IPSec and requires a fixed IP address on your side as well. In other words you can't use this VPN to connect from your laptop to the cloud on the go. The only cloud service that offers a mobile device to cloud connectivity at the time of writing is Azure's Point-to-Site VPN . It is also worth noting that VPN's can be used to connect cloud providers together. What VPN type is offered by all major cloud providers? Site-to-site Device-to-site Device-to-device What VPN protocol is offered by all major cloud providers? OpenVPN IPsec SSTP L2TP PPTP What VPN type can IPsec offer by itself? Site-to-site Device-to-site Device-to-device","title":"VPNs, private interconnects, and routing services"},{"location":"lectures/2-iaas/#dns","text":"The Domain Name Service is one of the services that are all but required for building an infrastructure. It provides domain name to IP address resolution, such as pointing your domain example.com to an IP address of your servers. There is a difference, however, between DNS services on offer. Some DNS services by cloud providers offer only simple resolution, other providers offer more advanced features. These features include being able to host the DNS service only on a private network without exposing it to the internet. More advanced features may include automatic DNS failover. This involves running regular health checks on your services and if your primary service fails the DNS service can automatically switch to the secondary IP. There are even services that offer advanced functionality such as routing traffic to different servers based on the geographic location of the client. This can be used for advanced builds such as building a custom CDN . CDNs are discussed in the next lecture .","title":"DNS"},{"location":"lectures/2-iaas/#monitoring","text":"Some cloud providers offer includes basic monitoring, such as CPU or memory usage. Some providers are offering monitoring agents you can install on your virtual machine to get more data in the monitoring interface. With some cloud providers monitoring alerts can be integrated with virtual machine pools to provide automatic scaling, either automatically or using lambdas/FaaS, which we will talk about in the next lecture . Often times the monitoring facilities offered by cloud providers are not sufficient for keeping an application running and more detailed systems are needed. These will be discussed in greater detail in the lecture 5 .","title":"Monitoring"},{"location":"lectures/3-xaas/","text":"Download PPTX \ud83d\udcbb Download MP3 \ud83c\udfa7 Watch Video \ud83c\udfac Beyond IaaS This lecture walks you through the most important services offered by cloud providers beyond the IaaS layers. Sometimes this is called \"PaaS\" (Platform as a Service) indicating that it is intended as a developer platform. The easiest way to remember the difference is that IaaS offers virtual machines and connected services. In order to operate on top of an IaaS platform you need someone skilled in running an operating system. In other words, you need a system administrator. PaaS on the other hand is intended for developers. The main goal of PaaS services is enabling developers to deploy applications without having to manage the operating system of the underlying virtual machine. There is, however, nothing preventing you from mixing IaaS and PaaS services. A typical use case would be using a managed database with virtual machines. This helps smaller teams because operating a database proficiently on a small scale can be an undue burden. This might not seem like a big deal but consider that databases store data. Every time when data storage is concerned disaster recovery becomes more complex. If an IaaS team were to operate a database themselves they would need to regularly test backups and disaster recovery procedures. Managed databases take that complexity away. Similarly, building a redundant database system that can perform an automatic failover requires a high skill level in managing that specific database engine. This skill level may not be available in small teams, or a small team may not want to spend time on managing the database instead of focusing on their core objective. Application load balancers # In a hurry? Application Load Balancers offer load balancing on a HTTP level. They offer advanced routing capabilities based on HTTP requests. Application load balancers provide load balancing capabilities on layer 7 of the OSI-model . In practice application load balancers support HTTP and HTTPS load balancing. HTTP is a request-response protocol. Typically connections are short-lived but longer connections (e.g. with Websockets ) are also possible. Good application load balancers allow sending traffic to a different set of backends based on the host name ( example.com ) as well as the path ( /some/address ). Some applications also require that subsequent requests from the same client end up on the same backend. This is called session stickiness and can be achieved in a variety of ways. Less advanced solutions route requests based on the source IP, while more advanced versions send a so-called cookie to the client and route subsequent requests based on that cookie. Sticky sessions, however, present a problem: when a backend goes down the users of that backend will be redistributed to other backends. In practice this usually means that users will be logged out. When a cascading fault, or a rolling update occurs that takes down multiple backends in rapid succession users can be subjected to multiple involuntary logouts. This has an adverse effect on user experience which is why newer, so-called \u201ccloud native\u201d applications don't use sticky sessions. Instead, cloud native applications put client-specific data (e.g. session data ) in database systems with redundancy. Sessions themselves have their own race condition problems , but that is not a discussion for this lecture. What are HTTP cookies? Tasty treats for developers doing good work A tiny piece of information stored on the user's computer A secure way to transfer information from one computer to another What are sticky sessions? A connection that is stuck and needs to be terminated A configuration on a load balancer to route requests from the same user to the same backend What ISO/OSI network layer do application load balancers work on? Layer 1 Layer 2 Layer 3 Layer 4 Layer 7 What protocol do application load balancers typically balance? Ethernet IP TCP HTTP Content Delivery Networks (CDNs) # In a hurry? Network connections have a high latency because the light bounces around in the fiberoptic cable. Therefore, access from the other side of the globe is slow. This is resolved by CDN's, which offer a network of machines that cache the content close to the end user. While it seems the Internet is blazing fast nowadays delivering content to the other side of the planet is still an issue. As you may know most of the Internet is comprised of fiber optic cabling . Data is transmitted by turning a laser on and off in rapid succession. Let's do a little mental exercise: the speed of light is 299.792.458 m/s. The circumference of our planet is 40,075 km. A ray of light should be able to round the planet in 40.075.000 / 299.792.458 = 0.113 seconds. In other words any data transmitted should be able to travel around the world in ~100 milliseconds. Yet, in practice we see latencies of around 300 milliseconds when transfering data to the other side of the globe. The issue is that light does not go \u201caround the corner\u201d. Fiber optic cables work by having reflective walls so the light bounces off the walls repeatedly to reach the end. This multiplies the distance light has to travel to reach the other side of the planet. This is further compounded by the fact that light can only travel so far in a fiber optic cable, repeaters and routers in between also need time to process the data. The problem is further compounded by how HTTP works. When a website is loaded there are several elements that have to wait for each other. The website may reference a style sheet (CSS file), the CSS file may reference an image and so on. This means several round trips have to be done to build a page. HTTP/2 server push attempts to mitigate this. CDN's work around this problem by replicating content to servers closer to the users. When a user requests a certain file, that request first lands on the CDN's edge node . If the CDN has the file in the cache the delivery is very fast. If, however, the CDN does not have the content in cache the delivery will take longer. In essence, CDN's help with latency issues if the content can be cached . In other words, this helps mostly with static content, but in combination with functions as a service (see below) dynamic content can also be delivered from edge nodes to some extent. Did you know? Low latency connections are important not just for delivering content. Stock exchanges benefit from very low latencies. For example, there is a private microwave network between Frankfurt and London that has twice the speed of the public internet. Did you know? SpaceX is building the Starlink network to provide lower latency connectivity across the globe. What is the typical latency from Sydney to Frankfurt? (16,473 km) 108ms 290ms 1082ms What is latency? The time it takes for a data packet to travel from the source to the destination. The time it takes for a data packet to travel from the source to the destination and back. The time it takes to transfer a 1 kB file. The maximum data transfer rate between two points. Object Storage # In a hurry? Object storages are file storages that offer a limited set of operations (upload, download, list, delete, change access permissions, etc). Due to the limited feature set object storages are able to offer redundant, low cost file storage. Most object storages also allow publishing files to the internet directly. In the previous lecture we briefly mentioned object storages. As a reminder, traditional block storage devices and the filesystems implemented on top of them have several features you may consider advanced: Multiple programs can open the same file in parallel, Files can be read and written partially, Locks can be placed on files preventing other processes from accessing them. Object storages are different. The most popular Amazon S3 protocol offers the ability to up- and download, list and delete files. However, files can only be uploaded as a whole, partial reads or writes are not possible. Consistency is also not guaranteed when multiple programs are accessing the same files in parallel. However, due to the limited feature set object storages have a few unique abilities not afforded by traditional block storage: They are redundant over multiple servers by design. The loss of a single physical machine does not mean a data loss. The S3 protocol offers the ability to place ACL's (access control lists) on the objects uploaded to the object storage. This allows making files publicly accessible over the web without needing to maintain a server. Some object storage implementations offer the ability to keep multiple versions of files. Clients can be prevented from deleting older versions making versioning an effective data loss prevention mechanism. Some object storage implementations offer the ability to lock files from being modified in the future. This is especially important when adhering to corporate or government data retention requirements. What is a typical use case for an object storage? Long-term storage for backups Storage of user-uploaded files (e.g. images) Storage backend for databases. How are object storages typically integrated? They are mounted as a drive in the operating system. The application typically supports using object storages directly. What filesystem feature do object storages NOT offer? The ability to partially write or read a file. The ability to obtain an exclusive lock for exclusive access. The ability to change access permissions. Cold storage # Some providers offer an extension to their object storage system that puts data in cold storage (e.g. on tape ). Data can be uploaded directly via the API, or in the case of very large data amounts shipped to the provider on hard drives. Did you know? \u201cNever underestimate the bandwidth of a truck full of hard drives.\u201d \u2014 is an industry saying that kept its validity to this day. Since the data is stored on offline (cold) storage the data retrieval is not as immediate as with the object storage. To retrieve data from cold storage you often need to wait several hours until the data becomes available. Therefore, an effective backup strategy to the cloud often involves moving data to the object storage first and only older backups to cold storage. Amazon, for example, allows for automating this process using S3 lifecycle rules . Databases as a Service (DBaaS) # In a hurry? DBaaS offers managed databases. Different database types and replication methods offer different levels of consistency. ACID-compliant databases are typically used for cases where data must not be lost under any circumstances, while BASE databases (eventually consistent) are used where the loss of a few records in a failure scenario is acceptable. As mentioned before, one of the most difficult services to operate are ones that store state. They are also highly standardized between cloud providers. This makes them an ideal candidate for a company to use PaaS instead of managing everything in-house. Database consistency # When considering which database offering to use database consistency plays a key role. It is a common occurrence that startups pick, for example, MongoDB because of its \u201chipness\u201d instead of it considering the benefits and drawbacks of this (or any other) database solution. One of the important aspects of databases is the way they offer consistency. Traditional SQL-style databases often supports what's called ACID-compliance . This stands for Atomicity, Consistency, Isolation, and Durability. Atomicity guarantees that series of database operations will either occur as a whole or not at all. The practical implementation of atomicity are transactions . Consistency guarantees that if an operation would violate the rules of a database (whatever they may be) the whole transaction is rolled back to a previously known good state. Isolation guarantees that two transactions executing at the same time will not affect each other. While this can be turned on it is very resource intensive and database engines often allow for a lower isolation level to increase performance. Durability guarantees that, if a user has received a success message the data is actually stored. This guarantee is not met by most NoSQL databases. In contrast, most NoSQL databases implement BASE: Basic Availability guarantees that the database is available most of the time. Soft-state allows for data to be non-consistent. In other words, the database doesn't have to be \u201cin-sync\u201d across all database replicas. Eventual consistency allows for written data to be translated across multiple replicas at a future point in time. This is, of course, the theory. In practice even BASE-compliant databases tend to offer some ACID-features. For example, MongoDB supports transactions for atomicity, but is still a BASE system. The above two database types are illustrated by the CAP theorem . The CAP theorem says that out of Consistency, Availability, and Partition tolerance only two can be chosen at any given time. The trick here is that partition tolerance means that a system can survive when the network is split in two parts. This is not optional, so we can pick between CP and AP databases. CP databases will guarantee that a database operation will either fail or if successful, be consistent across all replicas. We will never have a case where a change is not yet translated to a replica and we receive outdated information. This is, in practice, often achieved by quorum . When a network is split in two only the nodes that can form a majority are able to perform database operations. Nodes that are not able to join the majority cannot perform database operations. AP databases on the other hand will always serve database operations, but cannot guarantee consistency. The data read may be outdated. Database writes may be lost if the node goes down before it is able to transmit the data to the other nodes. There are benefits for both and the appropriate database type must be chosen for the task at hand. Relational databases (SQL) # SQL or relational databases (RDBMS) are one of the more traditional database systems. They use predefined data tables with fixed columns and datatypes. Most SQL databases also allow creating references between tables. These references or foreign keys allow for validation. For example, you could create database that has an articles and a comments table. Since the comments table could reference the articles table it would be impossible to create a comment with an invalid article ID. While it is easy to make the assumption that RDBMS implement ACID compliance (CP) in most cases it is only true when the database is run on a single node. When using replication most RDBMS implement asynchronous replication which puts them partially in the BASE / AP territory. When using a managed database make sure your cloud provider supports a replication that suits your consistency needs. Relational databases can be used for a wide variety of use cases, including the storage of large amounts of data. However, RDBMS meet their limits when it comes to structuring data. When a large number of relations are needed a graph database may be better suited for the task, while the storage of a deep structured object may call for a document database. Popular RDBMS systems are: Microsoft SQL Server, MySQL, PostgreSQL Key-value stores # Key-value stores are very simply databases that store data objects based on a key , a name for the object. The object itself is inherently opaque, the database engine does not know or care how it is structured. In other words, key-value stores treat the value part as a binary object. Typical key-value stores include: Memcached, Amazon S3 Document databases # Document databases offer the user the ability to store a structured document, such as a JSON file. Often it also creates indexes over these documents that allow for quick searching. While SQL databases enforce fixed columns, document databases often offer the ability to use fields dynamically. While this may make development easier in the early stages it also presents some risks. Since the database engine does not validate the field names it is easier to create a mess in the database with typos, inconsistent changes, etc. It is therefore recommended to create a strict data model in the application layer. Most document databases fall strictly in the BASE/AP category when it comes to replication. Popular document databases are: MongoDB, ElasticSearch, Amazon DynamoDB Time Series databases # Time-series databases are usually used in monitoring systems and store numeric values associated with a certain timestamp. One of the most famous cloud-native monitoring system with an integrated time-series databases is Prometheus . Graph databases # Graph databases are special databases that store relationships of datasets with each other and allow making queries over them. These database types can be used for creating social graphs, interests, etc. A popular graph database option is Neo4j Which of the following database engines would be suitable to store transfers on a bank account? A cluster of eventually consistent NoSQL databases (e.g. MongoDB) A single, strictly consistent NoSQL database (e.g. etcd) A cluster of relational databases with asynchronous replication (e.g. vanilla MySQL) A cluster of relational databases with synchronous replication (e.g. MySQL/Galera cluster) Functions as a Service (FaaS / Lambda) # Functions as a Service or Lambdas are a way to run custom code without having to worry about the runtime environment. These pieces of code are triggered by events: a timer, a message in a message queue, or an incoming HTTP request. To make this easier to understand let's consider a hypothetical scenario: let's say you want to have an up to date list of IP addresses of all virtual machines you are running. In this case you have two options. Option one is to run a piece of code every few minutes to update the IP list. However, this may not be fast enough, and it may also not scale with a very large number of machines. So your other option is to use a FaaS function to react to the fact that a new instance has been launched or an instance has been destroyed. This function would be called every time there is a change . In a different scenario FaaS would give you the ability to run small amounts of program code on CDN edge nodes giving you the ability for fast, dynamic content delivery. One such service is Lambda@Edge by Amazon . However, keep in mind that FaaS may have limitations in terms of performance. To conserve processing power the cloud provider will shut down runtime environments not in use and a restart may take longer than usual. This may, for example, destroy the advantage edge node lambda functions have in terms of latency for low traffic sites. Containers as a Service (CaaS) # Containers are a way to run applications in an isolated environment without dedicating a full virtual machine to them. However, as you might imagine, containers are not magic and cannot automatically move from one physical server to another. This, and many other tasks are the job of container orchestrators such as Kubernetes. Since Kubernetes and its alternatives are incredibly complex to operate many cloud providers take on this burden, and offer containers as a service to customers. Note that containers are the topic of the next lecture so we won't cover them in detail here. Stream processing and Business Intelligence tools # One of the most complex setups to run in an on-premises environment is something nowadays known as a datalake. IT aggregates data from many different source databases and allows data scientists to extract valuable information from it. It is not uncommon to see several dozen source database. What's more, some analyses require close to real time data processing. The tools usually seen in such as setup are HDFS , Apache Spark , Kafka , and several more. The number of source databases and the number of tools listed here should tell you how complex such a setup can be. This is one of the reasons why even large, traditional corporations are seriously considering moving their data analytics into the cloud. Deployment pipelines # While there are many, many more services on offer in the cloud we'd like to mention a final piece to the puzzle: deployment pipelines. These deployment pipelines allow a DevOps engineer to build automatic software deployment systems with relative ease. Deployment pipelines may start with something simple like a CI/CD system that an engineer can extend with calls to an API, or may include full-on code quality tools, test systems and management for the production environment. The general idea is that the code is handled in a version control system such as git. When the code is pushed into the version control system an automatic build and deployment process is started. This can involve compiling the source code to automatic tests running. The deployment pipeline then takes care of copying the software to the appropriate environment, such as creating a container and deploying it to the CaaS environment or creating virtual machines with the appropriate code. When this service is offered by the cloud provider that runs the infrastructure themselves, these are usually tightly integrated, allowing a prospective user to simply point-and-click to create the whole environment. It is debatable if such an automatically created environment is suitable for production use, but it may give a good starting point to customize the setup from. Typical services that offer deployment pipelines: CircleCI, GitHub actions, AWS CodePipeline","title":"3. Beyond IaaS"},{"location":"lectures/3-xaas/#application_load_balancers","text":"In a hurry? Application Load Balancers offer load balancing on a HTTP level. They offer advanced routing capabilities based on HTTP requests. Application load balancers provide load balancing capabilities on layer 7 of the OSI-model . In practice application load balancers support HTTP and HTTPS load balancing. HTTP is a request-response protocol. Typically connections are short-lived but longer connections (e.g. with Websockets ) are also possible. Good application load balancers allow sending traffic to a different set of backends based on the host name ( example.com ) as well as the path ( /some/address ). Some applications also require that subsequent requests from the same client end up on the same backend. This is called session stickiness and can be achieved in a variety of ways. Less advanced solutions route requests based on the source IP, while more advanced versions send a so-called cookie to the client and route subsequent requests based on that cookie. Sticky sessions, however, present a problem: when a backend goes down the users of that backend will be redistributed to other backends. In practice this usually means that users will be logged out. When a cascading fault, or a rolling update occurs that takes down multiple backends in rapid succession users can be subjected to multiple involuntary logouts. This has an adverse effect on user experience which is why newer, so-called \u201ccloud native\u201d applications don't use sticky sessions. Instead, cloud native applications put client-specific data (e.g. session data ) in database systems with redundancy. Sessions themselves have their own race condition problems , but that is not a discussion for this lecture. What are HTTP cookies? Tasty treats for developers doing good work A tiny piece of information stored on the user's computer A secure way to transfer information from one computer to another What are sticky sessions? A connection that is stuck and needs to be terminated A configuration on a load balancer to route requests from the same user to the same backend What ISO/OSI network layer do application load balancers work on? Layer 1 Layer 2 Layer 3 Layer 4 Layer 7 What protocol do application load balancers typically balance? Ethernet IP TCP HTTP","title":"Application load balancers"},{"location":"lectures/3-xaas/#content_delivery_networks_cdns","text":"In a hurry? Network connections have a high latency because the light bounces around in the fiberoptic cable. Therefore, access from the other side of the globe is slow. This is resolved by CDN's, which offer a network of machines that cache the content close to the end user. While it seems the Internet is blazing fast nowadays delivering content to the other side of the planet is still an issue. As you may know most of the Internet is comprised of fiber optic cabling . Data is transmitted by turning a laser on and off in rapid succession. Let's do a little mental exercise: the speed of light is 299.792.458 m/s. The circumference of our planet is 40,075 km. A ray of light should be able to round the planet in 40.075.000 / 299.792.458 = 0.113 seconds. In other words any data transmitted should be able to travel around the world in ~100 milliseconds. Yet, in practice we see latencies of around 300 milliseconds when transfering data to the other side of the globe. The issue is that light does not go \u201caround the corner\u201d. Fiber optic cables work by having reflective walls so the light bounces off the walls repeatedly to reach the end. This multiplies the distance light has to travel to reach the other side of the planet. This is further compounded by the fact that light can only travel so far in a fiber optic cable, repeaters and routers in between also need time to process the data. The problem is further compounded by how HTTP works. When a website is loaded there are several elements that have to wait for each other. The website may reference a style sheet (CSS file), the CSS file may reference an image and so on. This means several round trips have to be done to build a page. HTTP/2 server push attempts to mitigate this. CDN's work around this problem by replicating content to servers closer to the users. When a user requests a certain file, that request first lands on the CDN's edge node . If the CDN has the file in the cache the delivery is very fast. If, however, the CDN does not have the content in cache the delivery will take longer. In essence, CDN's help with latency issues if the content can be cached . In other words, this helps mostly with static content, but in combination with functions as a service (see below) dynamic content can also be delivered from edge nodes to some extent. Did you know? Low latency connections are important not just for delivering content. Stock exchanges benefit from very low latencies. For example, there is a private microwave network between Frankfurt and London that has twice the speed of the public internet. Did you know? SpaceX is building the Starlink network to provide lower latency connectivity across the globe. What is the typical latency from Sydney to Frankfurt? (16,473 km) 108ms 290ms 1082ms What is latency? The time it takes for a data packet to travel from the source to the destination. The time it takes for a data packet to travel from the source to the destination and back. The time it takes to transfer a 1 kB file. The maximum data transfer rate between two points.","title":"Content Delivery Networks (CDNs)"},{"location":"lectures/3-xaas/#object_storage","text":"In a hurry? Object storages are file storages that offer a limited set of operations (upload, download, list, delete, change access permissions, etc). Due to the limited feature set object storages are able to offer redundant, low cost file storage. Most object storages also allow publishing files to the internet directly. In the previous lecture we briefly mentioned object storages. As a reminder, traditional block storage devices and the filesystems implemented on top of them have several features you may consider advanced: Multiple programs can open the same file in parallel, Files can be read and written partially, Locks can be placed on files preventing other processes from accessing them. Object storages are different. The most popular Amazon S3 protocol offers the ability to up- and download, list and delete files. However, files can only be uploaded as a whole, partial reads or writes are not possible. Consistency is also not guaranteed when multiple programs are accessing the same files in parallel. However, due to the limited feature set object storages have a few unique abilities not afforded by traditional block storage: They are redundant over multiple servers by design. The loss of a single physical machine does not mean a data loss. The S3 protocol offers the ability to place ACL's (access control lists) on the objects uploaded to the object storage. This allows making files publicly accessible over the web without needing to maintain a server. Some object storage implementations offer the ability to keep multiple versions of files. Clients can be prevented from deleting older versions making versioning an effective data loss prevention mechanism. Some object storage implementations offer the ability to lock files from being modified in the future. This is especially important when adhering to corporate or government data retention requirements. What is a typical use case for an object storage? Long-term storage for backups Storage of user-uploaded files (e.g. images) Storage backend for databases. How are object storages typically integrated? They are mounted as a drive in the operating system. The application typically supports using object storages directly. What filesystem feature do object storages NOT offer? The ability to partially write or read a file. The ability to obtain an exclusive lock for exclusive access. The ability to change access permissions.","title":"Object Storage"},{"location":"lectures/3-xaas/#cold_storage","text":"Some providers offer an extension to their object storage system that puts data in cold storage (e.g. on tape ). Data can be uploaded directly via the API, or in the case of very large data amounts shipped to the provider on hard drives. Did you know? \u201cNever underestimate the bandwidth of a truck full of hard drives.\u201d \u2014 is an industry saying that kept its validity to this day. Since the data is stored on offline (cold) storage the data retrieval is not as immediate as with the object storage. To retrieve data from cold storage you often need to wait several hours until the data becomes available. Therefore, an effective backup strategy to the cloud often involves moving data to the object storage first and only older backups to cold storage. Amazon, for example, allows for automating this process using S3 lifecycle rules .","title":"Cold storage"},{"location":"lectures/3-xaas/#databases_as_a_service_dbaas","text":"In a hurry? DBaaS offers managed databases. Different database types and replication methods offer different levels of consistency. ACID-compliant databases are typically used for cases where data must not be lost under any circumstances, while BASE databases (eventually consistent) are used where the loss of a few records in a failure scenario is acceptable. As mentioned before, one of the most difficult services to operate are ones that store state. They are also highly standardized between cloud providers. This makes them an ideal candidate for a company to use PaaS instead of managing everything in-house.","title":"Databases as a Service (DBaaS)"},{"location":"lectures/3-xaas/#database_consistency","text":"When considering which database offering to use database consistency plays a key role. It is a common occurrence that startups pick, for example, MongoDB because of its \u201chipness\u201d instead of it considering the benefits and drawbacks of this (or any other) database solution. One of the important aspects of databases is the way they offer consistency. Traditional SQL-style databases often supports what's called ACID-compliance . This stands for Atomicity, Consistency, Isolation, and Durability. Atomicity guarantees that series of database operations will either occur as a whole or not at all. The practical implementation of atomicity are transactions . Consistency guarantees that if an operation would violate the rules of a database (whatever they may be) the whole transaction is rolled back to a previously known good state. Isolation guarantees that two transactions executing at the same time will not affect each other. While this can be turned on it is very resource intensive and database engines often allow for a lower isolation level to increase performance. Durability guarantees that, if a user has received a success message the data is actually stored. This guarantee is not met by most NoSQL databases. In contrast, most NoSQL databases implement BASE: Basic Availability guarantees that the database is available most of the time. Soft-state allows for data to be non-consistent. In other words, the database doesn't have to be \u201cin-sync\u201d across all database replicas. Eventual consistency allows for written data to be translated across multiple replicas at a future point in time. This is, of course, the theory. In practice even BASE-compliant databases tend to offer some ACID-features. For example, MongoDB supports transactions for atomicity, but is still a BASE system. The above two database types are illustrated by the CAP theorem . The CAP theorem says that out of Consistency, Availability, and Partition tolerance only two can be chosen at any given time. The trick here is that partition tolerance means that a system can survive when the network is split in two parts. This is not optional, so we can pick between CP and AP databases. CP databases will guarantee that a database operation will either fail or if successful, be consistent across all replicas. We will never have a case where a change is not yet translated to a replica and we receive outdated information. This is, in practice, often achieved by quorum . When a network is split in two only the nodes that can form a majority are able to perform database operations. Nodes that are not able to join the majority cannot perform database operations. AP databases on the other hand will always serve database operations, but cannot guarantee consistency. The data read may be outdated. Database writes may be lost if the node goes down before it is able to transmit the data to the other nodes. There are benefits for both and the appropriate database type must be chosen for the task at hand.","title":"Database consistency"},{"location":"lectures/3-xaas/#relational_databases_sql","text":"SQL or relational databases (RDBMS) are one of the more traditional database systems. They use predefined data tables with fixed columns and datatypes. Most SQL databases also allow creating references between tables. These references or foreign keys allow for validation. For example, you could create database that has an articles and a comments table. Since the comments table could reference the articles table it would be impossible to create a comment with an invalid article ID. While it is easy to make the assumption that RDBMS implement ACID compliance (CP) in most cases it is only true when the database is run on a single node. When using replication most RDBMS implement asynchronous replication which puts them partially in the BASE / AP territory. When using a managed database make sure your cloud provider supports a replication that suits your consistency needs. Relational databases can be used for a wide variety of use cases, including the storage of large amounts of data. However, RDBMS meet their limits when it comes to structuring data. When a large number of relations are needed a graph database may be better suited for the task, while the storage of a deep structured object may call for a document database. Popular RDBMS systems are: Microsoft SQL Server, MySQL, PostgreSQL","title":"Relational databases (SQL)"},{"location":"lectures/3-xaas/#key-value_stores","text":"Key-value stores are very simply databases that store data objects based on a key , a name for the object. The object itself is inherently opaque, the database engine does not know or care how it is structured. In other words, key-value stores treat the value part as a binary object. Typical key-value stores include: Memcached, Amazon S3","title":"Key-value stores"},{"location":"lectures/3-xaas/#document_databases","text":"Document databases offer the user the ability to store a structured document, such as a JSON file. Often it also creates indexes over these documents that allow for quick searching. While SQL databases enforce fixed columns, document databases often offer the ability to use fields dynamically. While this may make development easier in the early stages it also presents some risks. Since the database engine does not validate the field names it is easier to create a mess in the database with typos, inconsistent changes, etc. It is therefore recommended to create a strict data model in the application layer. Most document databases fall strictly in the BASE/AP category when it comes to replication. Popular document databases are: MongoDB, ElasticSearch, Amazon DynamoDB","title":"Document databases"},{"location":"lectures/3-xaas/#time_series_databases","text":"Time-series databases are usually used in monitoring systems and store numeric values associated with a certain timestamp. One of the most famous cloud-native monitoring system with an integrated time-series databases is Prometheus .","title":"Time Series databases"},{"location":"lectures/3-xaas/#graph_databases","text":"Graph databases are special databases that store relationships of datasets with each other and allow making queries over them. These database types can be used for creating social graphs, interests, etc. A popular graph database option is Neo4j Which of the following database engines would be suitable to store transfers on a bank account? A cluster of eventually consistent NoSQL databases (e.g. MongoDB) A single, strictly consistent NoSQL database (e.g. etcd) A cluster of relational databases with asynchronous replication (e.g. vanilla MySQL) A cluster of relational databases with synchronous replication (e.g. MySQL/Galera cluster)","title":"Graph databases"},{"location":"lectures/3-xaas/#functions_as_a_service_faas_lambda","text":"Functions as a Service or Lambdas are a way to run custom code without having to worry about the runtime environment. These pieces of code are triggered by events: a timer, a message in a message queue, or an incoming HTTP request. To make this easier to understand let's consider a hypothetical scenario: let's say you want to have an up to date list of IP addresses of all virtual machines you are running. In this case you have two options. Option one is to run a piece of code every few minutes to update the IP list. However, this may not be fast enough, and it may also not scale with a very large number of machines. So your other option is to use a FaaS function to react to the fact that a new instance has been launched or an instance has been destroyed. This function would be called every time there is a change . In a different scenario FaaS would give you the ability to run small amounts of program code on CDN edge nodes giving you the ability for fast, dynamic content delivery. One such service is Lambda@Edge by Amazon . However, keep in mind that FaaS may have limitations in terms of performance. To conserve processing power the cloud provider will shut down runtime environments not in use and a restart may take longer than usual. This may, for example, destroy the advantage edge node lambda functions have in terms of latency for low traffic sites.","title":"Functions as a Service (FaaS / Lambda)"},{"location":"lectures/3-xaas/#containers_as_a_service_caas","text":"Containers are a way to run applications in an isolated environment without dedicating a full virtual machine to them. However, as you might imagine, containers are not magic and cannot automatically move from one physical server to another. This, and many other tasks are the job of container orchestrators such as Kubernetes. Since Kubernetes and its alternatives are incredibly complex to operate many cloud providers take on this burden, and offer containers as a service to customers. Note that containers are the topic of the next lecture so we won't cover them in detail here.","title":"Containers as a Service (CaaS)"},{"location":"lectures/3-xaas/#stream_processing_and_business_intelligence_tools","text":"One of the most complex setups to run in an on-premises environment is something nowadays known as a datalake. IT aggregates data from many different source databases and allows data scientists to extract valuable information from it. It is not uncommon to see several dozen source database. What's more, some analyses require close to real time data processing. The tools usually seen in such as setup are HDFS , Apache Spark , Kafka , and several more. The number of source databases and the number of tools listed here should tell you how complex such a setup can be. This is one of the reasons why even large, traditional corporations are seriously considering moving their data analytics into the cloud.","title":"Stream processing and Business Intelligence tools"},{"location":"lectures/3-xaas/#deployment_pipelines","text":"While there are many, many more services on offer in the cloud we'd like to mention a final piece to the puzzle: deployment pipelines. These deployment pipelines allow a DevOps engineer to build automatic software deployment systems with relative ease. Deployment pipelines may start with something simple like a CI/CD system that an engineer can extend with calls to an API, or may include full-on code quality tools, test systems and management for the production environment. The general idea is that the code is handled in a version control system such as git. When the code is pushed into the version control system an automatic build and deployment process is started. This can involve compiling the source code to automatic tests running. The deployment pipeline then takes care of copying the software to the appropriate environment, such as creating a container and deploying it to the CaaS environment or creating virtual machines with the appropriate code. When this service is offered by the cloud provider that runs the infrastructure themselves, these are usually tightly integrated, allowing a prospective user to simply point-and-click to create the whole environment. It is debatable if such an automatically created environment is suitable for production use, but it may give a good starting point to customize the setup from. Typical services that offer deployment pipelines: CircleCI, GitHub actions, AWS CodePipeline","title":"Deployment pipelines"},{"location":"lectures/4-containers/","text":"Download PPTX \ud83d\udcbb Download MP3 \ud83c\udfa7 Watch Video \ud83c\udfac Containers & Container Orchestrators In the second lecture we have talked about how applications interact with the kernel and the hardware. By now you know that applications running in ring 3 do not have direct access to things like the disk. To access those details they need to execute a system call to the kernel. The kernel will then give the application the details required, for example a file from the disk. Let's play a hypothetical game: process A wants to access a file called test.txt . It calls the kernel, the file is opened, and process A is now free to read the contents. When process B comes along and does the same, it will receive the same file. Or does it? In the Linux kernel the filesystem root directory (starting with / ) is a virtual construct that can be changed on a per-process basis. It is possible to make process A see a completely different folder structure then process B. This mechanism is called chroot . It is, in fact, so old that it predates Linux by more than a decade and was first present in Version 7 Unix in 1979. Try it out yourself! Run the following command on a 64 bit x86 Linux in an empty folder: mkdir -p ./bin &&\\ curl \"https://www.busybox.net/downloads/binaries/1.21.1/busybox-x86_64\" -o ./bin/busybox && \\ chmod +x ./bin/busybox && \\ ./bin/busybox --install ./bin && \\ sudo chroot . /bin/sh You should now be able to run ls -la or similar commands and see that you are in an almost empty filesystem with no ability to access the files outside. This alone only isolates the two processes on a filesystem level, there are still plenty of opportunities for two processes to interact, for example: On the network Interprocess Communication (IPC) Sending signals to each other by process ID etc. In the early 2000's there were two projects that attempted to implement process isolation in the Linux kernel: OpenVZ and Linux-VServer . Running these projects required patching and recompiling the Linux kernel from source. Starting around 2006 several companies, including Google, IBM, SGI, Bull, and the OpenVZ project itself has put significant effort into taking OpenVZ apart and piece by piece and submitting it to the Linux kernel. The User Beancounters from OpenVZ, for example, became cgroups and allow resource allocation to a group of processes. Do you want to know more about history? The history of containerization is surprisingly hard to piece together despite the relative young age of the technology. If you want a bit of a deeper dive take a look at \u201cA Brief History of Containers (by Jeff Victor & Kir Kolyshkin)\u201d . All these isolation technologies, together, form what's today known as Linux containers. Windows has undergone a similar development in recent years, and has virtualized the typical Windows interaction points between processes: job objects, process table, registry, filesystem, etc. In essence, containers don't exist . They are a collection of technologies that provide process-level isolation for an application. These isolation techniques can, among others, include: A separate, virtual network card Process separation (no ability to send signals to other containers) A separate view of the filesystem Restricted IPC between containers User separation / user ID mapping Do you want to build your own container engine? You can! Take a look at Liz Rice's Containers From Scratch talk! Container images # Docker was not the first technology to create containers using the modern Linux API's, but it was the one that made containers popular. The edge Docker had over its competitors was the way it dealt with images. Other container solutions at the time, like LXC, tried to emulate virtual machines where a full operating system would be installed, and then updated. Docker chose a different route. A Dockerfile would contain the instructions needed to build a container image containing the application that should be containerized. These instructions would be executed and resulted in a container image. This container image could then be used to launch one or more copies of it. Since the container image contains everything the application needs to run these images are ideal operating system independent packages. This solves the age-old problem of having to install the correct version of the runtime environment (PHP, Python, Java, etc) as it is contained within the container image. When a container is run this container image is the basis for all the data that is contained within. The data is, however, not copied. If the application modifies or creates files only the difference is stored. This makes containers very lightweight on the storage front as well. Docker went one step further, they introduced a way to share images between machines using a registry , a server that stores container images. The public Docker hub contains a vast array of images built by the community. These technologies were later all standardized under the Open Container Initiative , creating a company-independent format for containers and technologies around them. Warning Third party container images should be treated with the same due diligence like installing third party software on your computer! They can contain malicious code and can cause harm despite the containerization. Which is not running in a container? A CPU A userspace A kernel What is in a Dockerfile A container image A container snapshot An instruction set how to make a container image The container lifecycle # You can, of course, update a running container just as you would a traditional virtual machine. However, that is not the intended, or indeed, optimal way to use them. The intention of containers is immutable infrastructure . Immutable infrastructure means that containers are never updated . Instead, they are replaced. When a new version of a software needs to be installed the old container is shut down and a new one is launched from a new image. Immutable infrastructure presents a massive benefit: instead of having to deal with a downtime when the upgrade is ran, the updated version can be tested before it is launched. However, this concept reaches its limits when it comes to running software that needs to store data in a persistent fashion, for example, databases. For this purpose containers can designate special folders to be mounted as volumes . Volumes can be persisted by either mounting them as a folder from the host machine, or by mounting a network-based storage system. Orchestration (Swarm, Kubernetes, etc) # Docker and other container runtimes do not manage containers across multiple servers by themselves. Docker has received an addon called Docker Swarm, but nowadays the clear winner of the \u201cContainer Wars\u201d is Kubernetes . Container orchestrators are responsible for determining which server has free capacity and schedule containers to run on them. When a server fails these orchestrators are responsible for discovering it and re-launching them on a different server. Furthermore, container orchestrators contain a certain amount of integration with the cloud providers. Docker Swarm has very basic integrations, but Kubernetes on the other hand, has integrations with just about anything. Depending on the cloud provider Kubernetes can automatically move block storage mounts to a different server when a container is moved, configure cloud load balancers and much more. This flexibility comes at a cost: Kubernetes is very complex. A detailed explanation on how to deploy an application on Kubernetes would vastly exceed the bounds of this course. If you are interested in this topic we recommend taking a look at one of the many Certified Kubernetes Application Developer courses on the Internet. Container networking # As mentioned previously, containers regularly have their own, virtual network interfaces. This virtual network interface can be connected in a number of ways. If we take a look at Docker's default networking model the virtual network interface is connected to an internal network bridge . This enables containers to connect to each other. For connections to the Internet a NAT is performed. More advanced options for container networking create a single virtual network across several hosts and connect the containers to this virtual network. This is an option with Docker Swarm and is always the case with Kubernetes . When you look at these graphics, one question may come to mind: the cloud is quite dynamic in its nature, so how does a Kubernetes cluster receive incoming connections? After all, each of the nodes may be replaced with a new one, with a new IP address at any time? This is where the immense feature set of Kubernetes begins to show itself. Kubernetes includes several controllers that can interact with cloud providers in a variety of ways. One of these integrations can be, for example, the load balancer integrations. The Kubernetes network can be extended to a load balancer and that load balancer can send traffic directly over the virtual network used by the containers. Further helping the dynamic nature of the cloud are features like the ability to create internal firewalls. These firewalls can help mitigate possible security vulnerabilities like SSRF . Since these firewalls can be created using the same API as the software deployment, the firewall rules for an application can be versioned in Git, for example. Since everything required to run an application can be stored in a Dockerfile and a Kubernetes manifest the application source code can be self-contained. This is, of course, only true if the application is written in such a way that workes well with the cloud. This is described in the next lecture . Kubernetes # We have talked about Kubernetes before, but we are dedicating this section to more detail as it has become quite apparent that Kubernetes is going to be the de-facto standard for container orchestration in the future. The history of Kubernetes lies in the Borg orchestrator from Google . The intention is to be able to orchestrate workloads across thousands of machines. Since Kubernetes is highly scalable its design is difficult to use at small scale. Developer aspect # Developers using Kubernetes are only confronted with a subset of Kubernetes' complexity. A developer can create a Kubernetes manifest file (in YAML format) that instructs Kubernetes to run a certain set of containers, network them together, provide load balancers, etc. The basic unit of scheduling is a Pod . A pod is a unit consisting of one or more containers that share a network namespace and can potentially also share other resources. Pods can either run continuously or can be scheduled to run to completion for one-off jobs or cronjobs. However, unlike a simple Docker installation Kubernetes offers a way to manage Pods on a higher level. Deployments offer a way to create a resource to manage downtimeless deployments and roll back to a previous version with a simple rollback command if desired. They do this by using ReplicaSets , an automatic resource creating multiple copies of a Pod. As you can already see this highlights what we will discuss in the next lecture : a cloud-native application has to be ready to have multiple parallel copies of itself running. For more difficult workloads Kubernetes also includes a StatefulSet . StatefulSets give a developer the ability to guarantee a predictable startup sequence of multiple Pods as well as a unique ID for each. This is especially important for replicated databases. DaemonSets are, on the other hand, a special workload type more geared towards Kubernetes administrators, offering the ability to run a workload on every node of the cluster. This is often used to run utility Pods such as log collectors, monitoring agents, etc. Since some Pods need to store data Kubernetes offers a flexible way to allocate storage in the cloud with the use of PersistentVolumeClaims (PVC) and PersistentVolumes (PV). A Kubernetes administrator would set up a PV which a developer then uses using a PVC. This decouples the developers and administrators work nicely. It is worth remembering though that a single PV can only be used by a single PVC. This makes allocation in a larger cluster cumbersome. Naturally, Kubernetes has a solution for this problem called provisioners . These provisioners can dynamically create PVs as needed, usually by creating a network block storage volume in the cloud. It is also worth noting that, while local storage is supported, it provides no resilience against host machine failure, as we discussed in lecture 2 . While Kubernetes will do its best to support local storage and won't reschedule a workload with such an attached storage, it also limits the ability for Kubernetes to react to a node failure and move workloads. In order to facilitate internal and external load balancing Kubernetes introduces the concept of Services to create internal network load balancers for each service. These services use the Pod labels to track which Pods they should send traffic to. A special type of service is the Loadbalancer which, given a cloud integration, dynamically creates an external IP address for a specific service. Alternatively, workloads can also make use of the Ingress resource that dynamically configures the ingress controller to send HTTP workloads to specific pods. The ingress controller acts as an application load balancer for Kubernetes. To augment these capabilities the Job resource gives developers the ability to use Kubernetes as a queue system and run one-off workloads. To run regular jobs the CronJob resource can be used. Which Kubernetes resource do you use to run a container on every node in a cluster? Pod Deployment ReplicaSet DaemonSet Job CronJob Which of the following resources are managed by a Deployment in Kubernetes? Pod ReplicaSet DaemonSet Job CronJob Which of the following is the smallest unit in Kubernetes? Pod Deployment ReplicaSet DaemonSet Job CronJob With which resource can you create a queue in Kubernetes? Queue Pipe Job CronJob Which resource guarantees the startup order of pods? OrderedSet StatefulSet Job Deployment Architecture # The Kubernetes architecture is highly modular and the description given here is fairly generic. Individual Kubernetes distributions may differ greatly in their use of tools. At the core of Kubernetes is the API server . This central piece is the endpoint for both the clients (for example the Kubernetes CLI called kubectl ), other orchestrator components such as the controller-manager, cloud controller, or scheduler, and also the workload coordinator called the `kubelet. The scheduler is responsible for deciding which container is supposed to run on which worker node. As the name says, it schedules the workload. The controller-manager is a component composed of many small parts that decides what to run. For example, the ReplicaSet controller is responsible for creating multiple copies of the same pod. The cloud controller is responsible for the cloud provider integration. This is optional for a static cluster, but required if autoscaling, or a load balancer integration is required. The kubelet runs on every node and is responsible for talking to the Container Runtime (e.g. containerd) to run the containers the scheduler deems necessary. It is worth mentioning that the Kubelet must be able to reach the API server on a network level, and vice versa. HTTP request flow in both directions in order to make a Kubernetes cluster work. The kube-proxy service also often runs on each Kubernetes node to provide load balancing, but there are replacements for this component. The Container Network Interface (CNI) is a network plugin deployed alongside the kubelet and provides the previously-described overlay network. There is a wide range of CNI plugins from bare metal routing to Calico . As a final piece of the puzzle Container Storage Interfaces (CSI) provide a way to integrate just about any storage provider as a driver for PVs. Which Kubernetes component talks to the container runtime? Controller-Manager Cloud Controller API server Scheduler Kubelet kubectl kube-proxy Which Kubernetes component is responsible for managing internal load balancers? Controller-Manager Cloud Controller API server Scheduler Kubelet kubectl kube-proxy","title":"4. Containers"},{"location":"lectures/4-containers/#container_images","text":"Docker was not the first technology to create containers using the modern Linux API's, but it was the one that made containers popular. The edge Docker had over its competitors was the way it dealt with images. Other container solutions at the time, like LXC, tried to emulate virtual machines where a full operating system would be installed, and then updated. Docker chose a different route. A Dockerfile would contain the instructions needed to build a container image containing the application that should be containerized. These instructions would be executed and resulted in a container image. This container image could then be used to launch one or more copies of it. Since the container image contains everything the application needs to run these images are ideal operating system independent packages. This solves the age-old problem of having to install the correct version of the runtime environment (PHP, Python, Java, etc) as it is contained within the container image. When a container is run this container image is the basis for all the data that is contained within. The data is, however, not copied. If the application modifies or creates files only the difference is stored. This makes containers very lightweight on the storage front as well. Docker went one step further, they introduced a way to share images between machines using a registry , a server that stores container images. The public Docker hub contains a vast array of images built by the community. These technologies were later all standardized under the Open Container Initiative , creating a company-independent format for containers and technologies around them. Warning Third party container images should be treated with the same due diligence like installing third party software on your computer! They can contain malicious code and can cause harm despite the containerization. Which is not running in a container? A CPU A userspace A kernel What is in a Dockerfile A container image A container snapshot An instruction set how to make a container image","title":"Container images"},{"location":"lectures/4-containers/#the_container_lifecycle","text":"You can, of course, update a running container just as you would a traditional virtual machine. However, that is not the intended, or indeed, optimal way to use them. The intention of containers is immutable infrastructure . Immutable infrastructure means that containers are never updated . Instead, they are replaced. When a new version of a software needs to be installed the old container is shut down and a new one is launched from a new image. Immutable infrastructure presents a massive benefit: instead of having to deal with a downtime when the upgrade is ran, the updated version can be tested before it is launched. However, this concept reaches its limits when it comes to running software that needs to store data in a persistent fashion, for example, databases. For this purpose containers can designate special folders to be mounted as volumes . Volumes can be persisted by either mounting them as a folder from the host machine, or by mounting a network-based storage system.","title":"The container lifecycle"},{"location":"lectures/4-containers/#orchestration_swarm_kubernetes_etc","text":"Docker and other container runtimes do not manage containers across multiple servers by themselves. Docker has received an addon called Docker Swarm, but nowadays the clear winner of the \u201cContainer Wars\u201d is Kubernetes . Container orchestrators are responsible for determining which server has free capacity and schedule containers to run on them. When a server fails these orchestrators are responsible for discovering it and re-launching them on a different server. Furthermore, container orchestrators contain a certain amount of integration with the cloud providers. Docker Swarm has very basic integrations, but Kubernetes on the other hand, has integrations with just about anything. Depending on the cloud provider Kubernetes can automatically move block storage mounts to a different server when a container is moved, configure cloud load balancers and much more. This flexibility comes at a cost: Kubernetes is very complex. A detailed explanation on how to deploy an application on Kubernetes would vastly exceed the bounds of this course. If you are interested in this topic we recommend taking a look at one of the many Certified Kubernetes Application Developer courses on the Internet.","title":"Orchestration (Swarm, Kubernetes, etc)"},{"location":"lectures/4-containers/#container_networking","text":"As mentioned previously, containers regularly have their own, virtual network interfaces. This virtual network interface can be connected in a number of ways. If we take a look at Docker's default networking model the virtual network interface is connected to an internal network bridge . This enables containers to connect to each other. For connections to the Internet a NAT is performed. More advanced options for container networking create a single virtual network across several hosts and connect the containers to this virtual network. This is an option with Docker Swarm and is always the case with Kubernetes . When you look at these graphics, one question may come to mind: the cloud is quite dynamic in its nature, so how does a Kubernetes cluster receive incoming connections? After all, each of the nodes may be replaced with a new one, with a new IP address at any time? This is where the immense feature set of Kubernetes begins to show itself. Kubernetes includes several controllers that can interact with cloud providers in a variety of ways. One of these integrations can be, for example, the load balancer integrations. The Kubernetes network can be extended to a load balancer and that load balancer can send traffic directly over the virtual network used by the containers. Further helping the dynamic nature of the cloud are features like the ability to create internal firewalls. These firewalls can help mitigate possible security vulnerabilities like SSRF . Since these firewalls can be created using the same API as the software deployment, the firewall rules for an application can be versioned in Git, for example. Since everything required to run an application can be stored in a Dockerfile and a Kubernetes manifest the application source code can be self-contained. This is, of course, only true if the application is written in such a way that workes well with the cloud. This is described in the next lecture .","title":"Container networking"},{"location":"lectures/4-containers/#kubernetes","text":"We have talked about Kubernetes before, but we are dedicating this section to more detail as it has become quite apparent that Kubernetes is going to be the de-facto standard for container orchestration in the future. The history of Kubernetes lies in the Borg orchestrator from Google . The intention is to be able to orchestrate workloads across thousands of machines. Since Kubernetes is highly scalable its design is difficult to use at small scale.","title":"Kubernetes"},{"location":"lectures/4-containers/#developer_aspect","text":"Developers using Kubernetes are only confronted with a subset of Kubernetes' complexity. A developer can create a Kubernetes manifest file (in YAML format) that instructs Kubernetes to run a certain set of containers, network them together, provide load balancers, etc. The basic unit of scheduling is a Pod . A pod is a unit consisting of one or more containers that share a network namespace and can potentially also share other resources. Pods can either run continuously or can be scheduled to run to completion for one-off jobs or cronjobs. However, unlike a simple Docker installation Kubernetes offers a way to manage Pods on a higher level. Deployments offer a way to create a resource to manage downtimeless deployments and roll back to a previous version with a simple rollback command if desired. They do this by using ReplicaSets , an automatic resource creating multiple copies of a Pod. As you can already see this highlights what we will discuss in the next lecture : a cloud-native application has to be ready to have multiple parallel copies of itself running. For more difficult workloads Kubernetes also includes a StatefulSet . StatefulSets give a developer the ability to guarantee a predictable startup sequence of multiple Pods as well as a unique ID for each. This is especially important for replicated databases. DaemonSets are, on the other hand, a special workload type more geared towards Kubernetes administrators, offering the ability to run a workload on every node of the cluster. This is often used to run utility Pods such as log collectors, monitoring agents, etc. Since some Pods need to store data Kubernetes offers a flexible way to allocate storage in the cloud with the use of PersistentVolumeClaims (PVC) and PersistentVolumes (PV). A Kubernetes administrator would set up a PV which a developer then uses using a PVC. This decouples the developers and administrators work nicely. It is worth remembering though that a single PV can only be used by a single PVC. This makes allocation in a larger cluster cumbersome. Naturally, Kubernetes has a solution for this problem called provisioners . These provisioners can dynamically create PVs as needed, usually by creating a network block storage volume in the cloud. It is also worth noting that, while local storage is supported, it provides no resilience against host machine failure, as we discussed in lecture 2 . While Kubernetes will do its best to support local storage and won't reschedule a workload with such an attached storage, it also limits the ability for Kubernetes to react to a node failure and move workloads. In order to facilitate internal and external load balancing Kubernetes introduces the concept of Services to create internal network load balancers for each service. These services use the Pod labels to track which Pods they should send traffic to. A special type of service is the Loadbalancer which, given a cloud integration, dynamically creates an external IP address for a specific service. Alternatively, workloads can also make use of the Ingress resource that dynamically configures the ingress controller to send HTTP workloads to specific pods. The ingress controller acts as an application load balancer for Kubernetes. To augment these capabilities the Job resource gives developers the ability to use Kubernetes as a queue system and run one-off workloads. To run regular jobs the CronJob resource can be used. Which Kubernetes resource do you use to run a container on every node in a cluster? Pod Deployment ReplicaSet DaemonSet Job CronJob Which of the following resources are managed by a Deployment in Kubernetes? Pod ReplicaSet DaemonSet Job CronJob Which of the following is the smallest unit in Kubernetes? Pod Deployment ReplicaSet DaemonSet Job CronJob With which resource can you create a queue in Kubernetes? Queue Pipe Job CronJob Which resource guarantees the startup order of pods? OrderedSet StatefulSet Job Deployment","title":"Developer aspect"},{"location":"lectures/4-containers/#architecture","text":"The Kubernetes architecture is highly modular and the description given here is fairly generic. Individual Kubernetes distributions may differ greatly in their use of tools. At the core of Kubernetes is the API server . This central piece is the endpoint for both the clients (for example the Kubernetes CLI called kubectl ), other orchestrator components such as the controller-manager, cloud controller, or scheduler, and also the workload coordinator called the `kubelet. The scheduler is responsible for deciding which container is supposed to run on which worker node. As the name says, it schedules the workload. The controller-manager is a component composed of many small parts that decides what to run. For example, the ReplicaSet controller is responsible for creating multiple copies of the same pod. The cloud controller is responsible for the cloud provider integration. This is optional for a static cluster, but required if autoscaling, or a load balancer integration is required. The kubelet runs on every node and is responsible for talking to the Container Runtime (e.g. containerd) to run the containers the scheduler deems necessary. It is worth mentioning that the Kubelet must be able to reach the API server on a network level, and vice versa. HTTP request flow in both directions in order to make a Kubernetes cluster work. The kube-proxy service also often runs on each Kubernetes node to provide load balancing, but there are replacements for this component. The Container Network Interface (CNI) is a network plugin deployed alongside the kubelet and provides the previously-described overlay network. There is a wide range of CNI plugins from bare metal routing to Calico . As a final piece of the puzzle Container Storage Interfaces (CSI) provide a way to integrate just about any storage provider as a driver for PVs. Which Kubernetes component talks to the container runtime? Controller-Manager Cloud Controller API server Scheduler Kubelet kubectl kube-proxy Which Kubernetes component is responsible for managing internal load balancers? Controller-Manager Cloud Controller API server Scheduler Kubelet kubectl kube-proxy","title":"Architecture"},{"location":"lectures/5-cloud-native/","text":"Download PPTX \ud83d\udcbb Download MP3 \ud83c\udfa7 Watch Video \ud83c\udfac Cloud Native Software Development Software development doesn't exist in a vacuum. The software will need to run somewhere. While not every software needs to, or indeed will, scale to Google levels, a little forethought can go a long way towards surviving the first contact with the customer hordes. The enemy of scalability: state # Any application that does not store or access data is easy to scale. Simply run more copies of it and put a load balancer in front of it. State can rear its ugly head in obvious, and in more subtle ways. The obvious point state is, for example, your database. When you have data in your database that's a form of state. You can run as many application servers as you want, if the database can't handle the load you're done. As we discussed in the third lecture scaling databases can be quite tricky since the consistency model may not permit scaling. A second point of contention is the ability to store data. Integrating an object storage instead of simply writing data to the disk will, again, help with scaling. A more subtle point of contention may be the storage of sessions. Sessions are usually built by sending the user a session ID via a cookie and then storing a blob of data on the disk or in the database. The problem is not only the storage, but also that moving from a local filesystem to a database may create a race condition that may present a security issue with your application. A similar issue arises when near-realtime data exchange is desired between a large amount of users (e.g. a chat application). While a single server can scale to tens of thousands of users, that server is neither redundant, nor will it serve an endless number of users. PubSub systems can help with that. The Twelve Factor App # Once we survive the first night after the launch of our application it often becomes time to deal with long-term architectural problems. The 12 factor app is a concept that collects the current best practices of writing applications for the cloud. Keep in mind that these are just guidelines and you should never make a religious tirade out of following these guidelines. Instead, apply common sense and be pragmatic about what to do. 1. Codebase # This first guideline is pretty simple: keep your application in a version control system. While this should be the default in 2020, it unfortunately still bears saying. 2. Dependencies # This recommendation deals with dependencies. Almost every programming language ecosystem nowadays has a dependency manager to handle downloading dependencies. These dependencies should be declared explicitly in the configuration file for the dependency manager (e.g. package.json , composer.json , pom.xml , etc.) Furthermore, these dependency managers often create a lock file ( npm-package-lock.json , composer.lock , etc.). They record the exact versions, and sometimes hashes of the third party library. This ensures that the same exact version is installed in the development and production builds. 3. Configuration # While in older times configuration files had many formats, modern, container-based applications distinctly move towards environment variables . Environment variables are a cross-platform way to provide variables to an application and give a DevOps engineer a flexible way to configure a containerized application. 4. Backing services # External databases, caches, etc. should be treated as such . This means that the connection options (server name, url, username, password, etc) should be configurable via environment variable and should also be replacable by a system administrator by simply reconfiguring the container. Testing is also important with services that are not as well standardized as, for example, a MySQL database. For example, many S3 implementations only support Amazon's version and cannot be configured to use alternative providers. An application following this recommendation should be tested against at least one other S3 provider. 5. The build process # The build recommendation says that the build process should be separated from the runtime configuration and the release process. In practice this means that you don't bake your configuration into your container. It also means that each release should have a version number or date, allowing you to roll back to a specific version. 6. Stateless processes # The processes recommendation says that your application should run as a single program without any shared data between different copies of your application. Any state should be outsourced to an external database or system as mentioned before. 7. Network configuration # The port binding recommendation says that an application is standalone and does not require an external webserver to run. (Typically PHP applications require a bit of legwork to satisfy this requirement.) 8. Process handling # The concurrency recommendation defines a number of rules for process handling. It requires that it should be possible to run multiple copies of the application, optionally across several machines. It also requires that the application should not daemonize . While this may seem as trivial with a green field project, it is definitely a challenge for legacy application. 9. Disposability # This recommendation states that an application should have a fast startup and should also shut down quickly when receiving the TERM signal. Applications should also be able to recover from unexpected crashes. 10. Development/production gap # The dev/prod parity recommendation postulates that developers of apps should strive to have a continuous method of deployment that allows for rapid rollout of updates. This lowers the work a developer needs to go through to deploy and therefore promotes smaller change sets. Smaller change sets make tracking down issues with deployments easier, but can be impractical when working with larger, slower moving clients. (e.g. banks, telcos, etc.) 11. Logs # The logs recommendation states that an app should simply write logs to the standard output in a standardized form (e.g. JSON). Everything else, like routing or storing logs, is not the applications concern. In practice each application contains at least some basic logic to filter logs to make it easier to configure the logging level. 12. Tooling # The admin process recommendation concerns itself with the command line tools needed to operate the application. These are things like running database migrations, etc. The recommendation says that these tools should be runnable directly from the applications directory without much extra configuration or installation. Tip These factors are, of course, not exhaustive. IBM has published a whole host of additional factors . Monitoring and metrics collection # The above 12 factors are, by necessity, limited in scope. The author(s) of those 12 factors have taken many things into account, yet left out others. One of the most important aspects left out of the list above is monitoring. Any developer wanting to working a cloud-native environment needs to consider monitoring. After all, the journey to the cloud is supposed to bring greater stability and facilitate rapid development cycles. In this section we will discuss the monitoring types and their practical implementation. Periodic checks # One of the most classic forms of monitoring are periodic checks. Periodic checks run a certain check, for example a connection, against the application. If the connection succeeds the check is green, if not the check is red and an alert is sent out. The check types are varied, ranging from a simple connection attempt to a fully blown interaction with the application. These checks not only serve the purpose of alerting the administrator but can also work as a trigger for a self-healing system. In a containerized environment, for example, we have several options to implement a self-healing system. You can add the HEALTHCHECK directive in the Dockerfile for example, or you can implement Liveness, Readiness, and Startup Probles in Kubernetes. The important question to consider with periodic checks is the rate of false positives. An intermittent network failure may not be a signal for an application failing and can still raise alarms. In order to avoid alert fatigue a well designed monitoring system keeps false alerts to a minimum. Feature tests # A very special case of periodic checks are full-on feature tests. These feature tests blur the boundaries between development-time software testing and operations. The intention is to run a full application test, such as with the Selenium test suite, against a production application. This test could test a full signup, for example. This level of testing ensures that the application keeps working even after deployment. While not practical in every scenario these tests can ensure a high level of certainty and can also function as a regression test when the infrastructure changes rather than the application. Metrics collection # Another important aspect of monitoring is metrics collection. Periodic checks can only check what's there, but they cannot indicate a brewing trouble before it happens. Metrics can be an early warning system for a failure in the application. For example, a software rollout can significantly impact signups. While the change may have been entirely intentional it is business critical to observe this happening and alert immediately to a potential defect in the software. Are the loading times too long? Did the layout change make the process too confusing? Metrics can help. As you will learn in the Prometheus exercise , Prometheus has established itself as a defacto standard for monitoring cloud-native applications. Prometheus makes it exceedingly easy to integrate metrics collection with just about any application. It is not uncommon to see applications include a small webserver that exposes internal metrics in the Prometheus data format . Prometheus then collects these metrics and provides alerting and query services. The reason why Prometheus has established itself as a standard is its ability to monitor a dynamic number of machines. It first queries the the cloud provider for a list of IP addresses and then queries those IPs for the metrics. As machines are started and stopped Prometheus always keeps track of them. Log collection # Depending on how well written the application is it may expose more or less useful logs. One thing is certain: error counts can also be an early warning system for latent defects. For example, your access log may contain the individual entries of failed requests. A log aggregation system such as the ELK stack can discover patterns in your logs and give you alerts when something is going south. Dashboards # No monitoring system is complete without a way to look at what's going on. This niche is filled by dashboards such as Grafana which we will discuss in greater detail in exercise 5 . Microservices # The proliferation of containers has brought about a change in software design as well: microservices. Microservices is a concept where, instead of building one monolithic application one builds a tiny service for each need. These services are connected across the network, usually via HTTP. On its face this solves the problem of creating an incoherent giant ball of code with bad code quality. However, experience has shown that this is not true. A team or company that is not able to write a good quality monolith is also not able to design and maintain microservices properly. To write a well designed system, no matter if microservices or monolith, one needs to have clean API boundaries between the different parts of the system. Unfortunately, there is no simple way to achieve clean boundaries as there is a host of literature on Clean Code that would exceed the bounds of this course. Where's the difference microservices and a monolith? For one, it's the network . When you write a monolith you can rely on every call across API boundaries to always, reliably return. With microservices, this is not true. Microservices calls can hang indefinitely. It is possible that the other service simply disappears from the network and you won't even know it. That's why a microservices architecture always needs to include the Circuit Breaker pattern Second, it's the runtime. If there are several microservices calling each other, each service being reasonably fast, the run time can still add up. Imagine 5 microservices calling each other, each with a maximum runtime of 200ms. That's a one second response time right there! Third, it's the API design. If you break your API in a monolithic application chances are your compiler will bark at you and refuse to compile the code. If you break compatibility across microservices possibly nothing happens. With badly written microservices that ignore missing parameters, for example, systems can break in a really horrifying manner. Microservices make sense, and indeed follow Conway's Law: Any organization that designs a system (defined broadly) will produce a design whose structure is a copy of the organization's communication structure. \u2014\u2009Melvin E. Conway Conway formulated this in 1968, long before microservices. It still holds true today: microservices in an organization will always follow the team or company structure. For example, there may be several development teams that enjoy the ability to develop and deploy their application parts independently from each other. Or, there may be one development team that needs to gather information from several source systems from different departments. Microservices are also a good idea where boundaries are needed between applications with different technology stacks (e.g. to pass information from Java to Javascript, etc.) However, it makes very little sense for a small development team in a startup, using a homogeneous technology stack to use microservices. Chances are, this team will be violating API boundaries all over the place and disregard the requirements for building a network-based distributed system. Ask the Authors If you want a fun story about how microservices can go horribly wrong, ask the authors in the consulting sessions. Be careful! It is easy to go over-board with microservices to the point where you have more Kubernetes manifests than actual program code! Don't forget, you not only have to write them, but also deploy them. If you go for services that are too small, or your team is ill-equipped to handle a large number of tiny services in production, you may have one of the many horrible microservices failures on your hand. Loosely typed languages Be careful with loosely typed languages and systems like JavaScript! These systems are very tolerant towards missing parameters, or parameters with incorrect types. This can lead to some disastrous consequences. Further reading While we will not be asking any of the following materials in the test we strongly recommend that you take them as a starting point for your further studies: Clean code S.O.L.I.D. principles Getting started in Object-Oriented Programming What people misunderstand about OOP What the ** is an IoC container? The clean code talks: Unit Testing The clean code talks: OO Design for Testability The clean code talks: Don't look for things! How to design a good API and why it matters Microservices Principles of Microservices 3 common pitfalls of microservices integration Avoiding Microservice Megadisasters Distributed systems Distributed Systems in One Lesson Service meshes, Frameworks, and Tools # Service meshes are a tool to help with building microservices. As mentioned before, microservices are hard. Not only do you have to take the network into account, not only do you have to make the API work together across many different components, you also have to make sure that components find each other. Kubernetes makes sure that there are internal load balancers for each service allowing you to do a basic rollout. However, Kubernetes does not retry failed calls, allow for versioning, implement a circuit breaker, etc. Service meshes like Istio do that. You don't strictly need a service mesh to build microservices, but they make the process easier. This is especially true if you run applications developed by different teams with different tooling, where implementing a common service registration would be hard. We recommend watching Building cloud-native applications with Kubernetes and Istio, by Kelsey Hightower for more details on how Istio works. Alternatively, you can of course use the tools afforded by your framework. If you are in the Java world, you could use Spring Cloud , for example. Spring Cloud will let you register your services with a wide variety of backend services. However, keep in mind: microservices are hard. As a developer you will be curious and willing to learn about them, of course. But, your curiosity may not serve the company's interest as much as you may think it does.","title":"5. Cloud-native software development"},{"location":"lectures/5-cloud-native/#the_enemy_of_scalability_state","text":"Any application that does not store or access data is easy to scale. Simply run more copies of it and put a load balancer in front of it. State can rear its ugly head in obvious, and in more subtle ways. The obvious point state is, for example, your database. When you have data in your database that's a form of state. You can run as many application servers as you want, if the database can't handle the load you're done. As we discussed in the third lecture scaling databases can be quite tricky since the consistency model may not permit scaling. A second point of contention is the ability to store data. Integrating an object storage instead of simply writing data to the disk will, again, help with scaling. A more subtle point of contention may be the storage of sessions. Sessions are usually built by sending the user a session ID via a cookie and then storing a blob of data on the disk or in the database. The problem is not only the storage, but also that moving from a local filesystem to a database may create a race condition that may present a security issue with your application. A similar issue arises when near-realtime data exchange is desired between a large amount of users (e.g. a chat application). While a single server can scale to tens of thousands of users, that server is neither redundant, nor will it serve an endless number of users. PubSub systems can help with that.","title":"The enemy of scalability: state"},{"location":"lectures/5-cloud-native/#the_twelve_factor_app","text":"Once we survive the first night after the launch of our application it often becomes time to deal with long-term architectural problems. The 12 factor app is a concept that collects the current best practices of writing applications for the cloud. Keep in mind that these are just guidelines and you should never make a religious tirade out of following these guidelines. Instead, apply common sense and be pragmatic about what to do.","title":"The Twelve Factor App"},{"location":"lectures/5-cloud-native/#1_codebase","text":"This first guideline is pretty simple: keep your application in a version control system. While this should be the default in 2020, it unfortunately still bears saying.","title":"1. Codebase"},{"location":"lectures/5-cloud-native/#2_dependencies","text":"This recommendation deals with dependencies. Almost every programming language ecosystem nowadays has a dependency manager to handle downloading dependencies. These dependencies should be declared explicitly in the configuration file for the dependency manager (e.g. package.json , composer.json , pom.xml , etc.) Furthermore, these dependency managers often create a lock file ( npm-package-lock.json , composer.lock , etc.). They record the exact versions, and sometimes hashes of the third party library. This ensures that the same exact version is installed in the development and production builds.","title":"2. Dependencies"},{"location":"lectures/5-cloud-native/#3_configuration","text":"While in older times configuration files had many formats, modern, container-based applications distinctly move towards environment variables . Environment variables are a cross-platform way to provide variables to an application and give a DevOps engineer a flexible way to configure a containerized application.","title":"3. Configuration"},{"location":"lectures/5-cloud-native/#4_backing_services","text":"External databases, caches, etc. should be treated as such . This means that the connection options (server name, url, username, password, etc) should be configurable via environment variable and should also be replacable by a system administrator by simply reconfiguring the container. Testing is also important with services that are not as well standardized as, for example, a MySQL database. For example, many S3 implementations only support Amazon's version and cannot be configured to use alternative providers. An application following this recommendation should be tested against at least one other S3 provider.","title":"4. Backing services"},{"location":"lectures/5-cloud-native/#5_the_build_process","text":"The build recommendation says that the build process should be separated from the runtime configuration and the release process. In practice this means that you don't bake your configuration into your container. It also means that each release should have a version number or date, allowing you to roll back to a specific version.","title":"5. The build process"},{"location":"lectures/5-cloud-native/#6_stateless_processes","text":"The processes recommendation says that your application should run as a single program without any shared data between different copies of your application. Any state should be outsourced to an external database or system as mentioned before.","title":"6. Stateless processes"},{"location":"lectures/5-cloud-native/#7_network_configuration","text":"The port binding recommendation says that an application is standalone and does not require an external webserver to run. (Typically PHP applications require a bit of legwork to satisfy this requirement.)","title":"7. Network configuration"},{"location":"lectures/5-cloud-native/#8_process_handling","text":"The concurrency recommendation defines a number of rules for process handling. It requires that it should be possible to run multiple copies of the application, optionally across several machines. It also requires that the application should not daemonize . While this may seem as trivial with a green field project, it is definitely a challenge for legacy application.","title":"8. Process handling"},{"location":"lectures/5-cloud-native/#9_disposability","text":"This recommendation states that an application should have a fast startup and should also shut down quickly when receiving the TERM signal. Applications should also be able to recover from unexpected crashes.","title":"9. Disposability"},{"location":"lectures/5-cloud-native/#10_developmentproduction_gap","text":"The dev/prod parity recommendation postulates that developers of apps should strive to have a continuous method of deployment that allows for rapid rollout of updates. This lowers the work a developer needs to go through to deploy and therefore promotes smaller change sets. Smaller change sets make tracking down issues with deployments easier, but can be impractical when working with larger, slower moving clients. (e.g. banks, telcos, etc.)","title":"10. Development/production gap"},{"location":"lectures/5-cloud-native/#11_logs","text":"The logs recommendation states that an app should simply write logs to the standard output in a standardized form (e.g. JSON). Everything else, like routing or storing logs, is not the applications concern. In practice each application contains at least some basic logic to filter logs to make it easier to configure the logging level.","title":"11. Logs"},{"location":"lectures/5-cloud-native/#12_tooling","text":"The admin process recommendation concerns itself with the command line tools needed to operate the application. These are things like running database migrations, etc. The recommendation says that these tools should be runnable directly from the applications directory without much extra configuration or installation. Tip These factors are, of course, not exhaustive. IBM has published a whole host of additional factors .","title":"12. Tooling"},{"location":"lectures/5-cloud-native/#monitoring_and_metrics_collection","text":"The above 12 factors are, by necessity, limited in scope. The author(s) of those 12 factors have taken many things into account, yet left out others. One of the most important aspects left out of the list above is monitoring. Any developer wanting to working a cloud-native environment needs to consider monitoring. After all, the journey to the cloud is supposed to bring greater stability and facilitate rapid development cycles. In this section we will discuss the monitoring types and their practical implementation.","title":"Monitoring and metrics collection"},{"location":"lectures/5-cloud-native/#periodic_checks","text":"One of the most classic forms of monitoring are periodic checks. Periodic checks run a certain check, for example a connection, against the application. If the connection succeeds the check is green, if not the check is red and an alert is sent out. The check types are varied, ranging from a simple connection attempt to a fully blown interaction with the application. These checks not only serve the purpose of alerting the administrator but can also work as a trigger for a self-healing system. In a containerized environment, for example, we have several options to implement a self-healing system. You can add the HEALTHCHECK directive in the Dockerfile for example, or you can implement Liveness, Readiness, and Startup Probles in Kubernetes. The important question to consider with periodic checks is the rate of false positives. An intermittent network failure may not be a signal for an application failing and can still raise alarms. In order to avoid alert fatigue a well designed monitoring system keeps false alerts to a minimum.","title":"Periodic checks"},{"location":"lectures/5-cloud-native/#feature_tests","text":"A very special case of periodic checks are full-on feature tests. These feature tests blur the boundaries between development-time software testing and operations. The intention is to run a full application test, such as with the Selenium test suite, against a production application. This test could test a full signup, for example. This level of testing ensures that the application keeps working even after deployment. While not practical in every scenario these tests can ensure a high level of certainty and can also function as a regression test when the infrastructure changes rather than the application.","title":"Feature tests"},{"location":"lectures/5-cloud-native/#metrics_collection","text":"Another important aspect of monitoring is metrics collection. Periodic checks can only check what's there, but they cannot indicate a brewing trouble before it happens. Metrics can be an early warning system for a failure in the application. For example, a software rollout can significantly impact signups. While the change may have been entirely intentional it is business critical to observe this happening and alert immediately to a potential defect in the software. Are the loading times too long? Did the layout change make the process too confusing? Metrics can help. As you will learn in the Prometheus exercise , Prometheus has established itself as a defacto standard for monitoring cloud-native applications. Prometheus makes it exceedingly easy to integrate metrics collection with just about any application. It is not uncommon to see applications include a small webserver that exposes internal metrics in the Prometheus data format . Prometheus then collects these metrics and provides alerting and query services. The reason why Prometheus has established itself as a standard is its ability to monitor a dynamic number of machines. It first queries the the cloud provider for a list of IP addresses and then queries those IPs for the metrics. As machines are started and stopped Prometheus always keeps track of them.","title":"Metrics collection"},{"location":"lectures/5-cloud-native/#log_collection","text":"Depending on how well written the application is it may expose more or less useful logs. One thing is certain: error counts can also be an early warning system for latent defects. For example, your access log may contain the individual entries of failed requests. A log aggregation system such as the ELK stack can discover patterns in your logs and give you alerts when something is going south.","title":"Log collection"},{"location":"lectures/5-cloud-native/#dashboards","text":"No monitoring system is complete without a way to look at what's going on. This niche is filled by dashboards such as Grafana which we will discuss in greater detail in exercise 5 .","title":"Dashboards"},{"location":"lectures/5-cloud-native/#microservices","text":"The proliferation of containers has brought about a change in software design as well: microservices. Microservices is a concept where, instead of building one monolithic application one builds a tiny service for each need. These services are connected across the network, usually via HTTP. On its face this solves the problem of creating an incoherent giant ball of code with bad code quality. However, experience has shown that this is not true. A team or company that is not able to write a good quality monolith is also not able to design and maintain microservices properly. To write a well designed system, no matter if microservices or monolith, one needs to have clean API boundaries between the different parts of the system. Unfortunately, there is no simple way to achieve clean boundaries as there is a host of literature on Clean Code that would exceed the bounds of this course. Where's the difference microservices and a monolith? For one, it's the network . When you write a monolith you can rely on every call across API boundaries to always, reliably return. With microservices, this is not true. Microservices calls can hang indefinitely. It is possible that the other service simply disappears from the network and you won't even know it. That's why a microservices architecture always needs to include the Circuit Breaker pattern Second, it's the runtime. If there are several microservices calling each other, each service being reasonably fast, the run time can still add up. Imagine 5 microservices calling each other, each with a maximum runtime of 200ms. That's a one second response time right there! Third, it's the API design. If you break your API in a monolithic application chances are your compiler will bark at you and refuse to compile the code. If you break compatibility across microservices possibly nothing happens. With badly written microservices that ignore missing parameters, for example, systems can break in a really horrifying manner. Microservices make sense, and indeed follow Conway's Law: Any organization that designs a system (defined broadly) will produce a design whose structure is a copy of the organization's communication structure. \u2014\u2009Melvin E. Conway Conway formulated this in 1968, long before microservices. It still holds true today: microservices in an organization will always follow the team or company structure. For example, there may be several development teams that enjoy the ability to develop and deploy their application parts independently from each other. Or, there may be one development team that needs to gather information from several source systems from different departments. Microservices are also a good idea where boundaries are needed between applications with different technology stacks (e.g. to pass information from Java to Javascript, etc.) However, it makes very little sense for a small development team in a startup, using a homogeneous technology stack to use microservices. Chances are, this team will be violating API boundaries all over the place and disregard the requirements for building a network-based distributed system. Ask the Authors If you want a fun story about how microservices can go horribly wrong, ask the authors in the consulting sessions. Be careful! It is easy to go over-board with microservices to the point where you have more Kubernetes manifests than actual program code! Don't forget, you not only have to write them, but also deploy them. If you go for services that are too small, or your team is ill-equipped to handle a large number of tiny services in production, you may have one of the many horrible microservices failures on your hand. Loosely typed languages Be careful with loosely typed languages and systems like JavaScript! These systems are very tolerant towards missing parameters, or parameters with incorrect types. This can lead to some disastrous consequences. Further reading While we will not be asking any of the following materials in the test we strongly recommend that you take them as a starting point for your further studies: Clean code S.O.L.I.D. principles Getting started in Object-Oriented Programming What people misunderstand about OOP What the ** is an IoC container? The clean code talks: Unit Testing The clean code talks: OO Design for Testability The clean code talks: Don't look for things! How to design a good API and why it matters Microservices Principles of Microservices 3 common pitfalls of microservices integration Avoiding Microservice Megadisasters Distributed systems Distributed Systems in One Lesson","title":"Microservices"},{"location":"lectures/5-cloud-native/#service_meshes_frameworks_and_tools","text":"Service meshes are a tool to help with building microservices. As mentioned before, microservices are hard. Not only do you have to take the network into account, not only do you have to make the API work together across many different components, you also have to make sure that components find each other. Kubernetes makes sure that there are internal load balancers for each service allowing you to do a basic rollout. However, Kubernetes does not retry failed calls, allow for versioning, implement a circuit breaker, etc. Service meshes like Istio do that. You don't strictly need a service mesh to build microservices, but they make the process easier. This is especially true if you run applications developed by different teams with different tooling, where implementing a common service registration would be hard. We recommend watching Building cloud-native applications with Kubernetes and Istio, by Kelsey Hightower for more details on how Istio works. Alternatively, you can of course use the tools afforded by your framework. If you are in the Java world, you could use Spring Cloud , for example. Spring Cloud will let you register your services with a wide variety of backend services. However, keep in mind: microservices are hard. As a developer you will be curious and willing to learn about them, of course. But, your curiosity may not serve the company's interest as much as you may think it does.","title":"Service meshes, Frameworks, and Tools"},{"location":"projectwork/","text":"You are the cloud architect for a small work-for-hire company. A client wants to hire you but they are sceptical about your abilities to build an autoscaling service. They propose a proof of concept: build a service that runs a web application that is deliberately slow and load test it. Your cloud management should automatically launch new cloud servers when the load is high and remove servers when demand is low. Warning Do not continuously run your setup on Exoscale or you will run out of budget! Budget limitations are part of building cloud system so you have to learn to deal with it. After taking a look at the capabilities of the cloud provider and discussing the constraints with your colleagues you decide that the following approach would be best: You are going to use Terraform to automate the setup and tear down of the cloud infrastructure. This is necessary because if you continuously use the cloud service you will not fit in the budget. You will use instance pools to manage the variable number of cloud servers and Network Load Balancers to balance the traffic between them. You will set up a dedicated monitoring and management instance which will run Prometheus to automatically monitor a varying number of servers. You will write a custom service discovery agent that creates a file with the IP addresses of the machines in the instance pool for Prometheus to consume. On the instance pool you will deploy the Prometheus node exporter to monitor CPU usage. You will install Grafana to provide a monitoring dashboard and the ability to send webhooks. You will configure an alert webhook in Grafana that sends a webhook to an application written by you. If the average CPU usage is above 80%, or below 20% to scale up or down respectively a webhook is sent. You will write an application that receives this webhook and every 60 seconds scales the instance pool up or down if a webhook has been received. As you also have to demonstrate to the client that you can work in an agile methodology you agree in 4 week sprints with a demo at the end of each sprint as outlined in the deadlines document. As a dummy service to generate load you will use http-load-generator . The manual way to implement this architecture is described on the Exoscale blog . We recommend reading and doing the steps described in the blog post at least once. Optionally, you can make use of the following ( incurs a 5% point-penalty each ): prometheus-sd-exoscale-instance-pools to feed instance pool data into Prometheus exoscale-grafana-autoscaler to drive the autoscaling behavior. Handing in your project work # In order to hand in your project work you must upload your code to a Git repository, for example on GitHub. Submit the link of the Git repository in Moodle . Your Terraform code must be able to install the complete infrastructure into an empty Exoscale account. It must ask for the Exoscale API key and secret using the exoscale_key and exoscale_secret variable. If you opt to only implement the manual method (without Terraform) please enter manual in instead of the text field and we will contact you for a manual review appointment. Warning Keep in mind that only submitting a manual solution will give you minimal points. We strongly recommend you ask for help if you are stuck rather than implementing the manual solution. Note The automated version has to run without manual intervention beyond inserting variables. Additionally, the solutions will be spot-checked for duplication, and you may be required to jump on a 10 minute call to quickly explain your call. Note If your code does not run after handing it in and that error is due to a minor mistake in your Terraform code you will be given a chance to fix it within 5 days. Getting help # Please see the getting help page .","title":"Project work"},{"location":"projectwork/#handing_in_your_project_work","text":"In order to hand in your project work you must upload your code to a Git repository, for example on GitHub. Submit the link of the Git repository in Moodle . Your Terraform code must be able to install the complete infrastructure into an empty Exoscale account. It must ask for the Exoscale API key and secret using the exoscale_key and exoscale_secret variable. If you opt to only implement the manual method (without Terraform) please enter manual in instead of the text field and we will contact you for a manual review appointment. Warning Keep in mind that only submitting a manual solution will give you minimal points. We strongly recommend you ask for help if you are stuck rather than implementing the manual solution. Note The automated version has to run without manual intervention beyond inserting variables. Additionally, the solutions will be spot-checked for duplication, and you may be required to jump on a 10 minute call to quickly explain your call. Note If your code does not run after handing it in and that error is due to a minor mistake in your Terraform code you will be given a chance to fix it within 5 days.","title":"Handing in your project work"},{"location":"projectwork/#getting_help","text":"Please see the getting help page .","title":"Getting help"},{"location":"testing/","text":"Tests will be conducted starting in December 2020. The exam will last 120 minute and consist of a multiple choice test. During the test you will be required to obey the following rules: Have an empty room with no disturbances. Close all other applications except for the single browser window the test runs in. Remove all electronic devices and notes from your desk and person. No phones, no smartwatches of any kind are allowed. Unless medically required no food is allowed during the test and only clear liquids in clear, unlabeled containers are allowed. During the test you will not be allowed to talk. You will be required to turn on your camera the entire time during the test. You will also be required to show that your environment suits the criteria described above. You will be required to turn on your screen sharing during the entire test.","title":"Testing"}]}